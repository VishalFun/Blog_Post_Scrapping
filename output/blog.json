[
{"tite": "Data Center Proxies vs. Residential Proxies", "date": "July 21, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution.", " proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.", "Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.", "Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.", "Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean.", "The biggest issue with residential proxies is, as it was mentioned, they are expensive. So usually the most effective way to scale your web data extraction project, is to try to maximize the value of data center proxies, by being smart about how you actually scrape the web and how you use proxies.", "Two things, that you can do to achieve this:", "If you want to learn more about these tactics, I recommend watching our FREE webinar on this: ", "If you missed our webinar on the topic of data center proxies and residential proxies don\u2019t worry you will be able to watch it here:", "If you feel like you know enough already and you don't want to spend way too much time on managing proxies, you can just use an already existing solution for ", "."]},
{"tite": "Scrapy Cloud Secrets: Hub Crawl Frontier and How To Use It", "date": "August 06, 2020 ", "author": "J\u00falio C\u00e9sar Batista", "blog_data": ["Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.", "Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where ", "(HCF) comes to our rescue.", "HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to ", ", but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with ", ", you may be wondering why one would use HCF, when Scrapy can store and recover the crawling state by itself.\u00a0", "The advantage is that Scrapy requires you to manage this state, by saving the content to disk (so needs disk quota) and if you are running inside a container, like in Scrapy Cloud, local files are lost once the process is finished. So, having some kind of external storage for requests is an alternative that takes this burden from your shoulders, leaving you to think about the extraction logic and not about the details on how to proceed in case it crashes and you need to restart.", "Before digging into an example of how to use HCF, I\u2019ll go over a bit on how it is structured. We can create many Frontiers per project, for each one we need a name. These Frontiers are then broken into slots, something similar to sharding, that can be useful in a producer-consumer scenario (topic of one of our upcoming blog posts). Usually, the name will be the name of the spider, to avoid any confusion. The catchy part is that we shouldn't change the number of slots after it was created, so keep it in mind when creating it.", "Now that we know what HCF is and how we could make use of it, it is time to see it working. For this purpose, we\u2019ll build a simple Scrapy spider to extract book information from ", ". To get started, we\u2019ll create a new scrapy project and install the proper dependencies as shown below (type them in your terminal).", "The commands above will create a new directory for our project and create a new virtual environment, to avoid messing up our Operational System. Then it will install Scrapy and some libraries to use HCF. Finally, it creates a new Scrapy project and a spider. A side note on the extra libraries for HCF. There are a couple of libraries we could use, like ", ", but it seems to be unmaintained for awhile. So, we\u2019ll be using ", " and HCF as a backed through ", ".", "Given that our project was successfully created and the dependencies were installed, we can write a minimal spider to extract the book data as shown in the following code snippet.", "If you are familiar with Scrapy, there\u2019s nothing so fancy in the code above. Just a simple spider that navigates the book pages and follows book links to their pages to extract the title and price.", "We can run this spider from the terminal by typing Scrapy crawl books.toscrape.com and we should see the result there (no errors and 1,000 items were extracted). So far, we\u2019re not interacting with HCF and we\u2019ll be doing so by configuring it in the following changes. First, we\u2019ll need to update our project settings.py file with the following.", "The SCHEDULER, SPIDER_MIDDLEWARES and DOWNLOADER_MIDDLEWARES are set so scrapy-frontera works. Then, we set HCF as the BACKEND and add the proper Scrapy Cloud API Key (HCF_AUTH) and the project in which we\u2019re creating the Frontier (HCF_PROJECT_ID). With these settings in place, we can update our spider, so it starts interacting with HCF. If you run the spider now, you\u2019ll see some new logs, but it won\u2019t be storing the requests in HCF yet. The following changes should be applied in books_toscrape_com.py file.", "Recall that we are using ", " to interact with HCF, that\u2019s the main reason we need to set ", "Basically, we are setting the Frontier name where we are going to store the requests", "and where we are consuming them", "The ", " setting means the number of slots we should be creating for this producer, in this case only one and ", " means the slot we\u2019re using for consumption which is the slot 0 (given that there is only 1 and starts from 0). Finally, we need to tell scrapy-frontera which requests it should send to the backend, and it happens by identifying the request callback. If the request callback is any of the names set in ", " it will be sent to the backend, otherwise it\u2019ll be processed as a local scrapy request.", "This is it, we\u2019ve got the moment that we can run our spider and it will be storing the requests in HCF. Just run the spider as we did before and it should work! But how can I tell that the requests were sent to HCF? For that, hcf-backend comes with a handy tool to help us, the hcfpal. From your terminal, just run the command below and you should see the Frontier name.", "There are some other commands available in hcfpal, like counting nthe requests in a given frontier.", "It will show you the request count per slot and total count (in case you have more than one slot).", "As we are storing the requests in HCF for further restart, it can be used as an example of incremental crawling. So, no need for special logic or so, just run the spider and it should start getting only new content. The requests are identified as in scrapy, by their fingerprint. There is one catch when working with multiple slots that is:, a given request is unique in a given slot (but we won\u2019t bother with it for now and leave it for a future article). To get started, let\u2019s clean our Frontier by typing the following in our terminal.", "Once it\u2019s done, run the spider but stop it before it finishes (simulating an early stop). To do it, press CTRL + C (Command + C) on the terminal once. It should send the signal to scrapy to finish the process. Then, wait a bit so the crawling process finishes. As the process finishes, it logs the stats in the terminal and we should use them to understand a bit of what\u2019s happening.", "For example, by looking into ", " I get that 80 items were extracted. Also, pay attention to stats starting with hcf/consumer and hcf/producer. These are related to the URLs we found in our run, how many were processed/extracted (consumed) and how many were discovered/stored (produced). In my case, it consumed 84 requests and found 105 links (all new, as we had cleaned the Frontier before running).", "After inspecting the stats, run the spider once again, without deleting the Frontier, and wait for it to finish. You should see that item_scrape_count is the difference between the previous crawl and the current one (in my case, 920 items). This happened because the duplicate requests were filtered by HCF and then they weren\u2019t processed again.", "You should also identify a similar behavior in ", " and ", " stats, showing that some links were extracted but not all of them are new.", "Finally,\u00a0 you can run the spider once more and it will just stop, logging no items scraped, because all the links it extracts were already processed in the previous runs. So, there is no new data to be processed and it finishes.", "HCF is a kind of external storage for requests that is available in Scrapy Cloud projects and it can be used by Scrapy spiders. There are many use cases for it, and we\u2019ve been through the recovery of a crawling process and incremental crawling scenarios. For a future article, we\u2019ll explore a bit more how we can configure HCF in our projects and how to use it in a producer-consumer architecture. If you got interested in it, I invite you to check the ", " (which has some information similar to this tutorial) and ", ".", "If you want to learn more about web data extraction and how it can serve your business you can check out our ", " to see how others are making use of web data. Also, if you\u2019re considering outsourcing web scraping, you can watch our on-demand webinar to help you decide between ", " web data extraction."]},
{"tite": "Web Scraping Basics: A Developer\u2019s Guide To Reliably Extract Data", "date": "July 07, 2020 ", "author": "Chira Mircea", "blog_data": ["The web is complex and constantly changing. It is one of the reasons why web data extraction can be difficult, especially in the long term. It\u2019s necessary to understand how a website works really well, before you try to extract data. Luckily, there are lots of inspection and code tools available for this and in this article we will show you some of our favorites.", "All major browsers come packed with a set of development tools. Although these have been built with the goal of building websites in mind, they can also be used to analyze web pages and traffic. These are some pretty powerful tools for working with websites.", "For Google Chrome, these developer tools can be accessed from any page by right-clicking then choosing 'Inspect' or using the shortcut 'CTRL + shift + I' (or '\u2318 + Option + I' on macs).", "You can use these tools to perform some basic operations:", "Most web pages contain a lot of javascript that needs to be executed before you can see the final output. But you can see how the initial request looks before all of this by checking the page source. To do that you can right-click and then click on 'View page source' or use the shortcuts:", "Press CTRL + U (\u2318 + Option + U) to view source", "Press CTRL + F (\u2318 + Option + F) to search", "Sometimes the information you're looking for is not loaded with the first request. Or perhaps there is an API call that loads the data you need. In order to \u201ccatch\u201d these calls, you'll want to see what requests are needed to load a certain page. You can find out this and more in the 'Network' tab. This is what it looks like:", "Of course, you'll also need to know the details on individual requests. If you click on any of these you'll be able to see a lot more information such as request and response headers, cookies and the payload used when sending POST requests for example.", "Let's take a look at one of the requests needed to load the main page of google.", "\u00a0", "This is the preview of a request, you can see here information such as the status code, url and type. Clicking on this request we can get even more information:", "Both are very useful for debugging since for JSON API calls, the 'Preview' tab will display an easy to navigate structure while for HTML responses you can view the rendered response or simply the source HTML in the 'Response' tab. The search feature also comes handy sometimes.", "You can clear your browser cookies and cache from the 'Network' tab, a feature often used when testing how fast a page loads for example, also can be used to clear things such as session ids. Right-click on any request to open this dialog box:", "There are a lot of functionalities you can add for the browser using a few extensions. Our favorites for web scraping::", "You should always check how changing your IP influences the page. You may be surprised! On some websites it's important to check how changing your location affects the displayed result (can be fewer items, a redirect or simply getting blocked). For this, you can use several tools such as:", "If you have lots of extensions like me, you may wonder at some point, how these influence certain requests. You can check how your browser cache changes the loaded page (defaults, cookies, extensions, etc.) by opening the page in an incognito tab", "Conversely, you can also enable any extension you need in incognito mode from the browser's extension settings.", "Sending requests is easy.", "UI is pretty intuitive and has lots of features. You have the basic information about your request such as the URL, it's parameters, and headers to fill in. You can also enable or disable each of these as you like for testing.", "You can of course send POST requests as wee using postman, the only difference is the request body which can be specified in the 'Body' tab where multiple formats are available.", "An awesome feature is the ability to store requests for later use, you can create a collection of requests. Just create a new collection and save the request there.", "It's also pretty straight forward to configure a proxy for postman. You can use the system settings or a specific proxy with or without login from the settings panel.", "You can import curl requests into postman (such as those generated from a browser request as I mentioned before). This can save you lots of time by helping you quickly reproduce a request.", "This can also be done the other way around: export your postman request for curl or python. By default, it will use the requests library so if you need it for ", "you must change it a bit.", "Sometimes you will need to code some logic to extract the data you need. Whether it is testing selectors or formatting data, Scrapy has you covered with the ", ". This is an interactive shell designed for testing extraction logic for web scraping but it works with python code as well.", "You need to have Scrapy installed in your python environment:", "$ pip install scrapy", "Then you can simply run 'scrapy shell' to start it or you can add an URL as the first parameter and it will send a get request to that.", "$ scrapy shell", "$ scrapy shell <URL> # to start with a plain GET request", "You may now want to check the docs by typing shelp() which will display information about various available objects. One useful feature is the 'view' function which can be called with the response as parameter.", "Once you are inside the shell, you can use 'fetch' to send a get request for a given url or directly fetch a scrapy request object (which you can later use in your spiders!):", "Since now you have access to a response object you can also test your selectors. This is a basic example of ", "and ", "selectors:", "Since the shell will use your project's settings it will also make use of existing middleware so you can use these to manage session, bans and more.", "As you have seen, there are many useful tools that you can use to effectively extract data from the web. Some websites are easier than others, but leveraging these tools should make your life a little easier, even when dealing with complex websites. If you want to learn more about web scraping best practises and how to ensure you can extract data not just now but also in the long term, we have a free downloadable guide that could help you on your web data extraction journey.", "\u00a0"]},
{"tite": "Your Price Intelligence Questions Answered", "date": "July 28, 2020 ", "author": "Himanshi Bhatt", "blog_data": ["Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.", "From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.", "During our webinar on \u201c", "\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:", "A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code.", "A: Websites showcase erroneous pricing data when they detect you scraping regularly. This especially happens when you are trying to scale - i.e scrape a lot of products very frequently. Erroneous pricing is not easily recognizable, but comparing the prices or other data fields with previously extracted data and manually checking if there is a big difference in the extracted data can help.", "The long-term solution for this would be to be smarter about how you scale and be more thoughtful about the ", " you use.", "A: Scraping accurate data is all about having a reliable quality assurance process. The first step towards this process is to have a well-defined JSON schema. Your QA process needs to be a balanced combination of automated ways of testing the data as well as manual ways. This blog post gives a detailed description of ", ".", "A: For javascript heavy sites, the simplest way would be to inspect the website and see if it uses any hidden APIs that have the data in JSON or other simple formats. This way, you can get the data without executing the javascript.", "However, sometimes that may not work. In that case, you will need to execute the javascript using a headless browser like ", " or Selenium or Puppeteer. The challenge here is that it will consume more resources making the process more expensive.", "A: There are many ways to conduct product matching. The main idea would be to gather as many product-specific parameters as you can about the product that you want to match and then compare those parameters.", "Eg: For a TV, the product-specific parameters would be resolution, weight, sound, etc. If in comparison, 90% of the parameters of any two products are the same, there is a high chance that it is the same product across two websites.", "You can build models on this concept to identify product duplication.", "Want to know more about how you can fuel your price intelligence decisions with web extracted data? Watch this webinar where our Technology Evangelist Attila Toth takes you on a deep dive through the main challenges affecting price intelligence projects from both a business and technical perspective, and more importantly, how you can solve them.", "If you have any more questions or queries on Price Intelligence data extraction, feel free to leave a comment below and we will try our best to answer them.", "\u00a0"]},
{"tite": "Job Postings API: Stable release", "date": "July 09, 2020 ", "author": "John Campbell", "blog_data": ["We are excited to announce our newest data extraction API. The ", " is now out of BETA and publicly available as a stable release.\u00a0", "If you are ready to roll up your sleeves and get started, here are the links you need:", "While this blog covers most of the notable improvements & extensive testing that the API has undergone, that warrants an exit from Beta, together with some high-level uses; it\u2019s important to remember that we have already covered it ", ".", "We are moving AutoExtract Job Postings out of beta after making substantial ", ", completely eliminating several classes of errors, and making ", ". Aggregator websites where the API had a tendency to return failed requests on the BETA release have now been addressed, paving the way for widespread use.", "These changes were released to production as part of 20.5.0", "If you are looking to discern insights on the activities of organisations of all sizes, from start-ups to Fortune 100 companies, job postings can provide context for analysts to understand the market landscape. Where and how are competitors, suppliers and customers or even the industry, in general, structuring their business. Which technologies they are investing in, which ones they are no longer actively pursuing, what key markets are they pushing into, amongst other things.", "The technology stack of a start-up can (and should) look extremely different from that of a Fortune 1000 company. As organisations grow and evolve, expanding their workforce is a must to answer the on-going demands of the marketplace proactively. This is true to all sectors and industries but especially so in the context of the information technology industry, with so many roles and disciplines that need to be filled amid an ever-changing landscape.", "Imagine this scenario - an aspiring organisation within the Information Technology (IT) industry wants to expand into new markets. To do this, they need to recruit for a plethora of roles. From hiring cybersecurity professionals to either provide InfoSec support or man the Security Operations Centres (SOC), to\u00a0 DevOps/IT teams will be needed to deploy & maintain what the software engineers have developed; someone will have to project manage and someone needs to do the administrative heavy lifting.\u00a0", "IT is a competitive market to be in, and surely enough competitors are racing to have a head start. Within this context, business intelligence is paramount to gauge whether our fictitious organisations\u2019 plans are worthwhile in the first place or not. To accomplish this, their insights function needs to understand the recruitment practices of their top 10 competitors that operate a global workforce and have job listings in 100 different countries and 10 different languages - that is 10 x 100 = 1K websites to constantly monitor. With traditional manual scraping techniques, this will amount to a multi-man year-long project.\u00a0 With Scrapinghub\u2019s ", ", a steady always-on reliable steam of data into our organisation\u2019s data warehouse and BI platform can be set up in a matter of days/hours/minutes.", "Our AutoExtract Job Postings API is tailor-made to answer the demands of the recruitment industry, particularly if data synchronisation between job boards or job aggregators and the individual recruitment agencies databases where uniformity, accuracy, semantic consistency and ", " reliability is an operational must.", "Without our Job Postings API, you would need to write custom code for each job posting page to extract and parse the data. On top of that, you would also have the maintenance overhead and all the troubleshooting that comes with it.\u00a0", "We are continuously improving the underlying machine learning technology, so you can be assured you get the highest quality job data possible.", "Using the API is simple:", "Our Job Posting data API is ideal for", "Some of the data fields you get in your API:", "Here\u2019s what you need to do if you want to get access to Job Postings API:", "If you want to check out any of our other AutoExtract APIs, ", "PS: I would appreciate it if you let us know what you think of our new API in the comments."]},
{"tite": "How to Get High Success Rates With Proxies: 3 Steps to Scale Up", "date": "July 14, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["Generally, there are 3 steps needed to find the best proxy management method for your web scraping project and to make sure you can get data not just today but also in the future, long-term.", "You need to define the traffic profile first to determine the concrete needs of your project. What is a traffic profile?", "It includes, first of all, the ", " that you're trying to get data from. And also if there's any technical challenges needed to be solved, like JS rendering.", "The traffic profile also includes the ", ", meaning how many requests do you want to or need to make per hour or per day. Also do you have any specific time window for the requests, like, for example you want to make all your requests only during work hours, for some reason. Or is it okay to get the data at night, when there's significantly less traffic hitting the site.", "Then the last thing in the traffic profile is, ", ". Because sometimes the website displays different content depending on where you are. So you need to use proxies that are in that specific region you need.", "So these three elements together make the traffic profile: websites, volume and geo locations. Now, with these, you can determine the exact proxy situation that you need a solution for.", "The next step to scale up is to get a proxy pool. Based on the traffic profile, now you can estimate", "You can get access to proxies directly from proxy providers, or through a proxy management solution as well. The drawback of getting proxies directly from providers -and not through a management solution - is that you need to do the managing yourself. There are a lot of things you need to look out for if you go with a provider that doesn\u2019t provide management of proxies.", "The final step is ", ". Because it's not enough to have just a proxy pool. You also need to use the proxies efficiently. For example, some features that our smart ", " network has to manage proxies and maximize their value:", "But either you're using Crawlera, or you create your own proxy management solution there are some key points to focus on if you want long-term scalability.", "First of all, ", ". Because if you're extracting data at scale, most probably, you will not have issues with parsing HTML and writing the spider. But you WILL have issues with proxies. That's why it needs to be a priority.", "Then, if you are managing your own proxies, it's important to keep the proxy pool clean and healthy. If you use a proper management service, it's not a problem, as that handles it for you.", "Finally, my last point is to ", " to websites. Ultimately, it is a huge factor when scaling a web scraping project. You don't want to hit websites too hard and you need to make sure you follow the website's rules.", "But again, if you're using a management tool, you will have a much easier time with proxies because everything is taken care of under the hood, you just need to send requests and extract the data.", "If you want to learn more, we have webinars on the topic of ", " and also about how to ", ", where we go into more details.", "And if you want to try Crawlera, the smart proxy network, you can do it for free.", "\u00a0"]},
{"tite": "Blog Comments API (BETA):\u00a0Extract Blog Comment DATA At Scale", "date": "July 30, 2020 ", "author": "John Campbell", "blog_data": ["We are excited to announce our newest data extraction API. The ", " is now publicly available as a BETA release.", "If you want to skip the introductions and just get stuck in, here are the links you need:", "AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as ", " and more into the arena of blog comment analysis.\u00a0", "The underlying data model for the API was released to production as part of 20.6.0 release of AutoExtract.", "Customer support management presents many challenges due to the sheer number of requests, varied topics, and diverse departments within a company that might have a say in resolving the matter.", "Sourcing structured data from blog comments as provided by our API can be used in tandem with ", " solutions to quickly and effectively identify, track and act upon particular conversation strings \u2018hidden\u2019 amongst the noise of thousands of comments. You are effectively highlighting warning signs that your CX team should become involved before an incident takes place.", "Another particular powerful insight that can be derived from comments revolving around the sphere of Voice of Customer (VoC) and product analysis. By tapping into blog comments, you can search keywords for a particular product or feature or use the parsed data to train sentiment analysis model to find only the information you need.", "Without our Comments API (Beta), you would need to write custom code for each blog post to extract and parse the data. Let alone the time and overhead spend required to maintain the necessary infrastructure to deliver the data in real-time, in a scalable fashion and ", ".", "Using the API is simple:", "Here\u2019s an example response:", " for more information about the fields.", "Here\u2019s what you need to do if you want to get access to Comments API:", "If you want to check out any of our other AutoExtract APIs, ", "PS: I said it before, no better place to gain product feedback than in a blog post comment. Please share your thoughts\u00a0 in the comments below\ud83d\udc47\u00a0\ud83d\ude4f"]},
{"tite": "Real Estate: Use Web Data Extraction to Make Smarter Decisions", "date": "August 27, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["As the internet continues to grow, the amount of data it generates grows with it, opening new opportunities to improve processes and make more informed decisions. Real estate is one of the many industries that are being disrupted by data-related technologies and innovations. Whether you are a broker, realtor, investor, or property manager you have the potential to become data-driven and gain invaluable insights from web extracted data.", "In this article, you will see the many ways real estate data can help you and how utilizing web scraping can help you and your organization become disruption-proofed and fully prepared for the world of tomorrow.", "There is more public data available in the real estate market than ever. There are numerous listing sites, endless data points available for everyone to see. And if there\u2019s data, there should be a way to ", " from the data to make better decisions. But there\u2019s one big problem...", "Unfortunately, many websites don\u2019t provide ", ". Or even if they do, you might not get all the data you want only in a limited fashion. But still, the publicly available data is there, you just don\u2019t have a straightforward way to get the data. This is where web data extraction comes in. Web data extraction allows you to get this publicly available real estate data at scale. Using the correct tools or partnering with a ", ", like Scrapinghub, allows you to tap into the world of web scraping and enjoy the benefits of ", ".", "There are many situations where estimating the value of a property is necessary. Maybe you\u2019re trying to list it online for the most accurate price, maybe you\u2019re trying to get financing or you\u2019re analyzing a property before purchasing. You want to get the most accurate value of how much the property is worth.", "Being in the real estate market means that you have a lot of competition. In order to be ahead of the competition, you need to find ways to know more than others. As most realtors get their data from a single listing like the MLS, you can differentiate yourself by accessing alternative data sources. Web data extraction can help by allowing you to fetch structured ", " from any publicly available listing website. And as web scraping is a new technology for many, it can give you huge value as you will have considerably more data, and thus, information, in your hands.", "With web scraping, you can gather all the data points that exist about the given property, if it\u2019s available online. Then, you can use this data to justify your price or position your offer more accurately. Because you see the full picture through web data, you have a better chance to accurately estimate the value of a property.", "You may have heard this mantra by agents and realtors. Location is one of the key factors that determine the value of a property. Unfortunately, it\u2019s not straightforward to get access to ", "from only that specific area you want to analyze.", "With web scraping though, you can automate the process of filtering through data so you only extract data that matters to you. Or you can just grab the whole market data from the web then filter the data yourself, depending on your requirements.", "When it comes to properties, there are many numeric data points that can influence the price: square footage, age, lot size, last sold price, etc. When buying a property for yourself, emotions play a huge role in your decision making. Sometimes you are willing to pay more because you have strong emotional reasoning.", "But also, it is always important to look at the raw numbers of a property before purchasing. You can make smarter - and more logical - decisions if you first look at the raw data and make a data-driven decision. Especially when it\u2019s a property you purchase for investment purposes. Without web scraping, you cannot see the full market\u2019s prices and other data points in a structured way.", "When buying a property for investment purposes, the vacancy rate is a crucial factor that can be a dealbreaker or even one of the main reasons you purchase a property. If the vacancy rate goes down in a market, the rents are expected to increase because the demand is higher. On the other hand, if the vacancy rate goes up that means the demand is lower so the rents are expected to decrease.", "Unfortunately, many agents use a static vacancy rate when analyzing a property and disregard the actual data. They do this, simply, because they don\u2019t have time to do the research themselves. Fortunately, with the help of web scraping, it doesn\u2019t take that much time to gather high-dimensional data about the real estate market and calculate the expected vacancy rate more accurately. Collecting fresh pricing and rents data, along with recent property completions and calculating lease lengths can help you to determine vacancy rates.", "The real estate market is always changing, going through cycles. The challenge is to identify where it\u2019s going right now and where it will be in the future. ", " is important to properly value property and to make investment decisions. These insights lie in the raw data of the real estate market. It would be impossible for an individual to gather all the data manually. That\u2019s why web data extraction can provide so much value by giving you all the data there is, in a timely manner.", "Also, if you start monitoring the real estate market today with the help of web scraping, in the upcoming months and years you will have a tremendous amount of historical data. Mining this data can help you see the direction the market is moving towards and show you patterns you wouldn\u2019t have been able to recognize otherwise.", "Scraping real estate data can seem simple at the beginning. These are the general steps if you decide to ", ":", "You will need to extract a ", " if you want to get the most insights. For this, you need to ", " There are ", " you need to go through if you decide to extract data from the web at a large scale.", "Solving all these challenges takes a lot of resources and technical experience in web scraping. Unless web scraping is the core of your business (probably not), you might be better off partnering with a vendor that can solve these problems for you, so you only get the quality data but don\u2019t have to deal with the hurdles.", "Scraping real estate data has huge potential for anyone who is in the real estate market. Especially because for many this is still an untapped opportunity. Also, the web data extraction tools you can choose from have evolved a lot in the past years so either you do it yourself or partnering with a web ", " like Scrapinghub, you will be in a great position to start gaining value from public web data and get a competitive edge.", "If you want to learn more about how to make the most out of real estate data, download our whitepaper here: ", "\u00a0"]},
{"tite": "A PRACTICAL GUIDE TO WEB DATA QA PART IV: COMPLEMENTING SEMI-AUTOMATED TECHNIQUES", "date": "September 03, 2020 ", "author": "Ivan Ivanov and Warley Ferreira Lopes", "blog_data": ["In this article, we build upon some of the semi-automated techniques and tools introduced in the ", " of the series.", "Let\u2019s say that the data we work with is separated by comma and line breaks:", "change,deep-thoughts,thinking,world", "abilities,choices", "However, there isn\u2019t a consistency of how many words are separated by comma per line. To make it easier, we can transform the data set so there\u2019s only one word in each:", "change", "deep-thoughts", "thinking", "world", "abilities", "choices", "In order to make this transformation of the data we can use search and replace functionalities of code text editors such as ", ", ", " or ", ".\u00a0", "This how you can do it using Sublime Text:", "Once the process finishes, all the words will be in a single row, separated by commas:", "Finally, we replace commas with ", " - newline", "Once the replacing is done, we have a normalized dataset with only one word per line.", "Let's work again with data from ", ". Our goal for this example is to make sure that the ", " stated on the website are indeed the top ten tags present in the scraped data. The web page looks like this:", "\u00a0", "After scraping the data from the page and loading it into a spreadsheet. this is what it looks like:", "We will be using Google Sheets for this example. The first step will be to split the tags column into several columns so that we can count each word individually and analyze the results better:", "Then your spreadsheet will look like this:", "The next step will be to convert the multiple tag columns into a single one. This can be done by defining a new range:", "Then apply the following formula to our data and expanded up to total expected length (in this case 8 columns by 100 words = 800 rows in total):", "=INDEX(Tags,1+INT((ROW(A1)-1)/COLUMNS(Tags)),MOD(ROW(A1)-1+COLUMNS(Tags),COLUMNS(Tags))+1)", "When you reach the end you will see an error \u201c#NUM!\u201d as shown below:", "\u00a0", "Next, we create a new column next to the tags populated with 1:", "You can enter 1 in cells N2 and N3 and then click twice on the bottom angle of the cell to duplicate the value to the rest of the rows.", "The final step is to create a Pivot table with columns M and N thus:", "\u00a0", "Then, this should be the result:", "Finally, we can sort our data by occurrence count - this can be done from the Pivot table editor which gives us the following:", "The top tags were verified in the previous example. What if we need to open the URLs of the top 10 and visually check them or random tags? In this case, we can use a combination of tools such as:", "If we take a closer look at the tag links, they follow this structure:", "Let\u2019s say we are picking the following tags:", "change", "deep-thoughts", "thinking", "world", "To visually check them, we can:", "The result will be:", "Now we can open all of them simultaneously by using the Chrome extension we mentioned, ", ".", "After checking the results page by page comparing with the scraped data we can close the ones without a problem and keep the bad ones by:", "The final result you\u2019ll have for analysis or reporting issues will be:", "Whenever it\u2019s visible that side by side comparison could be something helpful, diff check tools (simple text comparison tools such as WinMerge) could be used. For this example, let\u2019s say we received the task to assure all category links of books from ", " were indeed scraped.", "The data would look similar to this:", "category_links", "http://books.toscrape.com/catalogue/category/books/cultural_49/index.html", "http://books.toscrape.com/catalogue/category/books/politics_48/index.html", "http://books.toscrape.com/catalogue/category/books/health_47/index.html", "...", "The website has a listing of the categories just like this:", "\u00a0", "So since it\u2019s pretty simple to select all the categories we can copy it and then simply side by side compare it with the extracted data.", "Firstly upon copying all the categories from the website, we will need to normalize it in a similar way the scraped category links will be as well. So let\u2019s order them alphabetically and transform to lower case.", "In order to transform everything to lowercase, I\u2019ll be using a built-in command called \u201cTransform to Lowercase\u201d\u00a0 for Visual Studio Code (through the command palette - open through F1 keyboard key):", "Then I\u2019ll use an extension for Visual Studio Code called ", " which upon using the available commands, will give us the following result that later will be compared to the category_links scraped data:", "Now we normalize the category_links from the data scraped removing everything else not related to the category name with search and replace approaches similar to what was shown in the first method:", "Removal of the start part of the URLs:", "Then removing the header and the final part:", "Now wrapping up with everything else (replacing \u201c-\u201d with one whitespace and removing a final leftover part):", "Then we go over to WinMerge, open a new window through CTRL + N, paste the copied content from the website in one side and the categories from the links we normalized in the other. Upon hitting F5, both will be compared and if equal, every single option shown were indeed scraped:", "Some scraping methods rely on data and structures that may be available only internally in the page source code like microdata and JSON linked data. Therefore, to make the job of assuring the data quality easier and compare with the scraped data, these can be checked with a tool called ", ".", "SQL has been a key tool in the QA Engineer\u2019s toolbelt. Let\u2019s suppose we have 1000 rows of data scraped from ", " following the below pattern to assess its data quality:", "_type", "image", "price", "title", "URL", "All products Books to Scrape - Sandbox1", "http://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg", "\u00a351.77", "http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html", "http://books.toscrape.com/", "All products Books to Scrape - Sandbox1", "http://books.toscrape.com/media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg", "\u00a353.74", "http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html", "http://books.toscrape.com/", "Apart from field names not matching nicely for the scraped data (like a \u201ctitle\u201d column actually having URLs instead), with SQL we can:", "\u00a0", "\u00a0", "However, let\u2019s suppose we expected the field \u201c_type\u201d to be the genre/category of the books:", "All data had only one value for \u201c_type\u201d and therefore could be corrected to scrape the genre of the books instead to be more useful.", "In this post, we showcased multiple semi-automated techniques which combined with the approaches shown in the previous posts of the series will, hopefully, help bring creative ideas into your data quality assurance process to test your data better.\u00a0", "Web scraping can look deceptively easy when you're\u00a0 starting out. There are numerous open-source libraries/frameworks and data extraction tools that make it very easy to scrape data from a website. But in reality it can be very hard to extract web data at scale. Read our whitepaper and learn "]},
{"tite": "News & Article Data Extraction: Open Source vs Closed Source Solutions", "date": "September 10, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["Article extraction is the process of extracting data fields from an article page and putting it into a machine-readable structured format like JSON. In many use cases, the article page that you want to extract is a news page but it can be any other type of article. Based on our experience in the web data extraction industry for over 10 years, the demand for structured article data is getting higher. There is more information available on the internet than ever. But still, having access to structured news data and being able to consume relevant and timely information can set you apart and give you a competitive edge. This is what article extraction can do for you.", "In order to extract article data from the web, you need an extraction tool. It can be a challenge to find the tool that is best suited to meet your needs and provides the functionality and data quality that you expect. In this article, we discuss the most used tools for article extraction that you can choose from.", "We\u2019re going to look at both open source and commercial solutions. Hopefully, by the end of this blog post, you will have a better understanding of the available article extraction tools and you will be able to make an educated decision whether an open-source or closed-source tool is the best for you to extract news and/or articles.", "Before looking at the tools, let\u2019s have a quick overview of what is included in article extraction and why you would extract articles.", "An article\u2019s most important data field and many times the reason you want to extract the data is the ", " field, which contains the text of the article. But other than the text body, there are many other fields you can and might want to extract.", "At Scrapinghub, we\u2019ve seen numerous web data extraction projects that included article extraction.", "As you can see, article data can provide the most value when building on top of it. But in order to achieve this, you need a reliable way to get structured article data feeds.", "An important requirement for any article extraction tool is that it needs to work for most websites without writing any site-specific code. The reason for this is that writing custom rule-based extraction code requires a lot of maintenance, especially if you\u2019re extracting data from hundreds or thousands of domains.", "Now let\u2019s look at the available tools!", "When searching for the best tool for any problem, you can be almost sure that there\u2019s an open-source library for that. That\u2019s the case for article extraction as well. Generally though, with open-source libraries, you always need to compromise on some functionality or quality compared to commercial solutions. Regarding OSS article extraction tools, there are quite a few libraries that you can choose from.", "is a library that extracts the main body text from the HTML and cleans it. This is a Python port of a Ruby port of arc90's readability project. It uses heuristics to determine which HTML elements belong to the article\u2019s body.", "is a Python 3 library that can extract and curate articles. It can also detect language automatically. It can extract a lot of fields from the article using its handy API.", "is Moz\u2019s open-source solution to extract articles. The library is based on machine learning models.", "is a Python wrapper around a Java library that removes boilerplate code and extracts text from HTML pages. It uses linguistically-motivated heuristics to determine the article body boundaries.", "extracts the full text from an HTML page. It does not try to separate the article body from the rest of the page, instead, giving all text present on the page.", "Generally, open-source article extraction solutions provide worse results than commercial ones. Even though for some use cases, they still can be good enough if unclean data is acceptable. If your goal is to just get all the content without missing anything, use ", "or a similar \u201cdumb\u201d library. Keep in mind that simple HTML to clean text conversion is surprisingly tricky. Straightforward solutions like the Xpath string() function can produce messy whitespaces, missing or additional text.", "The go-to solution for extracting information from a web page is writing some custom code to do the job. A common choice is to write code in Python which then extracts information using XPath or CSS selectors, or sometimes traversing the HTML structure directly. This works well if you want to extract information from a single website with well-defined fields (e.g. product prices) where an XPath selector might look something like \u2018//div[@id=\u201dprice\u201d]\u2019 - that is, find a \u201cdiv\u201d element with \u201cid\u201d equal to \u201cprice\u201d.", "This works well, provided the website has such an element, with the main drawbacks being the time it takes to write and test manual extraction code, and the need to update the code when the website changes. Still, it\u2019s a fine approach in many cases, although it can quickly break when you\u2019re dealing with thousands of websites.", "But such an approach often does not work well even for a single website in the case of articles, not speaking of thousands of websites. Why is that? The reason is that even when we find the HTML element which contains the article text, extracting all text from this element often leads to poor results, as this element often contains a lot of unnecessary text, such as author information, advertisements, links to \u201crelated\u201d articles from the same platform, comments, tags, forms to subscribe, social network share buttons, etc. While some of these elements could be useful on their own, they are definitely not part of the clean article text and must be removed. But the rules for removal of such elements are quite hard to define, e.g. many of the above elements could be under just ordinary \u201cp\u201d (paragraph) or \u201cul\u201d (list) tags, and such rules would be often hard to scale from one page of the same website to another.", "Machine learning shines when such rules become too hard to maintain. With ML, instead of writing and maintaining thousands of rules, we create an annotated dataset, where we can specify the desired extraction result for each page in the dataset. Then the machine learning model is trained on the dataset, automatically deriving a set of features and weights that generalize well across different pages and websites. The resulting model can be applied to a new web-site without any tuning, providing good out-of-the-box quality, and allowing to collect articles from tens of thousands of sources.", " is Scrapinghub\u2019s automatic tool to extract structured articles. News API only needs the page URLs you want to extract the article from and then delivers the structured data. Main features include:", "Diffbot\u2019s Article API extracts clean text from news articles.", "Webhose turns unstructured web content into machine-readable data feeds.", "When it comes to article extraction there are some solutions that specialize in a specific niche. These solutions, going a step further from getting you access to structured data feeds, also provide some kind of ", " or analytics on top of the data. These providers usually specialize in the financial industry where relevant and timely information is key to success.", "Some considerations for niche-specific article extraction services:", "When it comes to web data extraction, data quality is always a key factor. At Scrapinghub, we continuously monitor the quality of AutoExtract News API extraction quality against benchmarks and other tools. We also created an ", ".", " you can learn about what metrics are important when measuring article body quality and it might help you choose your article extraction tool.", "Article body ", " if your business depends on this kind of data. If you\u2019re developing a product or software that needs structured article/news data constantly, you need to make sure you choose a solution which has the best quality on the market. This is what our whitepaper helps you with.", "Commercial solutions do provide better quality, as you can see in our evaluation. Besides better quality, commercial services provide other features as well. For example, AutoExtract News API can get you cleaned and normalized HTML of the article, article author, headline, date posted, images and many other attributes. It can also handle downloading, which is a whole different aspect and significantly affects the final result.", "As discussed above, commercial solutions do provide better extraction quality, which is crucial for many use cases.", "Furthermore, a commercial service like ", " can also extract cleaned and normalized HTML of the article, including article author, headline, publishing date, images and many other attributes.", "Downloading HTML files can seem easy but in reality it has a lot of challenges like javascript rendering and proxy management. Commercial services take care of HTML downloading as well.", "Based on our research, it\u2019s safe to say that the quality of article extraction is significantly worse when using open source libraries. Even the most precise open source library provides 4.6x more unwanted content in the results while missing 2.5x more content than ", ". That being said, open source libraries still can be useful in some use cases where unclean or messy data is acceptable to meet requirements."]},
{"tite": "A Practical Guide to Web Data QA Part I: Validation Techniques", "date": "March 24, 2020 ", "author": "Ivan Ivanov", "blog_data": ["When it comes to web scraping at scale, there\u2019s a set of challenges you need to overcome to extract the data. But once you are able to get it, you still have work to do. You need to have a data QA process in place. Data quality becomes especially crucial if you\u2019re extracting high volumes of data from the web regularly and your team\u2019s success depends on the quality of the scraped data.", "\n", "\n", "This article is the first of a four-part series of how to maximize web scraped data quality. We are going to share with you all the techniques, tricks and technologies we use at Scrapinghub to extract web data from billions of pages every month, while keeping data quality high.", "The first step is to understand the business requirements of the web scraping project and define clear, testable rules which will help you detect data quality problems. Understanding requirements clearly is essential to move forward and develop the best data quality process.", "Requirements are often incomplete, ambiguous or vague. Here you can find some general tips for defining good requirements:", "In order to show an actual example, in this article we are going to work with product data which was extracted from an e-commerce site. Here is a sample of what two typical scraped records are ", " look like:", "In addition to these sample records, the business requirements - that are provided to the QA Engineer - are as follows:", "Can you find some potential problems in the requirements above?", "The stipulation on data type for field price seems at first glance to be sufficient. Not quite. Is \"2.6\" valid? Or should it be 2.6? The answer is important if we want to properly validate. \u201cWe scraped the right thing, but did we scrape it right?\u201d.", "Similarly, there are 3 different date formats which will satisfy", ". Should we report warnings for the following if scraped?", "Take a minute and try to visually validate this data based on the rules above. See how your validation fares against the automated validation techniques that will be outlined below.", "Below are some example scraped records for this scraper and its requirements. For illustrative purposes, only the ", " record can be deemed to be of good quality; the other four each exhibit one or more data quality issues. Later on in the article we will show how each of these issues can be uncovered with one or more automated data validation techniques.", "Based on the requirements outlined above, we are going to define a JSON schema which will help us to validate data.", "If a schema is not created by hand in advance of spider development, one can be ", " from some known representative records using a tool like ", ". It\u2019s worth pointing out that although such inference is convenient, the schemas produced are often lacking the robustness needed to fully validate the web scraping requirements. This is where the experience of the QA Engineer comes into play, taking advantage of more advanced features of the ", "standard, as well as adding regex\u2019s and other more stringent validation, such as:", "By default, all fields are marked as mandatory - at the end, only the ones requested from the client will be left", "In the current version of JSON schema standard, it is not possible to enforce uniqueness.. While future drafts may support it, currently it is necessary to work around this by inserting a keyword that an automated data validation framework will recognise. In our case, we will use the keyword \u201cunique\u201d.", "Some examples:", "Price:", "Date:", "This is what the final schema looks like:", "With requirements clarified and subsequently mapped to a robust schema with stringent validation, the core ingredient for automated data validation is now in place. The Python library ", " will be used as part of a broader automated data validation framework built upon the ", " and leveraging ", " for additional, more advanced data analysis.", "Given the schema and sample data defined above, the validation processes clearly shows us the data quality issues that need to be investigated:", "Let's discuss some of them in more detail:", "Although schema validation takes care of a lot of the heavy lifting with respect to checking the quality of a dataset, it is often necessary to wrangle and analyse the data further. Either to sense-check schema validation errors, discern edge cases and patterns, or test more complex spider requirements.", " is often the first port of call, for its ability to concisely:", "In the following examples, df is a scraped dataset represented as a Pandas ", ".", "Before manipulating the data, it is often useful to see a high level overview of it. One way is to list the top values for all fields using value_counts() in conjunction with head():", "In the price data point there are several problems:", "The first step is to get all prices scraped as numeric:", "prices = df.price.apply(pd.to_numeric, args=('coerce',))", "We then determine mean and standard deviation:", "And finally to find all values which are to far from the standard deviation:", "The next possible source of error is tags and names. Let's try to find if there are any cases where tags are not part of the name. In this case we will expand nested data and iterate over the values.", "tags = df.tags.apply(pd.Series)", "Then we can access the first tag thus:", "tags[0]", "In this article, our goal was to give an overview of how data extracted from the web can be validated using whole-dataset automated techniques. Everything we\u2019ve written about is based on our experience validating millions of records on a daily basis.", "In the next post in the series, we\u2019ll discuss more advanced data analysis techniques using Pandas as well as ", ", with more real world examples. We\u2019ll also give an introduction to visualization as a way of uncovering data quality issues. Stay tuned!", "Data quality assurance is just one small (but important!) piece of the puzzle. If you need help with your whole web data extraction project and you\u2019re looking for a reliable data partner, have a look at our ", "or ", " to get started!"]},
{"tite": "COVID-19: Handling the Situation as a Fully Remote Company", "date": "March 13, 2020 ", "author": "Suzanne Hassett", "blog_data": ["Scrapinghub is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers.", "Businesses all over the world are trying to adapt to the new circumstances brought on by the Coronavirus such as being forced to implement a remote working environment while retaining productivity, a huge challenge for companies that normally don\u2019t work remotely.", "We have had a lot of queries from our customers, who are doing internal global risk assessments on their supply chains being affected by COVID-19 so want to share our continued commitment to providing our customers with ongoing services during this time while ensuring a safe environment for all of our employees.", "Our internal risk models predict no more than 2% leave as a worst-case scenario, only slightly above baseline. This is likely to be offset by people deferring normal vacation leave due to travel limitations.", "Scrapinghub is at exceptionally low risk from any disruption in supplying our customers from the pandemic for two reasons:", "As a remote-first workplace, our teams are fully distributed. We don\u2019t share office spaces and most of our teams work from home-office environments. With no exposure to co-workers or public transport, the infection risk is greatly reduced. This sort of remote working and social distancing practices other companies are rushing to implement have been business as usual for us since day-1. We do use some co-working spaces from time to time but have taken a step back from those in regions where COVID-19 is community spread. We occasionally have get-togethers but none are planned for the duration of the current pandemic.", "As a globally distributed workforce, our risk is also reduced. Our workforce is spread across 28 countries, chiefly Europe, Asia, and South America. While some of the team in China are coming out on the other side of the Coronavirus disruption, others in South America live in countries that have yet to experience any cases at all. The geographic spread somewhat insulates us, as even a global crisis like COVID-19 impacts different regions in different ways at different times. Some of our developers located in China have already experienced significant local disruption and quarantines with no impact on their work.", "In terms of our own supply chain, our key input is a data center and cloud-based infrastructure. Businesses of this type are by its nature, relatively well insulated from supply chain disruption. Most of our data center services are located in the EU but we maintain relationships with multiple providers with a global footprint. Also, if the need arose, we could pivot some or all of our infrastructure to different regions.", "We would like to offer support to our customers looking for advice on remote working or managing teams remotely as COVID-19 continues to disrupt business. Please reach out to your account or project manager for more information. We wish all our customers and partners a safe and uneventful experience of the pandemic."]},
{"tite": "Transitioning to Remote Working as a Company", "date": "March 25, 2020 ", "author": "Shane Evans", "blog_data": ["I\u2019d like to echo Joel Gasgoine\u2019s sentiments: This is not normal remote working!", "Like Buffer, we\u2019ve been a remote-first company for almost 10 years and we\u2019re also adjusting to the new normal as a result of COVID-19.", "Remote teams tend to favour asynchronous written communication. That is to say, we send text messages and don\u2019t expect an immediate response. This allows longer blocks of uninterrupted working time, it allows \u201cbatching\u201d of replies and works better when people have different working hours. Amir Salihefendic makes the case for Asynchronous Communication in his excellent blog post ", ". This is one of the reasons why so many remote workers feel more productive.", "On the other hand, synchronous communication is more effective when a topic requires a lot of back-and-forths, or if there is time pressure. Don\u2019t be afraid to \u201cjump on a call\u201d to quickly sort things out. Meetings are better for brainstorming or to achieve a consensus, however, this can be more difficult remotely, especially if participants are not used to it.", "I believe that there is a tradeoff and it\u2019s best to pick the right communication medium for the message. It helps if guidelines are established around this to make it easier to understand and for everyone to agree upon. It\u2019ll also be easier for everyone if you minimise the number of tools, and don\u2019t make too many changes at once.", "Here are ", "We have experimented with different initiatives to improve informal interactions and make remote working more fun!", "The most successful initiatives are often run by diverse teams made up of people from across the company. Even if you have to get something started yourself, it\u2019s a good idea to empower others to run with it.", "Here are some of our current initiatives:", "Keep in mind that these will change over time. People will have new ideas about what they would like to do and some won\u2019t work as well after a while. Don\u2019t be afraid to try new things, but at the same time don\u2019t introduce too much at once.", "Adjusting to remote working will take time. We find good onboarding for new hires helps enormously, perhaps you could consider an onboarding session for all employees? Here are some of our tips for new starters.", "Remote companies work best when there are guides and documentation available for most things people need. These need to be well-organised and kept up to date. It prevents people from having to ask questions all the time, which can be frustrating, especially in a remote setting.", "Gitlab is a great example of a company that does this well. They have shared their ", " publicly. It\u2019s one of the key things they use to run the largest all-remote company and a fantastic resource for the rest of us. Their ", " and ", " will be particularly helpful to many people right now.", "Employee feedback is always important, but it needs to be very deliberate when working remotely. It\u2019s essential to keep your finger on the pulse of your team when trying to navigate an evolving situation like the current COVID-19 infection.", "Here are some of the things we do:", "Many managers are checking in on their team on a personal level more regularly during this time to see how they are doing. Just being there, and lending an ear to listen, can go a long way.", "Most remote working companies have a high-trust culture and value openness and transparency. Trust is a key element to make remote working work.", "If you have the urge to always check on your employees - what they\u2019re doing, if they\u2019re working - you will find remote work difficult. Managers like this need to shift their mentality to enable their employees to do quality work. It\u2019s important to be clear with your team what your expectations are and what needs to be done. Then give them the resources and support they\u2019ll need to be successful. This is the only way remote working can work. You can set a high bar on quality and productivity by managing goals and outcome, not tasks and time. Be very clear in making requests - communicate clearly what you want, by when and why you want it. More clarity in defining and delegating, and getting on a call whenever needed can be an effective substitute for the experience of office working.", "On March 30th, 2020\u00a0 I hosted a webinar around this topic. We discussed the above topics and answered many questions. This webinar is for all managers, leadership teams, or anyone who is currently transitioning to remote working and wants to ensure they remain productive in this new environment."]},
{"tite": "A Practical Guide to Web Data Extraction QA Part II: Common validation pitfalls", "date": "April 09, 2020 ", "author": "Ivan Ivanov", "blog_data": ["When you extract data from the web at scale, quality assurance is an important process to make sure your web extracted data is consistently of high quality. Validation of this data can be complex though. There are many challenges and problems that need to be addressed. In the second part of this series on web data quality assurance, we will cover most common hurdles and pitfalls in data validation and how to deal with them.", "I still remember my first lesson when I joined the team. My manager shared 3 simple questions to keep in our mind working on data validation:", "The problems will be listed in their natural appearance in a typical web scraping project.", "In the previous post, we discussed the importance of clear, testable requirements. Now let's add more details about what else could be challenging at this point.", "The QA department is responsible for defining good tests, both in terms of ", "and in terms of ", ". Usually questions like the following are asked:", "There\u2019s a lot of different kinds of data on the internet. How do you deal with unfamiliar language or structure? Validation is tricky enough in one\u2019s own language. But what can you do when you need to compare two items in Japanese, for example? Let check one example:", "Can you visually spot the differences between old and new data? If you speak Japanese, you probably can. You will recognise these as the numbers 1 to 10. If you don\u2019t speak Japanese, then visual validation is going to be much more difficult.", "Another example of complex data is a situation where the data is stored as a nested structure in a single column. This usually happens for data like promotions, variants, options or features.", "Here\u2019s an example showing a single column which contains 3 nested fields: start_time, total_items and new_items. Working with nested fields requires additional steps.", "What if we need to extract the maximum price per product? The first step is to extract nested data:", "this will result in:", "\u00a0", "Data comparison is essential for regression tests - in this case two or more datasets of scraped data for the same spider need to be compared with each other. Data comparison can be done on individual or multiple items. Let's illustrate both with examples:", "By combining Pandas with ", ", individual items can be compared using the following techniques. First we will merge the items for comparison.", "The next step is to identify which items differ between the two datasets:", "Finally we will read the different values with:", "Comparison can be done for more than one column:", "Still hard to find the differences? Then let\u2019s go one step further by highlighting differences. Remember that two things can be similar or look the same but they can still be different(browsers hide extra trailing spaces) - this will impact stats and reports.", "Comparison with highlight: custom highlighting can be applied for comparison by defining a method and applying it on the comparison like:", "\u00a0", "This can be achieved in several ways. First you need to concatenate DataFrames:", "Next step is to swap the multiindex levels:", "In order to validate two datasets against one another we need to select appropriate criteria. Normally this is done by taking categorical data and checking counts on both datasets side by side.", "The default behavior is to show only categories which match each other and omit non-matching ones. If you need to find categories which are not present in both you can use: how='outer'.", "For any given data quality issue, what is its severity or impact on the overall quality level of the dataset? There isn't a general rule here. What\u2019s important here is to present a full overview of the found issues and their extent across the dataset.", "For example, finding count of unique and non-unique values can be achieved using the unique and nunique methods:", "Duplicated items can be checked against several or all columns:", "\u00a0", "If you want to display the number impacted items you can use shape or value_counts. In the next example we can see how to use value_counts with percentage:", "When such queries are combined with a test automation framework like ", ", we get well-formatted, actionable validation results.", "\u00a0", "For well-designed and simple websites with a relatively low number of scrapeable records, verification of completeness (i.e. did we scrape all the items that could be conceivably scraped?) can be straightforward, particularly when the website provides category counts. It gets considerably more complex under the following conditions:", "Often, this completeness verification (also known as \u201citem coverage QA\u201d) cannot be automated, at least not fully. Where possible, one should take advantage of the categorisation that the website\u2019s catalogue is built around, then use relevant data points to provide indicators as to possible incompleteness.", "Next steps:", "This code will show us the distribution of two fields against each other. Several conclusions can be drawn from this table:", "\u00a0", "\u00a0", "While this post has covered some advanced data-wrangling techniques as they are part of data QA, nothing can take the place of the QA Engineer\u2019s experience in order to decide when the data is of acceptable quality or not. In the next post, we\u2019ll discuss examples of how automated and manual data QA techniques can be combined to cover all bases and allow the QA Engineer to draw a line under testing and either say \u201cQA Passed\u201d or \u201cmore work needed\u201d.", "If you are in need of web data, we know quality is important for you. Over the years, we\u2019ve developed a truly unique and first-rate Quality Assurance methodology that ensures timely and uncompromised data quality and coverage. If you have an upcoming web data project and you\u2019re looking for a reliable data partner, have a look at our", " or", " to discover how we can help you get web data!"]},
{"tite": "Custom crawling & News API: designing a web scraping solution", "date": "April 28, 2020 ", "author": "J\u00falio C\u00e9sar Batista", "blog_data": ["Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues.", "The problem we propose to solve here is related to ", " that can be available in HTML form or files, such as PDFs. The catch is that this is required for a few hundreds of different domains and we should be able to scale it up and down without much effort.", "A brief outline of the problem that needs to be solved:", "In terms of the solution, file downloading is already built-in Scrapy, it\u2019s just a matter of finding the proper URLs to be downloaded. A routine for HTML article extraction is a bit more tricky, so for this one, we\u2019ll go with AutoExtract\u2019s ", ". This way, we can send any URL to this service and get the content back, together with a probability score of the content being an article or not. Performing a crawl based on some set of input URLs isn\u2019t an issue, given that we can load them from some service (AWS S3, for example).", "Daily incremental crawls are a bit tricky, as it requires us to store some kind of ID about the information we\u2019ve seen so far. The most basic ID on the web is a URL, so we just hash them to get an ID. Last but not least, by building a single crawler that can handle any domain solves one scalability problem but brings another one to the table. For example, when we build a crawler for each domain, we can run them in parallel using some limited computing resources (like 1GB of RAM). However, once we put everything in a single crawler, especially the incremental crawling requirement, it requires more resources. Consequently, it requires some architectural solution to handle this new scalability issue.", "From the outline above, we can think of three main tasks that need to be performed:", "By thinking about each of these tasks separately, we can build an ", " that follows a producer-consumer strategy. Basically, we have a process of finding URLs based on some inputs (producer) and two approaches for data extraction (consumer). This way, we can build these smaller processes to scale arbitrarily with small computing resources and it enables us to scale horizontally if we add or remove domains. An overview of the proposed solution is depicted below.", "In terms of technology, this solution consists of three spiders, one for each of the tasks previously described. This enables horizontal scaling of any of the components, but URL discovery is the one that can benefit the most from this strategy, as it is probably the most computationally expensive process in the whole solution. The data storage for the content we\u2019ve seen so far is performed by using ", " Collections (key-value databases enabled in any project) and set operations during the discovery phase. This way, content extraction only needs to get a URL and extract the content, without requiring to check if that content was already extracted or not.", "The problem that arises from this solution is communication among processes. The common strategy to handle this is a working queue, the discovery workers find new URLs and put them in queues so they can be processed by the proper extraction worker. A simple solution to this problem is to use Scrapy Cloud Collections as a mechanism for that. As we don\u2019t need any kind of pull-based approach to trigger the workers, they can simply read the content from the storage. This strategy works fine, as we are using resources already built-in inside a project in Scrapy Cloud, without requiring extra components.", "At this moment, the solution is almost complete. There is only one final detail that needs to be addressed. This is related to computing resources. As we are talking about scalability, an educated guess is that at some point we\u2019ll have handled some X millions of URLs and checking if the content is new can become expensive. This happens because we need to download the URLs we\u2019ve seen to memory, so we avoid network calls to check if a single URL was already seen.", "Though, if we keep all URLs in memory and we start many parallel discovery workers, we may process duplicates (as they won\u2019t have the newest information in memory). Also, keeping all those URLs in memory can become quite expensive. A solution to this issue is to perform some kind of sharding to these URLs. The awesome part about it is that we can split the URLs by their domain, so we can have a discovery worker per domain and each of them needs to only download the URLs seen from that domain. This means we can create a collection for each one of the domains we need to process and avoid the huge amount of memory required per worker.", "This overall solution comes with a benefit that, if there\u2019s some kind of failure, we can rerun any worker independently, without affecting others (in case one of the websites is down). Also, if we need to re-crawl a domain, we can easily clean the URLs seen in this domain and restart its worker. All in all, breaking this complex process into smaller ones, brings lots of complexity to the table, but allows easy scalability through small independent processes.", "Even though we outlined a solution to the crawling problem, we need some tools to build it.", "Here are the main tools we have in place to help you solve a similar problem:", "With 10+ years of web data extraction experience, our team of 100+ web scraping experts has solved numerous technical challenges like this one above. If your company needs web data, but you don\u2019t have the expertise in-house, ", ". So you can focus on what matters for you the most!", "\u00a0"]},
{"tite": "Vehicle API (Beta): Extract Automotive Data at Scale", "date": "April 16, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["Today we are delighted to launch a Beta of our newest data extraction API: ", ". With this API you can collect structured data from web pages that contain automotive data such as classified or dealership sites. Using our API, you can get your data without writing site-specific code. If you need automotive/vehicle data, sign up now for a beta version of our Vehicle API.", "Whether you are interested in car prices, VIN or other car specific details, our Vehicle API can extract that data for you, at scale.", "With ", ", you can get access to all the publicly visible details and technical information about the vehicle in a structured JSON.", "Some of the data fields you get in your API:", "Our Vehicle API is the perfect choice for", "Without AutoExtract Vehicle API you would need to write custom site-specific code for each page you want to extract data from. Plus, you would also need to maintain them and handle all the upcoming technical difficulties. With our Vehicle API, you only need to provide page URLs for the API and then everything else is taken care of, like magic.", "Under the hood, Vehicle API has a machine learning algorithm that finds all the relevant data fields on the page real-time. This algorithm is constantly improved to make sure you get the best data quality possible.", "Vehicle API works the same way as other AutoExtract APIs:", "Be aware, only the site URL is not enough to extract the data. You need specific ", " to use the API! (Or ", " to get URL discovery handled for you.)", "For more information about the API check the ", ".", "Visual representation of Vehicle API:", "\u00a0", "Here\u2019s an actual JSON example of a response:", "If you decide to try Vehicle API, this is the format you should expect. ", "\u00a0", "Here\u2019s what you need to do if you want to get access to the AutoExtract Vehicle API beta:", "Vehicle API is totally free for 14 days or until you reach 10K requests (whichever comes sooner). After that, you will be billed $60/month if you don\u2019t cancel your subscription.", "If you want to try the Vehicle API Beta,", "!"]},
{"tite": "A Practical Guide to Web Data QA Part III: Holistic Data Validation Techniques", "date": "June 09, 2020 ", "author": "Ivan Ivanov and Warley Ferreira Lopes", "blog_data": ["The manual way or the highway...", "In software testing and QA circles, the topic of whether automated or manual testing is superior remains a hotly debated one. For ", " QA and validation specifically, they are not mutually exclusive. Indeed, for data, manual QA can inform automated QA, and vice versa. In this post, we\u2019ll give some examples.", "It is rare that ", "can be adequately validated with automated techniques alone; additional manual inspections are often needed. The optimal blend of manual and automated tests depends on factors including:", "When considered in isolation, each have their benefits and drawbacks:", ", when rules are clearly defined and relatively static, this includes things like:", ", on the other hand, are invaluable for a deeper understanding of suspected data quality problems, particularly for data extracted from dynamic e-commerce websites and marketplaces.\u00a0", "From a practical point of view, the validation process should start with an understanding of the data and its characteristics. Next, define what rules are needed to validate the data, and automate them. The results of the automation will be warnings and possible false alarms that need to be verified using manual inspection. After the improvement of the rules, the second iteration of automated checks can be executed.", "Let's suppose we have the task of verifying the extraction coverage and correctness for this website: ", "If you try to achieve this task in a fully manual way, then you usually have the following options:", "The same task can be easily done with an automation tool. You will need to spend some time investigating what needs to be extracted, do some tests and voila. However, there are some points to be aware of:\u00a0", "To mitigate most of the cons of the manual and automated approaches, we can tackle the task using a semi-automated approach.", "Study the page, noting that there are 10 results per page and clear pagination:", "The last page is 10.", " Open all pages. You can use browser extensions like", "If you\u2019d like to build such list you can use excel: defining a template and simple formula:\u00a0", "=$B$1 & A3 & \"/\"", "Open all links with the above extension.", "Extracting the data. Now we can extract all quotes per page with a few clicks. For this purpose, we are going to use another browser extension like ", ".", "Upon installing the extension, for example, this is how we can extract all authors:", "Select the name of the first author of the page you are in, right-click on it and then click on \u201cScrape similar\u2026\u201d:", "Then you will get the following window opened. Export it elsewhere or simply within the window, use it to compare with the data previously extracted:", "Given a scenario of having failing data quality checks towards product data extracted from the web with only tests for prices, the tools and approaches we covered so far since the beginning of our series are capable of detecting different errors like:", "That said, the automated tests can fail to validate and report the wrong price because of a combination of different factors. Just to start, the total dependence on automated validation leads to a false sense of \u201cno errors\u201d, not to mention that if crucial care is not taken such as following the steps we covered in our series so far will lead to lower-than-possible test coverage.", "The key lesson here is that even when automation is done in the best way possible, it can still fail us due to nuances on the websites, or miss some edge cases like on less than 1% of the data - that\u2019s why it\u2019s important to maintain and support automated tests with manual validations.\u00a0\u00a0", "While building a JSON Schema we can try to be so strict on the data validation rules using as much as the validation definitions rules there are to assert the data to the best possible such as the following for ", ", for example:", "However, every website has a different behaviour, many won\u2019t have the price at all whenever the product is out of stock and it\u2019s expected for our extraction tool to set the price to 0 for such cases so, what\u2019s better? Be more lenient with our tests removing the ", " validation? No! There\u2019s another approach that we can take with more knowledge of the ", " possibilities and that is conditionals:", "So with this new second schema, we can prevent both the situations of having too many false-positive errors being raised (Happens when using 1st JSON Schema shown above) and also a misleading absence of errors (if we simply removed the minimum tag for price) which could lead us to miss the extraction of price even when the product was in stock due to malfunctioning or changes on the website.", "It\u2019s clear that a manual+automated approach is the way to go.", "Let\u2019s have an example of receiving the following sample data for the extraction of ", "to assess its quality:", "Automated tests are able to catch every single one of the values that failed to be extracted that are highlighted in green below (Null/NaN values), however, the following issues in red won\u2019t be caught without an additional step of manual or semi-automated approach:", "So taking on the possibility of using e.g. Selenium and the data on hands, we can build a script to check the coverage and that if the data extracted was indeed the one available for every single one of the cells:", "Which returns us the following Pandas DataFrame:", "This combined approach allowed us to detect 4 additional issues that would have slipped past standard automated data validation. The full Jupyter notebook can be downloaded ", ".", "In this post, we showed how automated and manual techniques can be combined to compensate for the drawbacks of each and provide a more holistic data validation methodology. In the next post of our series, we\u2019ll discuss some additional data validation techniques that straddle the line between automated and manual.", "At Scrapinghub, we extract billions of records from the web everyday. Our clients use our web data extraction services for ", ", ", ", ", " and ", "among ", ". If you and your business\u2019 success depends on web data, "]},
{"tite": "Product Reviews API (Beta): Extract Product Reviews at Scale", "date": "May 19, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["We are excited to announce our next ", ". Using this API, you can get access to product reviews in a structured format, without writing site-specific code. You can use the Product Reviews API to extract product reviews from eCommerce sites at scale. Just make a request to the API and receive your data in real-time!", "In today\u2019s competitive eCommerce world, product reviews provide a great way for online shoppers to determine what products to buy. Hence, monitoring product reviews are important for businesses. Making use of reviews data, you can find insights in the data that can improve your decision making, address feedback, and monitor customer sentiment.", "But getting access to structured web data is not easy, especially if you don\u2019t have the right tools. With Product Reviews API, we provide a convenient way for you to extract reviews at scale from any site.", "More info about the fields in the ", ".", "Whichever your use case is, you can always rely on Product Reviews API to deliver high-quality data.", "Before our Product Reviews API, you needed to write a site-specific code to extract reviews or other data. Furthermore, you also needed to maintain the code if the website changed its layout or frontend code.", "With AutoExtract Product Reviews API, you don\u2019t need to write custom code to extract data. Our AI-based tool will automatically find all the data fields you need and extract it from the page. You just need to submit the target page URLs. Then, you will receive your data in a structured JSON format.", "Product Reviews API works the same way as other AutoExtract APIs:", "Be aware, only the site URL is not enough to extract the data. You need specific ", " to use the API (Or ", " to get URL discovery handled for you.)", "For more information about the API check the ", "This is the format you should expect when using the API", "Read more about the fields in the ", ".", "Here\u2019s what you need to do if you want to get access to the AutoExtract Product Reviews API beta:", "Product Reviews API is free for 14 days or until you reach 10K requests (whichever comes sooner). After that, you will be billed $60/month if you don\u2019t cancel your subscription.", "If you want to try the Product Reviews API Beta, ", "!"]},
{"tite": "Extracting Article & News Data: The Importance of Data Quality", "date": "June 23, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["Article and news data extraction is becoming increasingly popular and widely used by companies. Data quality plays a vital role in making sure these projects succeed. If the quality of the extracted articles is not good enough, your whole business could be at risk, especially if it depends on the constant flow of high quality article data.", "When it comes to web data extraction, data quality is always a key factor. Without high data quality, organizations face increased costs (", ") let alone having their competitive standing undermined. ", "If you\u2019re looking for an article extraction solution, your top priority should be data quality. You need to know which service or library provides the best article data quality. You need to learn what metrics are important when you ", ". But also - moving further from general data quality - what measures are important in article extraction and article body extraction quality.", "Article body extraction quality is crucial if your business depends on this kind of data. If you\u2019re developing a product or software that needs structured article/news data constantly, you need to make sure you choose a solution which can prove they provide the best quality on the market.", "There are many use cases for article extraction. But one thing is common in each of them: extracting articles from the web gives you a competitive advantage that many companies fail to recognize yet. Web extracted articles and news can make you", "If you want any of these skills in your arsenal, your top priority should be to choose a solution that has the best article extraction quality on the market.", "If you have products sold online, there\u2019s probably a lot of discussion around them as well. ", " their good or bad experiences of a product they bought. These mentions can decide whether future customers buy from you or they choose another brand\u2019s product. Monitoring your brand online and fueling mentions into your business intelligence can improve the way you market, promote and present your products online. It can also show you why people are buying (or not buying) your products.", "In today\u2019s competitive market, every piece of additional information about your competitors and their activities is valuable. ", " invest in competitive intelligence. It\u2019s not enough to know your product and customers, you also need to follow your market and your competitors. What they are doing, what they are up to. Fortunately, there is one thing that still has the power to ", ": data. Either you\u2019re an investor or just trying to keep track of your competitors, web article extraction can work wonders delivering competitive intelligence at scale.", "Machine learning models depend on data. The more the better. Luckily, the web offers endless amounts of data. But it\u2019s not just volume that matters. Without high quality data your algorithm is", ". Bad data quality can cause mistaken analytics, poor decision making and unreliable predictions. Web data is often incomplete, inconsistent or inaccurate. And this can be a", " for your ML project.", "Nowadays people publish", " everyday on the web. But not all news is relevant for everyone. That\u2019s why we see more and more applications and websites that specialize in ", " for readers, based on their interests. Time is a valuable asset for everybody. Using these article extraction based solutions, people can only spend time on news that actually matters for them.", "Whether you are providing ", "(Know-Your-Customer) or KYB (Know-Your-Business) services, or just want to verify a business before engaging in a partnership, getting access in near real-time to related news and articles is important.", "News has always played a significant role within the financial market, but more so with the emergence of ", ". Economic reports, financial reports or global events can immediately affect the stock market. Thus, in order to make better investing decisions, getting access to articles and news data is essential. With a constant flow of news data, you can improve your quantitative stock selection model.", "We are sharing a deep study comparing article body extraction quality provided by commercial services and open source libraries. We evaluated the quality of article body extraction for Scrapinghub AutoExtract News API and many other commercial services and open source libraries. If you\u2019re interested to learn more about article extraction quality and want to see the comparison of different services, you can ", "!"]},
{"tite": "Price Gouging or Economics at Work: Price Intelligence to Track Consumer Sentiment", "date": "June 11, 2020 ", "author": "Robert Cosgrave", "blog_data": ["As the COVID-19 pandemic took hold, we at Scrapinghub began to wonder how it would impact on the data we crawl, and whether that data could tell us something useful about the pandemic and its impact.", "Retail price intelligence is one of our key areas of interest. On a daily basis, ", " for price and stock level information. What insights might be hidden in that data?", "To explore this, we identified a basket of goods related to pandemic preparedness (eg Face Masks, medications, etc) and began to track the number of individual items available online in this set, and the average item price over time.", "In the first chart, you can see the overall price (in blue), and the number of individual items (SKU\u2019s, in retail parlance, in red) over time for a collection of US data sources. You can see there is a lot of variation in the number of items on supply as new vendors enter the market with basic items. As you might expect, the price often moves in the opposite direction, as in the dip around April 5th. Basic economics works, but the average price doesn\u2019t drop away as strongly as you might expect. Underlying demand is clearly strong. Note that we aren\u2019t looking at stock levels here, just the range of items in stock and available for purchase. Variation in the number of items available could be due to opportunistic vendors entering the market, or rebranding routine supplies with COVID-19 related keywords.", "The strength of the underlying demand is such that the overall average price on the basket increased over 40% from March 29th (when we began to track the series) onwards over two months to May 28th. COVID-19 case counts in the US began to lift off in Mid-March, and with ", " leading that we would expect the \u2018normal\u2019 basket price had already risen considerably from its pre-COVID-19 base when we began to track the data.", "\u00a0", "The interactive map shows the % increase is the average item cost between 1st April and 1st June 2020. You can see that even in a homogeneous market like the US, there is considerable state by state variation in the amount of price increases, with prices more than doubling in some states (Alabama, Wyoming) but showing very modest increases in others (Minnesota). The increases do not seem to correlate with actual observed COVID-19 case levels.", "The second chart shows the price, overall, and 3 US states, for comparison. You can see the price \u2018wobbles\u2019 quite a lot, often with different states not quite in sync. We expected to see lower prices in states with less active cases, but that doesn\u2019t seem to be the case at all, as the fluid US internal market smooths out price variation within a few days, and thus no systematic variation in price.", "In late May, something peculiar happened, causing the base price to shoot right up. It\u2019s early to tell, but it appears that the number of items available fell away quite quickly in recent days, with items at lower price points becoming simply unavailable. Thus the average item price shot up to nearly $70 as only high priced items were left in stock. We observe this effect varied regionally, with some states, like New York, having little or no impact, and others showing a price spike without much reduction in supply. We hypothesize this may be due to the recent protests and civil disorder in the US affecting supply chains in interesting ways taking certain goods out of supply in certain regions for a few days and triggering a short price shock.", "This reminds us how retail price intelligence data, combined with stock level data and delivery wait times, can give a deep insight into supply chain vulnerabilities, if you know the right questions to ask. Price Intelligence data can be used for purposes not directly connected to simply setting competitive pricing. Economists tell us price is always a signal, and our data is rich in these signals if we know where to look for them, and can interpret them correctly.", "If you are interested in extracting COVID-19 related data for research purposes, we can offer our ", ".", "If you are interested in ", " and what data we could provide to help inform your business decisions, ", "."]},
{"tite": "How to leverage alternative data in asset management", "date": "January 09, 2020 ", "author": "Marie Moynihan", "blog_data": ["Whether you are managing a hedge fund trying to find innovative sources of alpha or are an analyst looking to future proof your company\u2019s financial investments, as big data continues to disrupt the investment research landscape, getting on top of these alternative datasets as early as possible is the key to capturing the immense alpha left in this data.", "In our blog post, we will show you what alternative data is and how harnessing the power of web extracted alternative data can give insights that keep you two steps ahead of the market.", "Alternative Data in Finance is the data that has been used to gain insights in order to make better investment decisions. This data is published by sources other than the company itself. Web data is the biggest source of alternative data, helping investment managers by informing them about new and hot market opportunities and arming them with the insight needed to act quickly and develop positions carefully. Data collection is usually through traditional data sources like press releases, product releases and financial statements as well as other activities like social media and hiring.", "When it comes to extracting high-quality data from the web, there\u2019s virtually no limit to the type and quantity of data available. Our blog post takes you through a few typical data types and how it can help with asset management decisions.", "Product data is one of the most complex but important sources of data that provide critical insights into the performance of a product or a brand. Amazon, Walmart, Target, eBay etc are great examples of websites that provide thousands of data points to reveal company and category performance.", "How is product data useful?", "All this data offers lucrative opportunities for investors when determining market orders and positions.", "Taking a dive into the lengthy and often nebulous pages of a company\u2019s SEC filings can lead to amazing investment insights. It arms investors with high-quality and reliable information, which is why the demand for web scraped SEC filing data is surging.", "SEC filings have an immediate impact on stock performance. Markets are highly responsive to filings, whose submission has a direct relationship to increased trade volume. Scraping SEC filings, 10-Ks and 10-Qs produce enormous datasets, revealing non-trivial patterns in huge aggregations of data. Investors can use this information to identify rare opportunities for alpha as well as high performing outliers.", "Investment managers want first-hand information on the product information and its current performance so that they can accurately predict the stock performance. While the companies quarterly earnings can provide such insights, it may be too late to take profitable decisions.", "Scraping product reviews can allow investors to proactively gather information on a product life cycle and make more up to date assumptions about company earnings. After all, what\u2019s one of the first metrics many of us look to when deciding on a product? The reviews.", "Utilizing scraped product reviews to evaluate how a company or product is trending in the category and against its competition helps hone the perception of risk in the investment decision-making process. Negative consumer reviews can increase stock volatility and pose a serious risk to shareholder value. By extracting product reviews historically and in real-time, smarter and more timely analyses (or corrective actions) can be performed.", "Where can we get information straight from the horse\u2019s mouth? A company\u2019s website and its communications are a good place to start. Scraping data from the web may include analyzing company web traffic, announcements, and hiring activity in the aim of accessing otherwise exclusive insights.", "The following are key pieces of data to monitor:", "These data give a full picture of a company\u2019s inside track generating extremely useful insights on the company\u2019s trajectories; saving companies from making serious investment blunders.", "The rise of social media platforms like Facebook and Twitter has led to a flood of public sentiment. Public sentiment on social media gives insight into both the public at large and also any given subset we might want to glean more information about.", "Tweets have incredible predictive value. Companies announce developments on social media channels like Twitter, Instagram, and Facebook, and sometimes breaking news hit the social media channels before any other traditional mediums. Company popularity and reputation can be monitored carefully and tracked against competitors.", "With an opportunistic mindset and consistent dataset, investors can track and analyze the changes in a company\u2019s internal framework to make financial decisions.", "Harnessing the power of quality alternative data will be a decisive advantage in your search for alpha. More data and better data means your investment decision-making process produces more value, more consistently.", "Want to know more about how Alternative Data for Finance can boost your business? We are here to help! ", "."]},
{"tite": "How to use a proxy in Puppeteer", "date": "January 23, 2020 ", "author": "Attila T\u00f3th", "blog_data": [" is a high-level API for headless chrome. It\u2019s one of the most popular tools to use for web automation or web scraping in Node.js. In web scraping, many developers use it to handle javascript rendering and web data extraction. In this article, we are going to cover how to set up a proxy in Puppeteer and what your options are if you want to rotate proxies.", "In this section, we\u2019re going to configure Puppeteer to use a proxy. For this, you will need a working proxy and a destination URL to send the request to.", "As simple as that. This code will ensure that every request goes through the defined proxy. One downside with Puppeteer is that you cannot define proxies for each request in a simple way. So, the specified proxy will be used for all the requests of the browser instance.", "When you scrape the web at scale, you need to ", " to avoid bans. If you want to implement your own IP pool in Puppeteer you will realize that you can only set up proxies on browser-level (code above) and not per request. This is not ideal if you need to use different proxies for each request. ", " for more information about this topic.", "To rotate proxies in Puppeteer and to use a different IP address for each request you need a proxy server. To have a proxy server, you can implement your own or just use a ", " service for this. Be aware, implementing your own proxy server might put you into a rabbit hole where you will need to solve problems that are totally unrelated to web scraping and you can get distracted from what you really want to achieve (extract the data). So it\u2019s not recommended. But if you decide to go this way, this is an example, created with ", ":", "If you don\u2019t want to implement your own JS proxy server, you can use a ", ", like Crawlera. This is the simplest way to use proxies with Puppeteer. If you don\u2019t want to struggle with IP rotation and just want successful requests, this is how to use Puppeteer with Crawlera:", "Here\u2019s an example:", "With Crawlera, you don\u2019t have to struggle with IPs and rotation. Crawlera will take care of making your requests successful. For more tips on how to use Crawlera with Puppeteer ", ". If you want to try Crawlera for FREE, ", "!"]},
{"tite": "Best Recruitment Tips: How to Scout Top Talent", "date": "February 12, 2020 ", "author": "Eric Burke", "blog_data": ["Attracting top talent is essential for the success and growth of a company. The majority of employers will agree that finding the best talent is just as hard as it is important. Which is why, rather than waiting for the right candidate to magically fall into your lap, it's time for employers to turn towards the untapped power of ", ".", "In this blog, I try to identify the problems that the talent acquisition industry is facing and how to best solve them.", "Maybe we don\u2019t really have a skills shortage, it's more that we are looking for the right people in the wrong location.", "For many roles, you don\u2019t need to be in a specific location in order to carry out your job. Software developers, for example, can carry out their job responsibilities from anywhere as long as they have access to the necessary hardware and tools to do the job. The same can be said for many other job roles. Yet many companies will continue to search for the same type of talent as they always have - people who live (or are willing to live) near the \u201coffice\u201d and who work 5 days a week.", "To combat this problem more and more companies are adapting to the remote/smart working distributed workforce model. We, at Scrapinghub, are a remote first company. We focus on hiring the right talent and allow people to work from where they do their best work! ", " is a great option to help recruitment teams to educate themselves on understanding and searching for great candidates outside of their immediate network or region.", "When we think about the \u201cgreat\u201d companies to work for it is also reasonable to suggest that they are \u201cgreat\u201d at attracting talent. Their recruitment process will likely screen for the \u201cbest\u201d talent that they can identify. Also, they are really great at making sure people want to work for their organization.", "A strong Employer Brand is vital to an organization\u2019s ability to attract talent but what does that mean in reality?", "As an organization, you must ensure that when a prospective candidate considers working for your organization, they get a positive experience right from the moment they start researching about the company.", "If all of this is taken care of, you are in a good place. But there is still plenty more to do.", "As Talent Acquisition professionals, we not only have to ensure that we position ourselves as an Employer of Choice, but also be considerate of the various channels or platforms that we use when sourcing Talent and the language that we use when promoting our roles in these channels.", "In the job description make sure that you communicate the most important points:", "These points need to be communicated in a way that captures the imagination and naturally attracts candidates who are aligned with our culture and vision. Using ", " you can get insights into best job titles for your positions, job and Values keyword research to make sure your Job Description always appears No 1 in all search queries.", "A lengthy application form can deter candidates from applying, especially in a market where candidates have plenty of choices and it might be just easier to apply elsewhere. So keep it simple and have a good qualifying process in place that allows you to determine the best candidates. Using web data extraction you can get easy access to competitors or market application forms or processes giving you an edge on hiring trends by location.", "How responsive are you to those candidates that do reply? Every company must aspire to be better at this - ourselves included! This is our biggest concern. After all the hard work we go through to put ourselves in a position to attract talent, we can leave ourselves down badly here by not responding as efficiently as we should. Talent Acquisition teams need to move quickly to get candidates on board, so looking at your Time to Hire KPI is always good, but do not ignore the fact that while you are moving candidates successfully through a process, you should also be giving timely feedback to candidates who are not progressing.", "We use many job boards and social platforms to advertise our open positions. Some of them include Glassdoor, Twitter, StackOverflow, Github etc.", "Ensure you engage with your Glassdoor profile it is where employees and candidates are talking about their experiences with your company. If your employer rating is 2 out of 5 then you have some work to do, but likewise, if your employer rating is 4 out of 5 you still have work on your hands - you need to stay on top of it and ensure that you actively participate.", "StackOverflow, Github, and other development forums should be treated with care. These are typically pure tech forums for those actively discussing various topics in the respective communities. They do not take kindly to unwarranted recruitment traffic. But there are dedicated channels within these spaces that you can engage with so careful consideration is needed.", "Be mindful of whether social channels could give you an edge - Facebook, Snapchat, Instagram, YouTube. These are all platforms where you can produce content for your future hires and thus it would be a mistake to totally ignore them. You need to share interesting and shareable content - make it intriguing. If you can build a brand on these platforms, the people you are looking for will find you. Work with your Marketing teams on adapting some of their materials for your recruitment needs.", "A great source for talent - but the same rules apply! If you are working with recruitment agencies, then ensure that you are working with recruitment partners that know your business and are equally invested in representing your company and brand in a positive way. They need to be an extended version of you and your Talent Acquisition team.", "Your recruitment partners can be the first port of call with your candidate and they will manage the candidate experience through this process, so they need to be fully briefed on the job role and the company specifics. Recruitment agencies source and influence talent - so invest time in identifying some key recruitment suppliers that will best represent your brand and work with them as you would somebody on your team.", "Companies will need to shift their mindset from \u201cWar for Talent '' to \u201cSeduction of Talent\u201d.", "The future of hiring amazing talent is going to be revolutionised by the automation and talent pooling tools, and new candidate engagement methodologies. With web extracted recruitment data, you have a source of quality data for job listings, candidate sourcing, salary levels, market insight or competitive intelligence enabling you to make better hiring decisions in an increasingly competitive labour market.", "Recruiters will be able to find a better fit for the company from the exhaustive list of ", ". Their focus will then be on relationship building and ensuring that the candidates get a great experience throughout the recruitment process.", "The workforce today is being disrupted, and as always, the companies who adapt and pivot will be the ones that will succeed. Let our experts handle the data acquisition so that you can focus on talent acquisition. "]},
{"tite": "Looking Back at 2019", "date": "January 30, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["2019 was an exciting year for Scrapinghub. We created things we have never created before and did things nobody in our industry had ever done before. Let\u2019s revisit what happened in 2019!", "We organized the first industry event focused on ", " in Dublin Ireland. With more than 140 attendees, 16 speakers covering topics from technical deep dives to business use cases, 12 presentations, a customer panel discussion, and unlimited Guinness. It was an absolute success.", "It was an awesome event. We loved it and the people who attended loved it, with 94% of attendees giving it an overall rating of Excellent/Good. So we are going to do it again! We\u2019re going to make sure Extract Summit 2020 is going to be even bigger and better. Watch out for our call for speakers which we will launch in February.", "We launched AutoExtract API our ", " Product. At the core of the AutoExtract is an AI-enabled data extraction engine able to extract data from a web page without the need to design custom code. Through the use of deep learning, computer vision and Crawlera, Scrapinghub\u2019s advanced proxy management solution, the data engine is able to automatically identify common items on product and article web pages and extract them without the need to develop and maintain extraction rules for each site.", "Other verticals will be added to the API in the coming months such as job postings, real estate, automotive, product reviews, blogs and discussions, to name a few, will be among the verticals that we plan to roll out in 2020. It\u2019s going to be exciting! We are always looking for Beta customers to join so ", " if you\u2019re interested in any of the verticals mentioned above.", "2019 was a big year for ", ". We made huge advancements on the backend side of things, improving reliability and robustness. The success rate Crawlera provides is unmatched in the industry but still, we found ways to make it stronger and more powerful to provide our customers with the best web scraping proxy solution in the market.", "Looking at the type of successful requests our Crawlera customers rely on us for, a huge portion of the requests are e-commerce websites followed by app stores, social networks and search engines. E-commerce web data extraction for ", " was one of the top solutions requested by customers in 2019.", "Open-source is something we believe in and put a lot of effort into last year. In 2019, we open-sourced two libraries to help Scrapy developers scrape the web. ", " is a Scrapy extension to monitor your spiders, get statistics, notifications and validate data. ", " is a tool for verifying scraped data. These two repositories together got 264 stars on GitHub.", "Shane Evans our CEO made us all extremely proud when he was nominated for ", ", Ireland. This is a unique global program that recognizes entrepreneurial achievement among individuals and companies that demonstrate vision, leadership, and success - and work to improve the quality of life in their communities, countries and around the world. You can read about his story in an interview with the ", ".", "Other than holding our own event, last year we also visited ", ", the ", " (IRX), and the ", ", in London. It is always great to spread the word of the power of web scraping and showing people what it can do for them.", "We love remote working. It\u2019s in our DNA. In 2019 we got involved with initiatives such as GrowRemote and RunningRemote to help educate others on how to run a successful remote-first company. Our Head of Marketing, Marie Moynihan was invited to speak on national radio on ", "to chat about what it\u2019s like to work remotely. It\u2019s a great podcast episode, ", "We have ambitious goals for the year and this year is going to be exciting!. We are seeing increasing demand plus new use cases for web data and are excited to continue to provide solutions to our current and future customers. We will also be celebrating our 10 year anniversary!", "If you would like to be part of our journey, have a look at ", ". If you want to know how web extracted data can help your business, contact us for a ", ". In 2020, let\u2019s grow together"]},
{"tite": "How To Scrape The Web Without Getting Blocked", "date": "February 27, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["Web scraping is when you extract data from the web and put it in a structured format. Getting structured data from publicly available websites and pages should not be an issue as everybody with an internet connection can access these websites. You should be able to structure it as well. In reality though, it\u2019s not that easy.", "One of the main use cases of web scraping is in the e-commerce world: price monitoring and ", ". However, you can also use web scraping for ", ", ", ", ", ", among others.", "In this article, you will learn what are the subtle ways a website can recognize you as a bot and not a human. We also share our knowledge on how to overcome these challenges and get access to publicly available data.", "At Scrapinghub, we care about ensuring that our services respect the rights of websites and companies whose data we scrape.", "There\u2019s a couple of things to keep in mind when you\u2019re dealing with a web scraping project, in order to respect the website.", "Always inspect the robots.txt file and make sure you respect the rules of the site. Make sure you only crawl pages that are allowed to be crawled.", "When you start scraping a website you should be really careful with the manner of your requests because you don\u2019t want to harm the website. If you harm the website that\u2019s not good for anybody.", "Still, even when you are careful with your scraper, you might get banned. This is when you need to improve how you do web scraping and apply some techniques to get the data. But remember, be nice how you scrape! ", "Anti-bot systems are created to block website access from bots. These systems have a set of approaches to differentiate bots from humans. Anti-bot mechanisms can mitigate DDOS attacks, credential stuffing, and credit card fraud. In the case of ethical web scraping though, you\u2019re not doing any of these. You just want to get access to publicly available data, in the nicest way possible. Often the website doesn\u2019t have an API so you have no other option but scraping it.", "The core of every anti-bot system is that they try to recognize if an activity is done by a bot and not human. In this section, we\u2019re going through all the ways a bot can be caught, while trying to access a website.", "When your browser sends a request to the server, it also sends a header. In the header, you have several values and they are different for each browser. Chrome, Firefox, Safari all have their own header patterns. For example, this is what a chrome request header looks like:", "A bot can be easily recognized if the header pattern is not equivalent to a regular browser. Or if you\u2019re using a pattern that is inconsistent with known browsers\u2019 patterns you might get throttled or even blocked.", "When you started out with web scraping you probably had user-agents like these:", "Then, you realized it\u2019s not enough to access the page so you need to set a custom user-agent that looks similar to a real browser's. Like this:", "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0", "In the past, changing user-agent (user-agent spoofing) might have been enough to access a website but nowadays you need to do more than this.", "A more sophisticated way to detect bots is by using TCP/IP fingerprinting. TCP is the backbone of the internet. When you or your scraper uses the internet you are using TCP. TCP leaves a lot of parameters (like TTL or initial window state) that need to be set by the used device/operating system. If these parameter values are not consistent you can get caught.", "For example, if you\u2019re sending a request posing as a Chrome browser on Windows but your TTL (time to live) is 64 (maybe because you use a Linux-based proxy), your TTL value is not what it\u2019s supposed to be (128) so your request can be filtered out.", "A lot of crawling happens from datacenter IP addresses. If the website owner recognizes that there are a lot of non-human requests coming from this set of IPs, they can just block all the requests coming from that specific datacenter so the scrapers will not be able to access the site. To overcome this, you need to use other datacenter proxies or residential proxies. Or just use a service that handles ", ".", "Some websites intentionally block access if your request comes from a specific (or suspicious) region. Another case where geographical location can be a challenge for you is when the website gives you different content based on where you are. This can be easily solved by utilizing proxies in the proper regions.", "Moving away from the back-end side of things and how your scraper can be recognized as a bot on the back-end, there are some ways on the front-end as well that can get your scraper in trouble.", "Every browser needs to have javascript to render modern websites properly. Accordingly, if you scrape a website that shows content using JS you need to execute JS in your scraper as well. You can either use a ", ", or a headless browser. Regarding bot detection, If your scraper doesn\u2019t have any JS rendering capabilities you can be easily detected as a bot. The website might make your scraper perform simple arithmetic operations just to test if it has JS. Javascript can be also used for AJAX requests/lazy loading or redirection. If your scraper just sends a regular request without handling JS you will either not see the content at all or you can get recognized as a bot.", "Browser fingerprinting is a combination of browser properties/attributes derived from Javascript API and used in concert with each other to detect inconsistencies. It contains information about OS, devices, accelerometer, WebGL, canvas, etc\u2026", "If there are some inconsistencies in this set of information, anti-bot systems can be triggered and the website starts showing you captchas or makes it difficult to scrape the site in some ways.", "Back in the day, captchas used HIP (Human Interactive Proof) with the premise that humans are better at solving visual puzzles than machines. Machine learning algorithms weren\u2019t developed enough to solve captchas like this:", "However, as machine learning technologies evolved, nowadays a machine can solve this type of captcha easily. Then, more sophisticated image-based tests were introduced, which gave a bigger challenge for machines.", "The most recent versions of captchas are much more transparent and user-friendly because they are based on behavioral patterns. The idea behind these captchas is that it\u2019s transparent to the use. They track mouse movements, clicks and keystrokes. Human behaviour on a website is much more complex than bot behaviour.", "The key to handling modern captchas is to be smart about the manner of your scraping. If you can figure out what triggers the captcha for that specific site you\u2019re dealing with, solve that problem first, instead of trying to handle the captcha itself.", "Use more or different proxies (if you\u2019ve been using datacenter IPs, try to switch to residential ones). Or make requests less frequently based on how the website reacts.", "A bot is designed to be efficient and find the quickest way to extract data. Looking behind the curtain and using a path that is not seen nor used by a regular user. Anti-bot systems can pick up on this behavior. Another important aspect is the amount and frequency of requests you make. The more frequent your requests (from the same IP) are the more chance your scraper will be recognized.", "It\u2019s not an easy task to scale up your web scraping project. I hope this overview gave you some insights on how to maintain successful requests and minimize blockings. As mentioned above, one of the building blocks of a healthy web scraping project is proxy management. If you need a tool to make web scraping easier, ", ". Crawlera's rotating proxy network is built with a proprietary ", " and ", " algorithm. Crawlera will ensure your web scraped data is delivered successfully!"]},
{"tite": "Building spiders made easy: GUI For Your Scrapy Shell", "date": "March 03, 2020 ", "author": "Roy Healy", "blog_data": ["As a python developer at Scrapinghub, I spend a lot of time in the Scrapy shell. This is a command-line interface that comes with Scrapy and allows you to run simple, spider compatible code. It gets the job done, sure, but there\u2019s a point where a command-line interface can become kinda fiddly and I found I was passing that point pretty regularly. I have some background in tool design and task automation so I naturally began to wonder how I could improve my experience and help focus more time on building great spiders. Over my Christmas break, I dedicate some free time to improve this experience, resulting in my first python package ", ".", "Scrapy-GUI offers two different UI tools to help build Scrapy spiders, but today I am going to focus on the part that integrates directly into a Scrapy shell - its ", " method.", "First things first, the installation part is simple, as it is on the Python Package Index. All you need to do is install it via pip. ", ".", "Once installed, to use it in your shell you just have to import the load selector function from the module. To do this use the following code in your shell.", "The tools tab allows you to test three elements of a parser - query, parser, and processor. This little tab is where the biggest improvement to my scraping experience comes in. I can easily see all three steps of my extraction in one place, which lets me quickly observe results and make changes to best select the kind of information I am looking for.", "The Query input is compulsory and can be found in the top left. While Scrapy can handle both XPath and CSS queries, the GUI currently only lets you test CSS queries. Anything you type here returns results equivalent to running the code.", "The Regex input allows you to test regular expression filtering for your query. When you have the box checked it will update your query so it runs the code.", "Finally, the Function input lets you write python code to process the output of your query. This code can be quite complex, for example in the image below I imported the JSON package and used it to parse the content and extract specific elements.", "For the functions tab there are two requirements:", "The code passes your function through python\u2019s ", " function to define a function named user_fun, then passes the results from the query and your initial selector through it.", "I\u2019ve come to rely on Scrapy-GUI for my day to day work here at Scrapinghub. I\u2019m a bit biased here for sure, but honestly, I feel the UX from using a simple UI over a CLI is a great improvement.", "As an end-user, I would say that the function section is probably one of my favorite parts of this tab. Having to write multiple lines of indented code on a CLI shell can get very painful very quickly. For example, I would commonly forget to write a collector variable before my for loops and only noticed a few lines in, forcing me to go all the way back to the start! Copy/pasting gets even worse - I would get indentation errors quite often, especially when I am copy/pasting a function definition that had blank lines in it.", "A very close second is the results tab - being able to easily see the results of my queries without having to flood my window has been a godsend! Not just because I can easily see all the results but because I don\u2019t need to scroll up to double-check my query or processor if there\u2019s something I need to change.", "Thus ends a brief introduction to Scrapy-GUI. I hope this is enough to get you started using it when building your own Scrapy spiders. If you come across any bugs or have any feedback feel free to open an issue on its ", ", or even create a fork and submit a Pull Request!", "If you\u2019re looking to scrape the web at scale, check out our ", " that make it easy to ", " and handle ", " or ", ".", "Or If you are tired of writing your own scrapers and debugging spiders, you might want to check out our ", " and let our experts handle your data extraction needs."]},
{"tite": "Introducing Crawlera free trials & new plans", "date": "February 18, 2020 ", "author": "Pablo Hoffman", "blog_data": ["One of the biggest pain points we\u2019ve heard from our Crawlera customers last year is the inconvenience of having to jump from one Crawlera plan to another, when more requests are needed in a month. For this reason, we have been working on rethinking our Crawlera plans to better accommodate these cases and be more flexible with customers that have variable crawling requirements from month to month.", "Today, we are introducing a new group of ", " that allow you to go over your monthly quota without the hassle of upgrading and downgrading plans. You stay on a single plan and pay for overages, only when you incur them. You will also be able to limit the amount of overages you can incur into, to protect against unexpected expenses.", "We are also simplifying the number of plans to just three:", "These new plans replace the old Crawlera \u201cC plans\u201d (C10, C50, C100, C200) which are no longer available to new customers. If you are already subscribed to an old plan, nothing changes for you at this time. We will notify you if there is a change that will impact your plan.", "In addition to overages, we are also introducing ", " for Basic & Advanced plans, another popular request we\u2019ve heard in our customer feedback. We believe customers have the right to experience the great performance and reliability of our Crawlera proxy network before committing to a paid plan, and this enables that.", "Finally, users on the new plans come with a stronger configuration to achieve better performance and success rates for the most difficult websites.", "Over the coming months we will be adding more features based on the needs of customers and their feedback. If you want to get notified of any updates coming to Crawlera, go to the ", " section in our Crawlera forum and click the \u201cFollow\u201d button. If you have any feedback or questions about the new plans, please post a comment in the ", "."]},
{"tite": "News Web Data Extraction to Predict Irish Election Results", "date": "February 24, 2020 ", "author": "Cristi Constantin and Rakesh Mehta", "blog_data": ["On February 9th, 2020, Ireland elected a new parliament. Prior to the elections, the political parties invested a lot of time, money and energy to get their political message to the people. A lot of research goes into selecting the right platform and the right medium.", "In recent years, social media has gained importance, however, traditional newspaper coverage is still crucial to political parties to get their messages across. Which political party is achieving the widest newspaper coverage? Does the coverage correspond with the political parties' voter shares? These are questions of interest for the political parties which strive for more newspaper coverage, as well as for the voters as they want to know whether their preferred newspaper is biased.", "In this post, we investigate the impact of news media coverage on the results of the Irish general election. To that end, more than 200,000 newspaper articles are analyzed to find the mentions of political parties. We consider \u2018mentions\u2019 as an important indicator because it estimates the overall space that media houses give to the political parties.", "Building on this we first investigate the correlation between the media coverage and the vote share. Next, we examine coverage bias by analyzing the political coverage across different media houses of Ireland. Finally, based on our analysis we determine if specific media houses are better suited to predict the trends of the Irish elections.", "Steps we followed for our analysis:", "During the data collection, the data from articles is extracted from the popular category links where the new articles appear. However, not all these articles are on the political topic. So we first filter out the political articles by selecting the articles which mention at least one political party. This results in a total of ", "out of the articles available to us.", "For our analysis, the vote share numbers are taken from this ", ".", "The independent candidates are not included in our analysis as monitoring the coverage of independent candidates is a complex task, therefore the vote share of independent candidates is discarded in our analysis. To reduce the bias we only consider one mention for a party per article, because multiple mentions for a party in the same article do not provide extra coverage. It should also be noted that the mention does not indicate whether its a positive or a negative mention, it can be either.", "To analyze the correlation between the media coverage and the vote share we first check the media coverage number computed from our data and current vote share of different political parties.", "The media coverage for each party is defined as the ratio of party mentions w.r.t total number of political mentions.", "Fig 1. Media coverage and vote share for different parties", "It can be seen that the vote share for different parties, in general, is quite proportional to the media coverage for these parties. It can be observed that the two main parties, Fianna Fail and Fine Gael, get a bit of disproportionate coverage. These are the only parties that get more media coverage than their vote share, for all other parties the vote share is relatively higher than the media coverage.", "To quantify our analysis we calculate the correlation coefficient between the media coverage and the vote share. Pearson coefficient is a number between -1 and 1, which in this case tells us how closely is vote share correlated to media coverage. A value close to 1 indicates a very high correlation while a value close to 0 indicates no correlation. For our data,", "From the overall coverage, it can be seen that most of the coverage is dominated by three main political parties. Is it the case for all the media houses or do some media houses have a preference towards some specific parties? To understand this we check the coverage/vote-share charts for different media houses. The results are shown for the popular newspapers in Ireland along with the number of articles analyzed from each.", " (Total articles 4738)", " (Total articles 1919)", "Total articles 1521)", " (Total articles 635)", "Total articles 374)", " (Total articles: 62)", "It is interesting to note that the ", " gives a relatively high coverage to the Fine Gael, while ", " provided higher coverage to Sinn Fein when compared with its vote share.", "We have seen that there is a high correlation between media coverage and the vote share of the parties. If we are interested in predicting the trends for the next election should we consider the coverage from all the media houses together or is coverage from specific media houses more accurate for prediction? In order to determine this we again consider the Pearson coefficient, but this time we calculate for each newspaper separately.", "Fig 8. Pearson Coefficient calculated for each newspaper.", "There are few media houses whose media coverage is quite highly correlated to the vote share. For instance, ", " has a correlation coefficient of 0.99410 which is quite high compared to correlation coefficient of the combined coverage. Therefore, for the prediction of next election it would make sense to also consider the coverage of the individual media house and not just the combined coverage.", "Even though in this post we only consider the mentions of the political parties for our analysis, more interesting conclusions can be drawn by doing ", " of the article body and article comments. Furthermore, machine learning models can be trained to perform predictive analysis for the elections using information such as article body, comments, date of publication, sentiment score, etc. Our initial analysis done over large scale data shows intriguing insights, in the future, we plan to extend it by applying more advanced machine learning models on top of the extracted data.", "Although the elections in Ireland are over, there are a number of cases where relevant information can be retrieved from articles. Extraction, when done at scale, has the potential to reveal interesting insights from a plethora of data available out there.", "If you need to extract news or other article data, but you don't want to deal with the technical challenges, ", "."]},
{"tite": "Extracting clean article HTML with News API", "date": "March 12, 2020 ", "author": "Iv\u00e1n de Prado Alonso", "blog_data": ["The Internet offers a", " in the form of ", ", news, blog posts, stories, essays, tutorials that can be leveraged by many useful applications:", "But anyone interested in using all this data available, will face some challenges.\u00a0", "Web pages are built of many components (menus, sidebars, ads, etc) and only a few of them represent the true article content, the actual valuable information. Being able to ", ". Especially when you want to obtain this information from a diverse set of sources that can have a different structure, styling and even be in different languages.", "This challenge is not minor as it is very common to find irrelevant content not only outside of the article body but also within the body itself: elements like ads, links to content not directly related with the article, call to action boxes, social media buttons, etc are very common nowadays.\u00a0", "Articles content ", " like images, figure captions, videos, tables, quotes, tweets, etc. A lot of meaning is lost if we only focus on plain text. Converting the content to some standardized and simplified format that is independent of the source page is required to leverage all this rich content. Having such a standard format would open the door to apply the same styling rules for any content, independently of the source. What is more, it would provide the flexibility to enable/disable particular components of the articles or even rearrange them. But converting the diverse content into this format is a ", ".\u00a0", "Relying on HTML for that is a good starting point but it is so flexible and it is used in so many different ways that simplifying it to a standard set of content elements is a titanic effort. The following case can serve as an example: the HTML tag ", " is the right way of annotating figure captions in the pages according to the HTML spec, but only a fraction of pages with figure captions really use it. Instead, they might use some ", " tags marked with some class (each page uses its own classes), or they might use a table structure to include both the image and the caption. So identifying figure captions within a page is a difficult problem. The same difficulties can be seen with other elements, like block quotes.", "Our ", " deals with all these challenges, offering an extraction service for articles where all the content is cleaned up (no irrelevant content on it) and served in a standard format for any source page. ", " are extracted (headline, author, publishing date, content in text format, etc) but the rest of this post will focus on the advantages of the attribute ", " which is offering the article content in a rich and standardized format as a subset of the HTML.\u00a0", "The full description of the AutoExtract ", " format can be found in the ", " but let\u2019s introduce here the main concepts:", "As a result, ", " attribute returns a clean version of the article content where all irrelevant stuff has been removed (framing, ads, links to content no directly related with article, call to actions elements, etc) and where the resultant HTML is simplified and normalized in such a way that it is consistent across content from different sites.", "The resultant HTML offers great flexibility to:", "But that\u2019s enough theory for now. The following sections will show some examples. You can also ", " if you want to experiment and play with them by yourself.", " is a simple REST API that can be accessed from any programming language. In this case, we are going to interact with the API using the ", ". Once installed, one way of performing requests to extract articles from URLs is by invoking the function ", ". The following function will take care of making requests:", "Let\u2019s perform one request over ", ":", " contains now a dictionary with all the ", " (headline, author, etc). Here we are going to show what is the content of the attribute ", ":", "Note that only the relevant content of the article was extracted, avoiding elements like ads, unrelated content, etc. AutoExtract relies on advanced machine learning models that can discriminate between what is relevant and what is not.", "Besides, note that figures with captions were extracted. Many ", ".", "Having normalized HTML code has some cool advantages. One is that the content can be formatted independently of the original style with simple CSS rules. That means that the same consistent formatting can be applied even if the content is coming from very different pages with different formats.", "Now let's see how the extracted article looks like after some CSS style rules are applied:", "It looks better, doesn't it? And the best is that this style (with a little bit more of work) would work consistently across content from different websites. These are the CSS rules applied:", "The very same CSS style sheet is going to be used for all the following examples. Note how this single styling is working consistently across content coming from diverse websites.", "AutoExtract is friendly with embedded content like social network content, videos, audios, interactive widgets, etc. In general, all content that was embedded using an ", " tag will also be included in ", ". This is covering many cases, like Youtube, Google Maps or Vimeo. There are other cases like Instagram, Facebook and Twitter that are integrated in such a way that the content is ready to be rendered using the corresponding javascript library provided by the vendor. In other words, if you want the content from Twitter, Facebook and Instagram to look pretty, you only have to ", " to your page or app.\u00a0", "In the following example, we are applying the Twitter javascript library ", " to the article extracted from ", ":", "\u00a0", "Another advantage of having a normalized structure is that we can pick only the parts we are interested in.", "In the following example, we are going to just pick the images from ", " with its corresponding caption to compose an image array.", "Let's see it working for ", ":", "The result is:", " and ", " libraries were used as helpers for the task. parsel makes it possible to query the content using XPath and CSS expressions and html-text converts HTML content to raw text.", "Note that there is not any ", " tag in the source code of the page in question: AutoExtract machine learning capabilities can detect that a particular section of the page is really a figure caption even if it was not annotated with the right HTML tag. Such intelligence is also applied to other elements like ", ".", "Let's go further. We are now going to compose a summary page that also includes independent sections for figures and tweets. It is really easy to cherry-pick such elements from articleBodyHtml. Let's see it applied to the Musk page:", "Let's apply it the Musk page:", "This is the result:", "The ", ": you might decide to exclude figure captions or to exclude multimedia content from ", ", or show figures in a separate carousel for example.", "The textual attribute ", " does not include any text from figure elements (i.e. figure captions) by default. This is generally desired because images cannot be included in raw text and showing a caption without its figure is disturbing for humans.", "But sometimes the body textual information is used as the input for some analysis algorithm. For example, you could be grouping articles by similarity using the simple technique of K-Nearest Neighbours. Or even you can be feeding very advance neural networks using deep learning models for NLP.", "In all these cases you might want to have the textual information for figure captions included. It is very easy to do. The following does it for the United article:", "The following does it for the United article:", " are being used very often in articles nowadays. A pull quote is an ", " of the article content ", " within the article ", "with a different format (i.e appearing in its own box and using a bigger font). A pair of examples can be seen ", ".", "Pull quotes are a nice formatting element, but it might be better to strip them out if we are converting the document to plain text because having repeated content should be avoided here: formatting is lost in raw text and therefore pull quotes are not useful but disturbing for the reader. The attribute ", " already contains a text version of the article, but pull quotes are not removed there. In the following example, we are going to convert the article to raw text but excluding all pull quotes.", "Note that AutoExtract detects quotes using machine learning techniques and returns them in ", " under ", " tags.", "The result is:", "Finally, we can obtain the full text but with pull quotes stripped out:", "Let's verify that we have removed the duplicated text:", "The output is:", "The former code works regardless the website used. Having it applied to a different page is just a matter of changing the URL.", "During the course of this post, we tried to show that News API is a powerful web data extraction tool to extract content from articles and news. But there is nothing as convincing as trying it by yourself. If you want to experience how easy it is to extract news and article content, ", "After that, you can also go to", " and run the examples.", "The source code used in this article can be found in ", "."]},
{"tite": "Job Postings Beta API: Extract Job Postings at Scale", "date": "March 05, 2020 ", "author": "Attila T\u00f3th", "blog_data": ["We\u2019re excited to announce our newest data extraction API, ", ". From now on, you can use AutoExtract to extract Job Postings data from many job boards and recruitment sites. Without writing any custom data extraction code!", "Aside from e-commerce products and news extraction, one of the most demanded web data types is job postings. Job Postings API enables you to get real-time job postings data, at scale.", "If you want to learn more about the activities of fortune 100/500/1000 companies, looking at their job postings could be an essential element of your research. It can indicate what direction the companies are heading and give you outside insights on what technologies they are investing in. Job postings can also be a great addition to your business intelligence (BI) data sources.", "By monitoring job postings, you can also get a clear understanding of what direction your competitors, partners or suppliers are moving. Getting access to job postings data can help you determine what markets your competitors are going after.", "Our Job Posting data API is ideal for", "Some of the data fields you get in your API:", "Without our automated tool, you would have to write custom code for each job posting page to extract the data. Then, you would also need to maintain them and handle the technical difficulties. Our gamechanger Job Postings API lets you focus on the data and not on extraction.", "With the Job Postings API, you can extract data from any job posting page, real-time. Just provide the page URLs; you won\u2019t need to write custom code to extract data.", "We are continuously improving the underlying machine learning technology, so you can be assured you get the highest quality job data possible.", "The way it works is easy:", "Here\u2019s an example response:", "Here\u2019s what you need to do if you want to get access to Job Postings API beta:", "If you want to try the Job Postings API Beta, ", "!"]},
{"tite": "Price intelligence with Python: Scrapy, SQL and Pandas", "date": "October 08, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["In this article I will guide you through a web scraping and data visualization project. We will extract e-commerce data from real e-commerce websites then try to get some insights out of it. The goal of this article is to show you how to get product pricing data from the web and what are some ways to analyze pricing data. We will also look at how price intelligence makes a real difference for e-commerce companies when making pricing decisions.", "This is the simple process we are going to follow for this article:", "In a real life project you\u2019d probably know which websites you want to get data from. For this article, I\u2019m choosing some popular European e-commerce stores.", "When scraping product information we have endless amount of data types we could get from an e-commerce site: product name, product specific attributes, price, stock, reviews, category etc. For now, we will focus on four fields that have the potential to give us the most interesting insights:", "Before we start writing code to extract data from any website, it\u2019s important to make sure we are scraping ethically. First, we should check the robots.txt file and see if it allows us to visit the pages we want to get data from.", "Example robots.txt:", "Some things you could do to be compliant:", "This is the part where we fetch the data from the website. We\u2019re going to use several modules of the Scrapy framework like Item, ItemLoader and pipeline. We want to make sure that the output is clean so we can insert it into a database for later analysis.", "We are using Scrapy, the web scraping framework for this project. It is recommended to install Scrapy in a virtual environment so it doesn\u2019t conflict with other system packages.", "Create a new folder and install virtualenv:", "Install Scrapy:", "If you\u2019re having trouble with installing scrapy ", ".", "Now that we have Scrapy installed in our environment we can create a new Scrapy project:", "This will generate the file structure:", "\u00a0", "Before we can write the spiders for each website, we have to create an item in the ", " file which contains the previously defined data fields. One ", " will represent one ", " and hold all its data.", "Each of our spiders will look the same except the selectors of course.", "To create a spider, first we should look at the website and its source code, for example:", "This is the html of one e-commerce website I\u2019m scraping and this part contains the name and price information. One thing to look out for is the ", " attribute. Many e-commerce sites use ", ". In the source code above we have", " which contains the product name and ", " which contains the product price.", "Selecting data fields based on ", " attributes gives us a better chance that the scraper won\u2019t break in the future when the website layout changes.", "I\u2019m using ", " with a default input processor to remove html tags. As you can see, I\u2019m selecting the category field from the breadcrumb.", "If we want to run analysis on our data we need to store it in some kind of database. For this project, I\u2019m using a MySQL database for storage. If you want to use MySQL as well you should install ", " if it isn\u2019t already installed:", "Then in Scrapy we create a new class called ", " in the pipelines.py file:", "In this class we have several things to do:", "Technically, you could also hardcode your database connection info in the pipeline but I suggest putting it into the settings file like this:", "Now we only have to activate this pipeline in the settings file:", "We have focused on how to extract e-commerce pricing data now let\u2019s look at some basic ways you can analyse it and get actionable insights. In this section I\u2019m going to introduce some basic ways to analyze price data and how to get actionable insights from it. I\u2019m using ", " and SQL queries on the backend to get the data from the database. To generate the charts on frontend, I\u2019m using ", ".", "One important analysis is the price history of one product. It shows you how one product was priced in the past. This could be one way to help determine the pricing strategy of an e-commerce store. Obviously, for this, you need to scrape their data regularly for a longer time. But when you actually have access to the data you can see how their pricing has changed or not changed in the past. It\u2019s also interesting to see what pricing strategy they use on important shopping days like black friday.", "If you look at the history chart above, you can have a good understanding of how you and your competitors set the prices for one product. Based on the past you could forecast how the competitors will change their prices in the future so you can adjust your strategy to prepare for it.", "One of the key factors when shopping online is the availability of the chosen product. Maybe I\u2019m willing to wait a week or two till the product I want to buy is in stock again but most of the time I want it in my hands as soon as possible and maybe even pay a little more as well just to get it faster.", "To use this dynamic to our advantage, we can scrape the stock information from the product page and get alerted if all of our competitors are out of the given product so we can increase the price.", "On a day-to-day basis, maybe the best insight price intelligence can give us is the overall view on the market and how our products fit in. Without web scraping we would have a hard time knowing how our main competitors are pricing the same products as we sell.", "On a higher level we can analyze how many of our products are priced lower, the same or higher than each of the competitors. On the chart below, we have 34 products that have a higher price than competitor3 and 9 products with lower price than competitor5.", "For example, we might want to position ourselves to have higher prices than one of our competitors or we want to be lower than another one. This kind of analysis can help you adjust prices accordingly.", "When we were writing the spider we extracted the category of the products as well. This way we can also group together products based on category.", "This chart shows what our price position is compared to the competitors in each product category. As you can see, here we have 12 products in the \u201cCameras\u201d category where we have the lowest price. In the other categories we are either in the middle or highest price position.", "So this is the process of how you can scrape e-commerce websites and get actionable insights from the data using python and some visualization. First, you plan what data fields you exactly need and from what websites. Second, you create the web spiders to extract and store the data. (If you don\u2019t want to struggle with selectors/xpath, use an AI-based web scraping tool, like ", "). Finally, you visualize the data to understand it and find business opportunities."]},
{"tite": "Web Scraping Questions & Answers Part I", "date": "October 10, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["As you know we held the ", " last month. During the talks, we had a lot of questions from the audience. We have divided the questions into two parts - in the first part, we will cover questions on Web Scraping at Scale - Proxy and Anti-Ban Best Practice, and Legal Compliance, GDPR in the World of Web Scraping. Enjoy! You can also check out the full talks on these topics ", "A: As both antibots as well as bot developers have access to similar tools, there will be a constant ebb and flow and never a complete stop to web scraping.", "A: We handle all types of datacenter proxies from multiple providers to ensure a diverse pool for every use case. Our proxy pools are in the order of hundreds of thousands, while that's an important figure, we are constantly focusing on delivering successful responses to our customers.", "A: While Crawlera does provide browser profiles, going forward there will be more features built under the hood which will help with more sophisticated anti-bots.", "A: By crawling responsibly. Using ", ".", "A: We use headless browsers and all browsers can be detected as bots.", "A: It requires careful inspection of the response body, headers and at times the entire network traffic to understand the behaviour of the underlying anti-bot. We do use some internal tools to identify the type of detection used. There are also open source tools like \"don't fingerprint me\" which allow you to assess the browser fingerprinting used by the website.", "A: Yes they are and antibot companies do have a signature directory which is able to identify inconsistencies in the request headers.", "A: While we cannot name them, there are several examples of ecommerce websites faking the data. It is thus important to perform QA and look for these anomalies by checking past data.", "A: Cloudflare and Akamai are quite ubiquitous and are encountered frequently on websites. They come in various flavours so there are a whole host of different approaches to scrape such websites right from proxy rotation to get around geoblocking to using headless browsers.", "A: By default Scrapy does not handle CloudFlare challenges, you need to write code in order to do so.", "A: Yes, HTTP headers are case-sensitive.", "A: Yes, it's an important value to consider. Crawlera handles the optimal rate depending on the concurrency input from the client, how the proxies are performing and how the site is responding.", "A: ", " has a proven throttling algorithm that uses the sites' stats and concurrency from the customer to ensure a reasonable rate is used when targeting the site.", "A: Yes, there are many default ban rules that can be automated, while others such as captchas could be more customized. The different mechanisms to obtain successful responses are rotating proxies, do proper throttling and making sure requests are neat.", "A: Handling different CDN's or antibot mechanisms depends mainly on every site. Using proxies is one big part of it, while ensuring client is following user based patterns of crawling is important as well.", "A: Yes, recycling is an important part of the process. Usually web servers unban proxies after a period of time.", "A: The UK Information Commissioner's Office provides the following guidance: The government intends to incorporate the GDPR into UK data protection law when the UK exit the EU \u2013 so in practice there will be little change to the core data protection principles, rights and obligations found in the GDPR. The EU version of the GDPR may also still apply directly to a UK company if it operates in Europe, offers goods or services to individuals in Europe, or monitors the behaviour of individuals in Europe.", "A: If the US company is not established in the EU and is not targeting the EU market then the processing will not come within the GDPR.", "A: A company email address or phone number will come within GDPR if it is considered to be personal data. GDPR defines personal data as \u201cany information relating to an identified or identifiable natural person.\u201d A generic company contact email such as sales@scrapinghub.com is unlikely to be considered personal data under GDPR. The email address of an individual working at that company will be considered personal data if it that individual is identifiable, for example, if the email address contains that individual's name. If you are scraping personal data you need to have a lawful basis to do so under GDPR.", "A: When personal data is collected from other sources (i.e. not collected directly from the individual by you), there are certain exceptions to your obligation to provide privacy information which may apply, for example, if providing the information is impossible or involves a disproportionate effort. If you intend to rely on one of these exceptions you must carry out a Data Protection Impact Assessment.", "A: A data controller determines why and how personal data is processed. If you are processing personal data on behalf of a data controller then you are a data processor under GDPR. Both data controllers and data processors must comply with GDPR and have obligations under GDPR. However, a data processor has fewer obligations than a data controller.", "A: The principle of data minimization under GDPR will not apply if no personal data is being processed. However the data collection may be subject to other non-GDPR related restrictions.", "A: Yes usernames may be personal data. If an individual is identified or identifiable from a username then it will be considered to be personal data under GDPR.", "\u00a0", "If you have any more questions or queries on the above discussed topics, feel free to leave a comment below and we will try our best to answer them. In our next post we will cover questions on The Next Generation of Web Scraping and How Machine Learning can be used in Web Scraping. Stay tuned! Also you can access the recordings of the ", " talks."]},
{"tite": "How to use Crawlera with Scrapy", "date": "November 14, 2019 ", "author": "Attila T\u00f3th", "blog_data": [" a ", ", specifically designed for web scraping. In this article, you are going to learn how to use Crawlera inside your Scrapy spider.", "Crawlera is a smart HTTP/HTTPS downloader. When you make requests using Crawlera it routes them through a pool of IP addresses. When necessary, it automatically introduces delays between requests and discards IP addresses to help manage crawling challenges. And simply like that, it makes a successful request hassle-free.", "In order to use Crawlera you need to have an account with ", ". If you haven\u2019t signed up yet you can ", ", it\u2019s free. When you subscribe to a plan you will get an API key. You will need to use this API key in your Scrapy project to use Crawlera.", "First thing you need to do is to install the Crawlera middleware:", "Next, add these lines to the project settings:", "By using the middleware you add ", " to your project that you can configure. These settings can be overridden in Scrapy settings. For example it\u2019s recommended to set these:"]},
{"tite": "4 Sectors That Benefited Most from Business Intelligence Software", "date": "December 04, 2019 ", "author": "Louie Andre", "blog_data": ["Data moves around the marketplace. It can be sourced internally or externally and collected from vendors, manufacturers, retailers, wholesalers, consumers, and other players in the marketplace. This data is then processed and used by businesses in making insights and decisions regarding new business ventures, product ideas, conflict resolution, and process improvement.", "The data collected by companies is the input to business processes that transforms unstructured data into a ready-for-interpretation form that can be used by the appropriate decision-makers in each department of the organization.", "A build-up of data is a common problem for most organizations. They face issues on how to effectively store, update, maintain, and dispose of company information. Without the right tools, a business will incur unnecessary and excessive expenses for handling too much insignificant information.", "When company data is managed correctly, business outcomes will be favorable to the organization. When incorrectly managed, it may result in business failure and bankruptcy due to excessive spending on the management of voluminous data. That said, you can maximize data to ", ".", "Business intelligence software is an important tool that every business should possess. It assists businesses in collecting, arranging, processing, maintaining, retrieving, and disposing of company data.", "Business intelligence platforms are used to effectively and efficiently assist the decision-makers of the firm. It coordinates information from all the other systems of the company and transforms it into outputs helpful for making decisions. The decision-makers are the primary end-users of the information processed by the business intelligence software.", "The data gathered by the company from various sources are processed and converted into a helpful and understandable version through the implementation of business intelligence tools and techniques for decision-makers", "Data that is building upon the system of business is in raw form. This means that for it to be useful for decision-makers, it should first undergo a process to transform it into a form good for making informed decisions. The data analytics tools included in the business intelligence software help process and convert these raw data into more comprehensible forms.", "The combinations of tools used for business intelligence are numerous. The tools included in business intelligence software depend on the type of service provider. There are many to choose from in the online market world. Thorough research and planning are necessary before spending money on business intelligence software.", "Many sectors are participating in a competitive environment. They have unique needs that may be resolved through the integration of business intelligence software in their business processes.", "The following are some of the sectors that benefited most from business intelligence software:", "In the retail industry, the main problem of most businesses is theft. With that, effective inventory management is critical.", "Business intelligence software can help retail businesses by handling the inventory of the organization. Through the use of the software, information regarding the inventory levels of businesses per storage location or warehouse can be monitored on a real-time basis. This can help alert the managers when to request additional supplies when inventory levels are approaching critical levels in a certain warehouse.", "The real-time reporting capability of the software helps managers make timely and relevant decisions. The business intelligence software may be used to help the decision-makers of the firm to re-evaluate the strategies set for the inventory management of the organization.", "Data management is an important component in the business processes in the telecommunication business. Data storage is a common challenge for businesses that functions around high volumes of data.", "The number one reason for integrating the business intelligence software in an organization\u2019s business processes is its ability to effectively and efficiently handle large amounts of data. It can process and retrieve data from the different departments of the organization.", "Information relay from one department to another is easier and more convenient because the business intelligence software can manage the storage and flow of information to the different departments in an organization.", "There is little to no error in the transfer of data from one department to another, and other stakeholders of the company because of the high accuracy of data that flows into the different systems in the business intelligence software.", "The business intelligence software also helps with the company in acquiring data from various sources which can be utilized for needs assessment and customer retention and loyalty strategy.", "As one of the most challenging industries, its market is a highly competitive ground for various players. The analytical tools used in business intelligence software helps segregate businesses\u2019 target markets from the entire population. It primarily assists the business in making effective decisions.", "Another challenge in this industry is its sales goals achievement.", "The business can improve its sales outcomes through the assistance of business intelligence software. It can provide helpful outputs such as reports on the performance of sales personnel and their efforts to sell the products of the company. Through this information, the decision-makers in the sales department will be given an insight into the performance of the sales staff. This is important because sales staff is the first line of people who personally interacts with the company\u2019s customers.", "The real-time information provided by the business intelligence software will allow the decision-makers in the sales department to have a real-time view of the progress of the department towards their sales goals for a certain period. This helps the managers of the department of sales to monitor the progress and their position in the targets of the team for a certain period.", "Agencies that provide various businesses with human resources need a powerful tool in handling company personnel.", "The business intelligence software is connected to the different systems being mobilized by the organization. The software can assist the management in storing and organizing employee information such as personal information, certifications, qualifications, position in the company, and more. This software also helps an organization to retrieve information in a matter of seconds to minutes with a click of the mouse.", "The business intelligence software may also help the HR managers in evaluating the performance of employees through processing data inputs to the system. This will help provide the managers with timely information regarding whether there is a need for additional employee training or promotion.", "The business intelligence software may also help these businesses monitor staff who are assigned from far and remote places.", "According to the ", ", by the year 2023, the market for big data analytics is expected to reach $103 billion. This proves the increasing trend of the need for a competitive data management tool for businesses.", "Business intelligence software is a useful tool for businesses that aim to have an organized and efficient data search such as helping businesses", "find and store data from various web sources. It can also assist a business to achieve profitability goals like helping develop effective product pricing schemes through web searches.", " assists enterprises manage company data and transforms these data into helpful outputs for stakeholders to make informed decisions.", "Business intelligence software can benefit multiple business processes in an organization. With that, the sectors that benefit most from business intelligence software are the retail industry, telecommunication industry, fashion industry, and the human resource industry.", "\u00a0"]},
{"tite": "Scrapy & AutoExtract API integration", "date": "October 15, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["We\u2019ve just released a ", " which makes it easy to integrate AutoExtract into your existing Scrapy spider. If you haven\u2019t heard about ", " yet, it\u2019s an AI-based web scraping tool which automatically extracts data from web pages without the need to write any code. ", ".", "This project uses", " and", ". A", " is strongly encouraged.", "This middleware should be the last one to be executed so make sure to give it the highest value.", "These settings must be defined in order for AutoExtract to work.", "Optional", "AutoExtract requests are opt-in and they must be enabled for each request, by adding:", "If the request was sent to AutoExtract, inside your Scrapy spider you can access the AutoExtract result through the meta attribute:", "In the Scrapy settings file:", "In the spider:", "Example output:"]},
{"tite": "Scrapy, Matplotlib and MySQL: Real Estate Data Analysis", "date": "November 07, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["In this article, we will extract real estate listings from one of the biggest real estate sites and then analyze the data. Similar to our previous ", ", I will show you a simple way to extract web data with python and then perform descriptive analysis on the dataset.", "We are going to use python as a programming language.", "Tools and libraries:", "Although this could be a really complex project as it involves web scraping and data analysis as well, we are going to make it simple by using this process:", "Let\u2019s start!", "For every web scraping project the first question we need to answer is this - What data do we exactly need? When it comes to real-estate listings, there are so many data points we could scrape that we would have to really narrow them down based on our needs. For now, I\u2019m going to choose these fields:", "These data fields will give us freedom to look at the listings from different perspectives.", "Now that we know what data to extract from the website we can start working on our spider.", "We are using Scrapy, the web scraping framework for this project. It is recommended to install Scrapy in a virtual environment so it doesn\u2019t conflict with other system packages.", "Create a new folder and install virtualenv:", "Install Scrapy:", "If you\u2019re having trouble with installing scrapy check out the ", ".", "Create a new Scrapy project:", "Now that we have scrapy installed, let\u2019s inspect the website we are trying to get data from. For this you can use your browser\u2019s inspector. Our goal here is to find all the data fields on the page, in the html, and write a selector/xpath for them.", "This html snippet above contains many list elements. Inside each ", "tag we can find many of the fields we\u2019re looking for, for example, the listing_type field. As different listings have different details defined we cannot select data solely based on html tags or css selectors. (Some listings have ", " defined others don\u2019t) So for example, if we want to extract the ", " from the code above, we can use xpath to choose the one html element which has the \u201cListing Type\u201d in its text then extract its first sibling.", "Xpath for ", ":", "Then, we can do the same thing for house size, etc\u2026", "After finding all the selectors, this is our spider code:", "As you can see, I\u2019m using an ", ". The reason for this is that for some of the fields the extracted data is messy. For example, this is what we get as the raw house size value:", "This value is not usable in its current form because we need a number as the house size. Not a string. So we need to remove the unit and the comma. In scrapy, we can use input processors for this. This is the ", " field with a cleaning input processor:", "What this processor does, in order:", "The output becomes", "\u00a0", "At this point we have the data extraction part handled. To prepare the data for analysis we need to store it in a database. For this, we create a custom scrapy pipeline. If you are not sure how a database pipeline works in scrapy, ", ".", "Now we can run our spider:", "If we\u2019ve done everything correctly we should see the extracted records in the database, nicely formatted.", "Now we are going to visualize the data. Hoping to get a better understanding of it. The process we are going to follow to draw charts:", "A few important things to point out before drawing far-reaching conclusions while analyzing the dataset:", "Starting off, let\u2019s \u201cget to know\u201d our database a little bit. We will query the whole database into a dataframe and create a new dictionary to get min, max, mean and median values for all numeric data fields. Then create a new dataframe from the dict to be able to display it as a table.", "This table shows us some information:", "Overall, we have three types of listings: Condo/Townhome, Multi-Family and Single-family. Let\u2019s see which are the more popular among the listings.", "A little more than half of the listings are Single family homes. About a third of them are considered Condo/Townhome. And only 12.3% is a Multi-family home.", "Next, let\u2019s have a look at the prices. It would be good to see if there\u2019s any correlation between price and age of the building.", "What we see here is that we have a listing for pretty much every year. But there\u2019s not really a clear conclusion we can draw from this. It looks like there are quite a few houses that were built in the first half of the 20th century and they are more expensive than recently built ones.", "In this one we look at the price tags for each city.", "Among the cities we analyzed, San Francisco and Seattle are the most expensive ones. San Francisco has an average house price of ~$1 650 000. For Seattle, it\u2019s ~$1 850 000. The cheapest cities to buy a house are Cheektowaga and Buffalo, the average price is about $250 000 for both.", "In the next analysis we examine the average price for different house types. Out of the three house types, which one is the most expensive and the cheapest one?", "As we can see, there\u2019s not much of a difference between Condo/Townhome and Multi-Family Type houses. Both are between $1,000,000 - $ 1,200,000. Single Family homes are considerably cheaper, with an average price of $800,000.", "As I said at the beginning, the data sample I used to create these charts is really small. Thus, the results of this analysis cannot be considered reliable to generalize to the whole real estate market. The goal of this article is just to show you how you can make use of web data and turn it into actual insights.", "Want to know how you can use web extracted real-estate data to transform your business? ", "If you have a data-driven product which is fuelled by web data, "]},
{"tite": "Web Scraping Questions & Answers Part II", "date": "October 17, 2019 ", "author": "Attila T\u00f3th", "blog_data": [", we answered some of the best questions we got during ", ". In today\u2019s post we share with you the second part of this series. We are covering questions on Web Scraping Infrastructure and How Machine Learning can be used in Web Scraping.", "Spider unit testing is difficult because it depends on the actual website which you don't have control over. You can use ", " for testing.", "Currently, the only tool that is specifically designed for web scraping and to render javascript is ", ". Another option can be to use a headless browser like selenium.", "In ", " we use NoSQL database for data storage. One of the advantages of NoSQL is the lack of schema which means you can change Scrapy item definition at any time without any problems.", "It depends on the project. In some projects there is a dedicated QA team that checks data, in some projects there is no such need. Sometimes it is enough to add automated checks for typical problems, and only do data checks after some major spider update.", "It depends on the client\u2019s needs. If they need a screenshot of some page as the user sees it they will always need javascript rendering. If they need some content from the website, it is difficult or may be impossible to detect which site needs javascript rendering and which don't.", "If you don't want to develop your own implementation use a high level tool like ", "that takes care of it. Otherwise you have to develop your own solution.", "Our ", " open source library handles these formats.", "The data field locators need to be changed as well. Or use an AI tool like ", "which handles the changes for you.", "If you mean storage of string with url you can use anything, you can have HTTP API that returns a list of strings with urls to visit, you can have txt file, you can make the spider connect to some database to obtain a list of urls to crawl. If you want to create some sort of delta-crawl, only visit a page once and don\u2019t revisit again then you can use ", ".", "HTML responses can be huge, storing them in database can be challenging. It depends on your use case, but sometimes it makes sense to store responses in files rather than database.", "There's only one spider management tool available for the public today which is ", ".", "You can check if the required data fields are populated or check if the produced JSON schema is what you expected. If something doesn't seem right send an alert.", "Yes there is an ", " available. Using a headless browser takes considerably more resources so it takes longer to execute.", "There is no easy way to deal with that. You need to determine why the request hangs. Does it hang because of rate limit? If yes, use ", ". Does it hang because of poor quality server? If this is the case, you cannot do much.", "You could use CSS selectors/XPATH instead. These are much easier to read and maintain also there are libraries for this. Only use regex if the HTML is very messy and you cannot use CSS or XPATH. Or use ", " and you don\u2019t have to deal with any of these.", "Scrapy is more mature, it is on the market for several years, it has more options for configuration and is easier to extend.", "There are useful libraries made by ", ".", "It looks like most of the good research is hidden behind the doors of commercial entities (search engines, web data extraction companies). You need to do your own research to get good results.", "No, AutoExtract doesn't have this feature yet.", "We use hundreds of thousands of pages from different domains. It could work with thousands as well.", "Pytorch", "It keeps working. No need for additional training.", "Yes it can.", "We create models for different website types, but the approach/architecture is the same. It is possible to have a single model, as a performance optimization.", "We evaluated our article extraction against dragnet and other open source libraries; they were not useful for us in the end, because quality is much worse. We're not contributing to them, because the approach is very different.", "No.", "ML explainability is an area of active research and development. To contribute, we created ", " which includes many explanation methods. Recently image support was added to this library.", "\u00a0", "If you feel like we missed an important question leave a comment below and we will try our best to answer it. Also if you missed the ", " but are interested in the talks "]},
{"tite": "Price Scraping: The Best Free Tool To Scrape Prices", "date": "November 26, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["Price scraping is something that you need to do if you want to extract pricing data from websites. It might look easy and just a minor technical detail that needs to be handled but in reality, if you don\u2019t know the best way to get those price values from the HTMLs, it can be a headache over time.", "In this article, first, I will show you some examples where price scraping is essential for business success. We will then learn how to use our open-source python library, ", ". This library was made specifically to make it easy to extract clean price information from an e-commerce site.", "If you\u2019re at the beginning of your web scraping journey, here are some examples to give you inspiration on how price-scraping can help you.", "The e-commerce world has become very noisy and competitive. Companies are searching for ways to raise margins, cut expenses and ultimately display prices that increase their overall revenue the most. This is where competitor price monitoring comes in. There\u2019s no real online retail seller that doesn\u2019t monitor competitor prices on a daily basis in one way or another. Price scraping is a big part of this task - extracting real-time data from millions of price points on a regular basis.", "Another huge use case of price scraping is brand monitoring. When your brand is visible on multiple platforms online, maintaining price compliance for your product is as important as keeping an eye on the competitor\u2019s pricing. You would ideally want to scrape the product pages that display your products (i.e. your resellers) as well as the competitor\u2019s product data to make sure your pricing strategy is up to date. This would help you establish a competitive price and keep the pricing policy violators in check.", "You would also want to scrape prices if you do any kind of e-commerce market research. Whether it\u2019s a one-time project or an ongoing one, if you scrape multiple web pages with different price strings it\u2019s important to find a solution for effectively extracting pricing data.", "At Scrapinghub we\u2019ve developed our own open-source library for price scraping. You can find it on GitHub, as ", ". It is capable of extracting price and currency values from raw text strings.", "You want to use this library for two important reasons:", "2. Use this library to clean up the string", "Normally, at this point, you would need to write a custom function to get the numeric value from the string. Using regex or some python code. However, with price-parser, you just need to import the library and use the same function every time:", "Then we can retrieve the amount and currency values using attributes:", "The library has been tested with 900+ real-world price strings, ", ".", "If you feel like you need a helping hand in your data extraction project, learn more about our ", " solutions and how we can help you build your own ", "."]},
{"tite": "Building Blocks of an Unstoppable Web Scraping Infrastructure", "date": "December 19, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["More and more businesses leverage the power of web scraping. Extracting data from the web is becoming popular. But it doesn't mean that the technical challenges are gone. Building a sustainable web scraping infrastructure takes expertise and experience. Here, at ", " we scrape 9 billion pages per month. In this article, we are going to summarize what the essential elements of web scraping are. What building blocks you need to take care of, in order to develop a healthy web data pipeline.", "Let's start with the obvious, spiders. In the web scraping community, a spider is a script or program that extracts data from the website. Spiders are essential to scrape the web. There are many libraries and tools available that we could use. In Python, you have ", ", the web scraping framework or ", ". If you\u2019re programming in java, you have ", " or ", ". In Javascript it\u2019s ", " or ", ". But I could mention many other libraries, these are just the most popular ones.", "It can be difficult to choose which library or tool to choose, especially if you\u2019ve never done web scraping before. For one-off projects, it doesn't really matter what library you use. Choose the one which is the easiest to get started within your preferred programming language. But for long term projects, where you will need to maintain your spiders and maybe build on top of them, you should choose a tool which lets you focus on your project-specific needs and not on the underlying scraping logic.", "Now that we got the spiders right, the next element you need to have in your web scraping stack is spider management. Spider management is about creating an abstraction on top of your spiders. A spider management platform, like ", ", makes it quick to get a sense of how your spiders are performing. You can schedule jobs and automate spiders. And you can actually focus on the crawlers and not the servers.", "Javascript, as a technology, is widely used on websites. Sometimes it can be hard to get data that is rendered with JS. Many people render JS when they don't even have to. Generally, when you know the website is using JS to render its content, your first instinct might be to grab some headless browser like ", ". That is the easiest way to go.", "The trade-off is that in the short term it's much quicker to just render JS and get the data, but in the long term it will take too much hardware resources. So make sure that if execution time and used hardware resources are important for you, inspect the website first properly and see if there's any AJAX request or hidden API calls in the background. If yes, then try to replicate that in your scraper, instead of executing JS.", "If there's just no other way to get the data and you have to render JS no matter what, then try to use a more lightweight solution like ", " instead of using a full-fledged headless browser. Because it is created for scraping and it can only render JS, nothing else, which is exactly what we need to scrape a site.", "The next building block is data QA. All our scraping efforts are only worth it if the output produced is the data we expect in the correct format. To make sure this is the case, we can do several things:", "It's also useful to set up alerts and notifications that are triggered when a certain action happens. Scrapinghub recently open sourced two tools that we use for spider monitoring and to ensure data quality. One is ", " which, for example, can be used to send email or slack notifications, it can also create custom reports based on web scraping stats. The other one is ", ", which can be used to analyze and verify scraped data using a set of rules.", "The last thing, but a very important one, I want to mention is proxy management.", "It\u2019s not an easy task to scale web scraping. You cannot solve everything just by throwing more hardware at the problem. Sometimes it\u2019s impossible to scale. For example, if your target website has 50,000 visitors/month and you want to scrape it 50,000 times in a month\u2026 that\u2019s not going to work.", "There are projects when we need ongoing maintenance and we scrape hundreds of millions or even billions of URLs. For this kind of volume, you definitely need a proxy solution, like ", ". You can figure out how many and what kinds of proxies you need based on target websites, requests volume and geolocation (example.com, 3M requests/day, USA).", "Before finishing up this article, it\u2019s important to set this straight. When you scrape a website, you have to make sure ", ". Below are some best practices you can follow to scrape respectfully.", "The most important rule when you scrape a website is not to harm it. Do not make too many requests. Making requests too frequently could make it hard for the website server to serve other visitors. Limit the number of requests in accordance to the target website.", "Before scraping, always inspect the robots.txt file first. This will give you a good idea what parts of the website you are free to visit and what pages you should not.", "Define a user-agent that clearly describes you or your company. Also, it\u2019s best to include contact information in your user-agent as well, so they can let you know if they have any issues with what you\u2019re doing.", "There are cases, when you can only access certain pages if you are logged in. If you want to scrape those pages, you need to be very careful. By logging in and/or explicitly agreeing to the website's terms and conditions which state you cannot scrape, then you CANNOT scrape. You should always honor the terms of any contract you enter into, including website terms and conditions and privacy policies.", "Getting started with web scraping is easy. But as you scale, it\u2019s getting more and more difficult to keep your spiders working. You need a gameplan on how to tackle the challenges of website layout changes, keeping data quality high and proxy needs. Perfecting the mentioned building blocks, you should be well on your way.", "Learn more about ", " or our ", " that make web scraping a breeze."]},
{"tite": "Solution Architecture Part 4: Accessing The Technical Feasibility of Your Web Scraping Project", "date": "June 13, 2019 ", "author": "Ian Kerins", "blog_data": ["In the fourth post of this ", ", we will share with you our step-by-step process for evaluating the technical feasibility of a web scraping project.", "After completing the legal review of a potential project, the next step in every project should be to assess the technical feasibility and complexity of executing the project successfully.", "A bit of upfront testing and planning can save countless number of wasted hours down the line, if you start developing a fully featured solution only to hit a technical brick wall.", "The technical review process focuses on the four key parts of the web scraping development process:", "We will look at each one of these individually, breaking down the technical questions you should be asking yourself at each stage.", "Using the project requirements we gathered in the requirement gathering process you should have all the information you need to start accessing the technical feasibility of the project.", "The first step of the technical feasibility process is investigating whether it is possible for your crawlers to accurately discover the desired data as defined in the project requirements.", "For most projects, where you know the exact websites you want to extract data from and can manually navigate to the desired data there are usually no technical challenges in developing crawlers to discover this data. The crawler just needs to be designed to replicate the user behaviour sequence you need to execute to access the data.", "Typically, the main reasons for experiencing technical challenges at the data discovery phase is if you don\u2019t know how to discover all the data manually yourself:", "For example, the most common technical challenge we run into during the data discovery stage is when the client would like to extract a specific type of data but either", " or ", ".", "In cases like these, we can investigate the viability of discovering the data they require using a number of approaches we have at our disposal:", "Each of these approaches have their limitations, most notably data quality and coverage. As a result, we generally recommend that we develop customer crawlers for each website if data quality is of priority.", "The next step is to verify the technical feasibility of the data extraction phase.", "Here the focus is on verifying that the data can be accurately extracted from the target websites and give an assessment on the complexity required. Our solution architecture team will do a series of tests to enable them to design the extraction process and verify that it is technically possible to extract the data at the required quality. These tests will test for:", "Once this step is complete the solution architect will then investigate the feasibility of extracting the data at the required scale...", "With the ability to discover and extract the data on a small scale verified then the next step is to verify that the project can be executed at the required scale & speed to meet the project requirements. This is often the most difficult and troublesome area when it comes to web scraping and the area where a good crawl engineers experience and expertise really shines.", "Provided there are no legal or glaring data discovery/extraction issues it is normally feasible to develop a small scale crawler to extract data without running into any issues. However, as the scale and speed requirements of the crawl increases you can quickly run into trouble:", "With this information, our solution architecture team is able to get a deep understanding of the complexity of the project and the difficulty of delivering and maintaining it at scale.", "The final technical feasibility step is verifying that the data delivery format and method is viable. In most cases, there are very few issues at this stage as we have numerous data format and delivery methods available to us to satisfy any customer requirement.", "However, certain options might be more complex than others as some require more development time (example: develop a custom API, etc.).", "The most common complexity adding step is if the project requires data post-processing or data science to meet its requirements. These can significantly increase the complexity of the project.", "So there you have it, they are the four steps to conducting a technical feasibility review of your web scraping project. In the next article in the series, we will share with you how we take the project requirements and the complexity analysis and develop a custom solution.", "At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.", "Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.", "If you have a need to start or scale your web scraping project then our ", " are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0"]},
{"tite": "Backconnect Proxy: Explanation & Comparison To Other Proxies", "date": "December 10, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["Scaling up your web scraping project is not an easy task. Adding proxies is one of the first actions you will need to take. You will need to manage a healthy proxy pool to avoid bans. There are a lot of proxy services/providers, each having a whole host of different types of proxies. In this blog post, you are going to learn how backconnect proxies work and when you should use them.", "Before we get into the details of backconnect proxies it\u2019s important to understand the different types of proxies. Here\u2019s a summary:", "So, first of all, we can refer to proxies based on the IP address type:", "A backconnect proxy network can be a set of any of these or even a combination of these.", "Knowing what kind of address you need for your web scraping project is important. You can read more about IP address types in our ", "!", "We can also group proxies together based on their quality:", "Public proxies are free and can be used by anyone. Hence, the quality is poor and you probably can\u2019t use them to scale your web scraping. Dedicated proxies are the best for web scraping. Only you can access them and you have all the control over them.", "Finally, we can define proxies based on if they are managed proxies or not:", "Now let\u2019s see the difference between these two types.", "The way a regular proxy server works is pretty simple. You send your request through one proxy and hope that you will get a successful response back. If the IP address is banned you will need to try it with another proxy. Here\u2019s an illustration:", "Using regular proxies is not a scalable solution, unless you implement your proxy management solution. Main challenges of proxy management: identify bans, rotate proxies, user agent management, add delays and geo targeting.", "Backconnect proxies are an easy way to handle multiple requests. You can think of it as a pool of IP addresses, from the list above, plus proxy management. Unlike regular proxies where you need to send your requests through different proxies manually, with backconnect proxies you need to send all your requests through one proxy network only. Which then assigns a working IP address for you. If it gets banned you automatically get another IP address, then another and so on. As a user, it\u2019s hassle-free.", "If you use a backconnect proxy for scraping, you don\u2019t directly access proxies one-by-one. Instead, you access a pool of proxies and you will instantly get a proxy that can reach the target website with relative ease. As an example, let\u2019s see how our ", " works:", "Using backconnect proxies with automatic ban detection, you can scale up your web scraping projects to ", ". As you don\u2019t directly access the proxies, but through a network, your original IP address will be untraceable.", "Features to look for when choosing a backconnect proxy provider:", "It\u2019s also important to ", ". This means, do not harm the website by sending too many requests. If necessary, limit the number of concurrent requests or wait between requests. Read our ", " for more information.", "Having a quality proxy pool at your fingertips is already a huge advantage over regular proxies but our managed backconnect proxy network has much more features to combat blocks. Features like automatic proxy rotation, geolocation, custom user agents, configurable browser profiles and cookies. With these, you will be able to ", " or throughput with a minimum of fuss.", "Crawlera's ", " combined with intelligent proxy rotation and automatic ban avoidance capabilities allows you to leverage datacenter proxies to the greatest possible degree. Using residential proxies optimally, therefore keeping the costs down."]},
{"tite": "Solution Architecture Part 5: Designing A Well-Optimised Web Scraping Solution", "date": "July 04, 2019 ", "author": "Ian Kerins", "blog_data": ["In the fifth and final post of this ", ", we will share with you how we architect a web scraping solution, all the core components of a well-optimized solution, and the resources required to execute it.", "To give you an inside look at this process in action, we will give you a behind the scenes look at examples of projects we\u2019ve scoped for our clients.", "But first, let\u2019s take a look at the main components you need for every web scraping project\u2026", "There are a few core components to every web scraping project that you need to have in place if you want to reliably extract high-quality data from the web at scale:", "However, depending on your project requirements you might also need to make use of other technologies to extract the data you need:", "The amount of resources required to develop and maintain the project will be determined by the type and frequency of data needed and the complexity of the project.", "Talking about the building blocks of web scraping projects is all well and good, however, the best way to see how to scope a solution is to look at real examples.", "In the first example, we\u2019ll look at one of the most common web scraping use cases - Product Monitoring. Every day Scrapinghub receives numerous requests from companies looking to develop internal product intelligence capabilities through the use of web scraped data. Here we will look at a typical example:", "The customer wanted to extract product from specific product pages from Amazon.com. They would provide a batch of search terms and the crawlers would search for those keywords and extract all products associated with them (~500 keywords per day).", "The extracted data will be used in a customer facing product intelligence tool for consumer brands looking to monitor their own products along with the products of their competitors.", " Typically, extracting product data poses very few legal issues provided that the crawler (1) doesn\u2019t have to scrape behind a login (which often isn\u2019t the case), (2) is only scraping factual or non-copyrightable information, and (3) the client doesn\u2019t want to recreate the target website\u2019s whole store, which may bring database rights into question. In this case, the project had little to no legal challenges.", "Although this project required a large scale crawling of a complex and ever-changing website, projects like this are Scrapinghub\u2019s bread and butter. We have considerable experience delivering similar (and more complex) projects for clients so this was a very manageable project. We would be able to reuse a considerable amount of code used elsewhere to enable us to get the project up and running very quickly for the client.", " After assessing the project the solution architect then developed a custom solution to meet the client's requirements. The solution consisted of three main parts:", "Scrapinghub successfully implemented this project for the client. The crawlers developed now extract ~500,000 products per day from the site, which the client inputs directly into their customer-facing product monitoring application.", "In the next example, we\u2019re going to take a look at a more complex web scraping project that required us to use artificial intelligence to extract the article data from over 300+ news sources.", "The customer wanted to develop a news aggregator app that will curate news content for their specific industries and interests. They provided an initial list of 300 news sites they wanted to crawl, however, they indicated that this number was likely to rise as their company grew. The client required every article in specific categories to be extracted from all the target sites, crawling the site every 15 minutes to every hour depending on the time of day. The client needed to extract the following data from every article:", "Once extracted this data would be fed directly into their customer-facing app so ensuring high quality and reliable data was a critical requirement.", " With article extraction, you always want to be cognizant of the fact that the articles are copyrighted material of the target website. You must ensure that you are not simply copying an entire article and republishing it. In this case, since the customer was aggregating the content internally and only republishing headlines and short snippets of the content, it was deemed that this project could fall under the fair use doctrine under copyright law. There are various copyright considerations and use cases to take into account when dealing with article extraction, so it is always best to consult with your legal counsel first.", "Although the project was technically feasible, due to the scale of the project (developing high-frequency crawlers for 300+ websites) the natural concern was that it would be financially unviable to pursue such a project.", "As a rule of thumb, it takes an experienced crawl engineer 1-2 days to develop a robust and scalable crawler for one website. Doing a rough calculation will quickly show that to manually develop 300+ crawlers would be a very costly project if it required 1 work day per crawler.", "With this in mind, our solution architecture team explored the use of AI enabled intelligent crawlers that would remove the need to code custom crawlers for every website.", " After conducting the technical feasibility assessment the solution architect then developed a custom solution to meet the client's requirements. The solution consisted of three main parts:", "Scrapinghub successfully implemented this project for the client, who now is able to extract 100,000-200,000 articles per day from the target websites for the news aggregation app.", "So there you have it, this is the four-step process Scrapinghub uses to architect solutions for our client's web scraping projects. At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.", "Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.", "If you have a need to start or scale your web scraping project then our ", " are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."]},
{"tite": "GDPR Update: Scraping Public Personal Data", "date": "July 25, 2019 ", "author": "Sanaea Daruwalla", "blog_data": ["One common misconception about scraping personal data is that public personal data does not fall under the GDPR. Many businesses assume that because the data has already been made public on another website that it is fair game to scrape. In actuality, GDPR makes no blanket exceptions for public personal data and the same analysis for any other personal data must be conducted prior to scraping public personal data as well (see our previous posts on ", " and ", "). It is also worth noting that there are some exemptions under GDPR and the ICO provides a great overview of these exemptions - read them ", ". In this post, we will focus on public personal data in general, as it comes up frequently as a point of confusion.", "A recent decision from the Polish GDPR regulator clearly sets forth the necessity to comply with GDPR even when dealing with public personal data.", "In March 2019, the Polish regulator issued a \u00a3187,000 fine against a company for scraping public personal data and reusing that data without notifying the data subjects. The company in question is said to have taken personal data on over six million Polish citizens from the country\u2019s Central Electronic Register and Information on Economic Activity. However, it only informed 90,000 of the individuals that it had email addresses for, asserting that \u201chigh operational costs\u201d prevented it from doing more. The company attempted to use the argument that there was a disproportionate effort in notifying all the individuals for whom they did not have email addresses, but the Polish regulator did not find that convincing. It should be noted that it\u2019s unclear whether they conducted a full DPIA or not, which is something we always recommend if you are conducting any type of personal data scraping without the data subject\u2019s explicit consent or contractual agreement.", "Despite the company\u2019s arguments regarding the disproportionate costs, the Polish regulator found that the company should have used the postal addresses and telephone numbers it had to notify individuals about (1) the data they used, (2) the source of their data, (3) the \u201cpurpose and the period of the planned data processing,\u201d and (4) their rights under the GDPR. So the Polish regulator found that even when taking public personal data, and even when the operational burden to notify is high, you still have very strict obligations to the data subjects that you must comply with.", "This is a clear signal that there is likely no way around your obligations to notify individuals of your scraping of their public personal data. If you have their email, telephone, physical address, or other means to contact them, you are obliged to provide the requisite notifications. Furthermore, if you are being investigated, ensure that you are clearly taking actions to rectify any issues or you may open yourself up to further unnecessary fines. Finally, if you do decide to take the DPIA route, ensure that it is well documented and that if there is a way to notify the data subjects to do so.", "It is really important for Web Scraping companies to stay updated with the rules and regulations around data extraction to remain web compliant. At the ", ", we will discuss issues like this and many more so that you can make sure that your scraping process is productive and respectful, so make sure you attend to get best practice tips to ensure you remain compliant.", "If you are considering commencing a web scraping project for your business that might extract personal data from public websites and you want to ensure it is GDPR compliant, then ", ". Our engineering team of 60+ crawl engineers and data scientists can build a custom web scraping solution for your specific needs."]},
{"tite": "How to set up a custom proxy in Scrapy?", "date": "August 08, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["When scraping the web at a reasonable scale, you can come across a series of problems and challenges. You may want to access a website from a specific country/region. Or maybe you want to work around anti-bot solutions. Whatever the case, to overcome these obstacles you need to use and manage proxies. In this article, I'm going to cover how to set up a custom proxy inside your Scrapy spider in an easy and straightforward way. Also, we're going to discuss what are the best ways to solve your current and future proxy issues. You will learn how to do it yourself but you can also just use ", " to take care of your proxies.", "If you are extracting data from the web at scale, you\u2019ve probably already figured out the answer. ", ". The website you are targeting might not like that you are extracting data even though what you are doing is totally ethical and legal. When your scraper is banned, it can really hurt your business because the incoming data flow that you were so used to is suddenly missing. Also, sometimes websites have different information displayed based on country or region. To solve these problems we use proxies for successful requests to access the public data we need.", "Setting up a proxy inside Scrapy is easy. There are two easy ways to use proxies with Scrapy - passing proxy info as request parameter or implementing a custom proxy middleware.", "Another way to utilize proxies while scraping is to actually create your own middleware. This way the solution is more modular and isolated. Essentially, what we need to do is the same thing as when passing the proxy as meta parameter:", "In the code above, we define the proxy URL and the necessary authentication info. Make sure that you also enable this middleware in the settings and put it before the ", ":", "To verify that you are indeed scraping using your proxy you can scrape a test site which tells you your IP address and location (", "). If it shows the proxy address and not your computer\u2019s actual IP it is working correctly.", "Now that you know how to set up Scrapy to use a proxy you might think that you are done. Your IP banning problems are solved forever. Unfortunately not! What if the proxy we just set up gets banned as well? What if you need multiple proxies for multiple pages? Don\u2019t worry there is a solution called IP rotation and it is key for successful scraping projects.", "When you rotate a pool of IP addresses what you\u2019re doing is essentially randomly picking one address to make the request in your scraper. If it succeeds, aka returns the proper HTML page, we can extract data and we\u2019re happy. If it fails for some reason (IP ban, timeout error, etc...) we can\u2019t extract the data so we need to pick another IP address from the pool and try again. Obviously, this can be a nightmare to manage manually so we recommend using an automated solution for this.", "If you want to implement IP rotation for your Scrapy spider you can install the ", " middleware which has been created just for this.", "It will take care of the rotating itself, adjusting crawling speed and making sure that we\u2019re using proxies that are actually alive.", "After ", " you just have to add your proxies that you want to use as a list to ", ":", "Also, you can customize things like, ban detection method, page retries with different proxies, etc\u2026", "So now you know how to set up a proxy in your Scrapy project and how to manage simple IP rotation. But if you are scaling up your scraping projects you will quickly find yourself drowned in proxy related issues. Thus, you will lose data quality and ultimately you will waste a lot of time and resources dealing with proxy problems.", "For example, you will find yourself dealing with unreliable proxies, you\u2019ll get poor success rate and poor data quality, etc... and really just get bogged down in the minor technical details that stop you from focusing on what really matters: making use of the data. How can we end this never-ending struggle? By using an already available solution that handles well all the mentioned headaches and struggles.", "This is exactly why we created Crawlera. Crawlera enables you to reliably crawl at scale, managing thousands of proxies internally, so you don\u2019t have to. You never need to worry about rotating or swapping proxies again. Here's how you can ", ".", "If you\u2019re tired of troubleshooting proxy issues and would like to give Crawlera a try then ", "It has a 14-day FREE trial!"]},
{"tite": "Learn how to configure and utilize proxies with Python Requests module", "date": "August 22, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["Sending HTTP requests in Python is not necessarily easy. We have built-in modules like urllib, urllib2 to deal with HTTP requests. Also, we have third-party tools like ", " Many developers use Requests because it is high level and designed to make it extremely easy to send HTTP requests.", "But choosing the tool which is most suitable for your needs is just one thing. In the web scraping world, there are many obstacles we need to overcome. One huge challenge is when your scraper gets blocked. To solve this problem, you need to use proxies. In this article I\u2019m going to show you how to utilize proxies when using the Requests module so your scraper will not get banned.", "In this part we're going to cover how to configure proxies in Requests. To get started we need a working proxy and a URL we want to send the request to.", "The proxies dictionary must follow this scheme. It is not enough to define only the proxy address and port. You also need to specify the protocol. You can use the same proxy for multiple protocols. If you need authentication use this syntax for your proxy:", "In the above example you can define proxies for each individual request. If you don\u2019t need this kind of customization you can just set these environment variables:", "This way you don\u2019t need to define any proxies in your code. Just make the request and it will work.", "Sometimes you need to create a session and use a proxy at the same time to request a page. In this case, you first have to create a new session object and add proxies to it then finally send the request through the session object:", "As discussed earlier, a common problem that we encounter while extracting data from the web is that our scraper gets blocked. It is frustrating because if we can\u2019t even reach the website we won\u2019t be able to scrape it either. The solution for this is to use some kind of proxy or rather multiple ", ". A proxy solution will let us get around the IP ban.", "To be able to rotate IPs, we first need to have a pool of IP addresses. We can use free proxies that we can find on the internet or we can use commercial solutions for this. Be aware, that if your product/service relies on scraped data a free proxy solution will probably not be enough for your needs. If a high success rate and data quality are important for you, you should choose a paid proxy solution ", "So let\u2019s say we have a list of proxies. Something like this:", "Then, we can randomly pick a proxy to use for our request. If the proxy works properly we can access the given site. If there\u2019s a connection error we might want to delete this proxy from the list and retry the same URL with another proxy.", "There are multiple ways you can handle connection errors. Because sometimes the proxy that you are trying to use is just simply banned. In this case, there\u2019s not much you can do about it other than removing it from the pool and retrying using another proxy. But other times if it isn\u2019t banned you just have to wait a little bit before using the same proxy again.", "Implementing your own smart proxy solution which finds the best way to deal with errors is very hard to do. That\u2019s why you should consider using a managed solution, like ", ", to avoid all the unnecessary pains with proxies.", "As a closing note, I want to show you how to solve proxy issues in the easiest way with Crawlera.", "What does this piece of code do? It sends a ", ". When you use Crawlera, you don\u2019t need to deal with proxy rotation manually. Everything is taken care of internally.", "If you find that managing proxies on your own is too complex and you\u2019re looking for an easy solution, "]},
{"tite": "The First-Ever Web Data Extraction Summit!", "date": "August 29, 2019 ", "author": "Marie Moynihan", "blog_data": ["The range of use cases for web data extraction is rapidly increasing and with it the necessary investment. Plus the number of websites continues to grow rapidly and is expected to exceed 2 billion by 2020.", "Sourcing accurate reliable web data is becoming a focus for most forward-thinking companies to give them the advantage they need to compete. Most start with internal, home-grown systems but as their projects need to scale or get more complex they quickly realise they don't have the necessary expertise in house and look to outsource. Finding the right partner is key.", "Scrapinghub has been helping companies get access to this data by turning websites into structured data for the last 10 year, and the ", " is aimed at bringing together the thought leaders and visionaries in the industry in a one-day event.", "Presented by ", ", the first Web Data Extraction Summit will be held in Dublin, Ireland on 17th September 2019. This is the first-ever event dedicated to web data and extraction and will be graced by over 100 CEOs, Founders, Data Scientists and Engineers.", "This one-day event gives exclusive access to case studies and insights on how the world\u2019s leading companies like Just-Eat, OLX, Revuze and Eagle Alpha, leverage web data to stay a step ahead of the industry.", "Hear about the trends and innovations in the industry from leaders like Shane Evans, founder and CEO of Scrapinghub, Or Lenchner, CEO of Luminati, and Andrew Fogg, founder and Chief Data Officer of Import.io, along with other pioneers like David Schroh, Amanda Towler and Juan Riaza.", "Amidst so many data lovers all under one roof, it is going to be a day to remember. Don't miss this opportunity to meet other like-minded data lovers and be a part of the future of data extraction. Hurry up and ", "! Use this ", " for a 20% discount on the tickets!"]},
{"tite": "News Data Extraction at Scale with AI powered AutoExtract", "date": "September 17, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["A huge portion of the internet is news. It\u2019s a very important type of content because there are always things happening either in our local area or globally that we want to know about. The amount of news published everyday on different sites is ridiculous. Sometimes it\u2019s good news and sometimes it\u2019s bad news but one thing\u2019s for sure: it\u2019s humanly impossible to read all of it everyday.", "In this article we take you through how to extract data from two popular news sites and perform basic exploratory analysis on the articles to find some overarching theme across news articles and maybe even news sites.", "Going through this article you will get some insights about our newest AI data extraction tool - ", ", and how it can be used to extract news data without writing any xpath or selectors.", "You will also learn some easy-to-do but spectacular methods to analyze text data. The exact process we are going to follow, for each news site:", "Discovering URLs on the site is an essential first step because otherwise we won\u2019t have the input data that is needed for the AutoExtract API. This input data is the URL of a news article. Nothing else is needed. So first, let\u2019s grab all the main page news article URLs. We need to setup a new Scrapy spider and use Scrapy\u2019s Linkextractor to find the correct URLs:", "Now that we\u2019ve got the URLs we can go ahead and use the AutoExtract API. These are the things you need to have, packaged in a JSON object, to make an API request:", "This is how the AutoExtract API call should look like, inside Scrapy:", "Adding this piece of code to our previously created spider:", "This function will first collect all the news article URLs on the main page using Scrapy\u2019s LinkExtractor. Then we pass each URL, one by one, to the AutoExtract API. AutoExtract will get all the data that is associated with the article, for example: article body, author(s), publish date, language and others. And the best part: without any html parsing or xpath.", "AutoExtract uses machine learning to extract all the valuable data points from the page and we don\u2019t need to write locators manually. It also means that if the website changes the design or the layout we don\u2019t need to manually change our code. It will keep working and keep delivering data.", "In our API request we use ", " as the callback function. This callback function will parse the response of the API request, which is a JSON. This JSON contains all the data fields associated with the extracted article. Like headline, url, authors, etc. Then we populate the Scrapy item.", "With this code above, we populated the ArticleItem with data from the AutoExtract API. Now we can just run the full spider and output the data for later analysis:", "We were able to extract news data from the web. Even though, at this point, this is just pure data you might find it useful to be able to get news data from any website, anytime. Let\u2019s go a little bit further and perform some exploratory data analysis on the extracted text.", "When it comes to text analysis one of the most popular visualization techniques is the word cloud. This kind of visualization is meant to show what words or phrases are used the most frequently in the given text. The bigger the word, the more frequently it is used. Perfect for us to learn what words are most commonly used in headlines and in the article itself.", "In python, there\u2019s an open source library to generate word clouds, ", ". This is a super easy-to-use library to create simple or highly customized word clouds (actual images). To use wordcloud we have to first install these libraries: numpy, pandas, matplotlib, pillow. And of course wordcloud itself. All these packages are easy to install with pip.", "Let\u2019s say we want to know what are the most common words in headlines on the homepage. To do this, first we read data from the json file that Scrapy generated. Then we create a nice and simple wordcloud:", "At the time of writing this article, it looks like the analyzed US news site puts a lot of focus on \u201cHurricane Dorian\u201d in the headlines. There are a lot of news articles on this topic. The second most common word is \u201cTrump\u201d. Other frequently used words are: \u201cAmerican\u201d, \u201cBahamas\u201d, \u201cmillion\u201d, \u201cCarolinas\u201d, \u201cclimate\u201d.", "The other website we extracted news from is one of the most visited UK news sites. Even without looking at the results we could probably guess that the frequently used words on the UK site would differ from what we found on the US site. The most used words in the headlines are \u201cBoris Johnson\u201d, \u201cBrexit\u201d, \u201cUS\u201d. Though there are similarities as well, \u201cHurricane Dorian\u201d is frequently used here too, the same is true for \u201cTrump\u201d.", "When analyzing not the headline but the article itself it becomes very noisy on the US news site. \u201cTrump\u201d is frequently used in the text as well. Other words are \u201csaid\u201d, \u201cwill\u201d, \u201cpeople\u201d, \u201cyear\u201d. There\u2019s not as big of a difference between the frequency of the words as we saw it with the headlines.", "\u00a0", "For the UK news site, it\u2019s less noisy. It looks like there are many words used relatively frequently. But it\u2019s hard to choose only one or two. \u201cPeople\u201d, \u201cJohnson\u201d, \u201cgovernment\u201d, \u201cTrump\u201d, \u201cUS\u201d are among the most used words in articles.", "I hope this article gave you some insights on how you can extract news from the web and what you could do with web scraped news data. If you are interested in our news data extraction tool, ", ", on a 14 day free trial.", "At the core of the AutoExtract is an AI enabled data extraction engine able to extract data from a web page without the need to design custom code. Through the use of deep learning, computer vision and Crawlera, Scrapinghub\u2019s advanced proxy management solution, the data engine is able to automatically identify common items on product and article web pages and extract them without the need to develop and maintain extraction rules for each site. Other verticals will be added to the API in the coming months."]},
{"tite": "Gain a Competitive Edge with Product Data", "date": "September 12, 2019 ", "author": "Marie Moynihan", "blog_data": ["Product data - whether from e-commerce sites, auto listings or product reviews, offers a treasure trove of insights that can give your business an immense competitive edge in your market. Getting access to this data in a structured format can unleash new potential for not only business intelligence teams, but also their counterparts in marketing, sales, and management that rely on accurate data to make mission critical business decisions.", "At Scrapinghub, we have a unique view on how this data is used - we extract data from 9 billions web pages per month and can see firsthand how the world\u2019s most innovative companies are using product data extracted from the web to develop new business capabilities for themselves and their customers. Whether you\u2019re a hedge fund manager, start-up or an e-commerce giant, here are a few inspiring new uses for web scraped product data:", "To make profitable pricing decisions, having access to timely, reliable source of high quality data is crucial. By scraping pricing data, business intelligence teams are empowered to confidently position products and services. Conveniently, this data can be integrated directly into product management systems. Automated data collection leaves little scope for human error and enables an organization to monitor the whole market, and always stay a step ahead. Real-time ", " to stay on top of price changes has already proved to be so effective that it has become a must practise for top performing retail organizations.", "Using product data to track your competitors is as old as the internet itself. However, we are increasingly seeing companies completely redefining the scale and scope of their competitor monitoring capabilities through the use of big data, ", "and sophisticated machine learning technologies. Businesses are aware of the huge opportunities to generate actionable competitor insights by scraping product data and data teams have proliferated to meet the rising need. Powerful data extraction solutions enable companies to implement dynamic pricing and not miss market opportunities.", "Whether it\u2019s a long, slow shift or a trend cycle as rapid as those in fashion or technology, those who can see the farthest ahead - and most accurately - into future trends emerge on top. The scale and multidimensional aspect of product data makes it perfect for web scraping, and once integrated into product management systems, product data enables companies to see farther, predict more accurately, and drive revenue growth. Web scraped data empowering clothing makers to optimize their products, prices, trends, fashions and inventories to more accurately respond to seasonal and cyclical changes. This means high affinity with consumers, high revenue and bringing the right product to market at the right time.", "Hedge funds and account managers are increasingly incorporating ", "streams into their decision making processes, hoping to gain an information edge over the market and generate alpha. As a result, the demand for product data continues to surge, with a majority of institutional investors increasing the amount of web scraped data they consume. Given its predictive utility, it\u2019s not hard to see why product data is of huge interest to them. Today, web scraped big data, aided by artificial intelligence and machine learning allows firms to predict revenue momentum, stock prices, company valuations and identify risks and opportunities for investment and this is just the beginning!", "Access to accurate reliable product data empowers businesses to monitor every aspect of their omnichannel strategy. It\u2019s akin to being a fly on the wall of every authorized merchant, re-seller, and retail location selling a particular product, enabling quick turnaround on merchandising audits and MAP violations. In the past, this required constant vigilance and a lot of sunk time - today, software can automate these headaches.", "Real time pricing through continuously scraping your own product data is not only the fastest and most effective way to maintain compliance across resellers and retailers, but perhaps one of the only methods of truly enforcing consistent compliance in real time. Web product data allows companies to keep a check on the supply chain and immediately notify legal teams about compliance violations.", "As the market continues to grow, harnessing the power of web product data has become even more imperative for companies to ensure future growth. Intrigued by these use cases? ", " for an in-depth analysis. If you need product data extracted from the web ", " today!", "Plus we have a ", " for product data, download now and see how you can turn unstructured product pages into valuable insights!"]},
{"tite": "Four Use Cases for Online Public Sentiment Data", "date": "September 05, 2019 ", "author": "Marie Moynihan", "blog_data": ["The manual method of discovery for gauging online public sentiment towards a product, company, or industry is cursory at best, and at worst, may harm your business by providing incorrect or misleading insights.", "Simply put, this data can only be properly aggregated to any useful extent if generated in quantities too large for manual input, and web scraping such data has therefore become best-practice across the world\u2019s many public-facing industries.", "Sentiment analysis can transform the subjective emotions of the public into quantitative insight that a company or leader can use to drive change. Let's look at some popular use cases for online public sentiment data:", "The market moves quickly, and being even half a step ahead of the competition can be incredibly valuable. If many firms are receiving the same traditional data, investors can differentiate their strategy (and bolster their earnings) by incorporating sentiment data into their decision making. Sentiment data can inform the investment process by predicting future firm fundamentals or stock returns plus event-based sentiment analysis can show savvy investors when to move.", "Any business intelligence strategy is incomplete without an understanding of public sentiment. The way a company\u2019s product or service is portrayed in news articles and reviews directly impacts its bottom line. By scraping qualitative public sentiment data about your product from the web, businesses can integrate this information directly into AI-driven business solutions to produce actionable insights into which products are performing - or underperforming - and why. The uses for sentiment data across the product monitoring are diverse. Here are a few examples: polarity analysis, social media monitoring, aspect-based text analysis, and website variance.", "The adage that any press is good press may be losing its veracity in an era when press and reviews are at our fingertips 24/7. What steps can a company take to protect its reputation in this socially accelerating world? By scraping sentiment data, business intelligence (BI) teams can both capitalize on positive publicity and work to mitigate negative sentiments. A comprehensive BI strategy acknowledges the multi-pronged nature of online sentiment by scraping data from many sources and analyzing many of the factors that comprise sentiment.", "Sentiment analysis based on web scraped data equips product development teams with data-driven insights into the changes customers want to see and the quality of a product\u2019s performance. In this way, sentiment analysis is not just a retrospective but a vital tool in the product creation, planning, and design process. Get insights for every step of development from launching new features to customer feedback from support tickets and monitoring how customers are using your product to help determine new product creation or new features.", "Sentiment Analysis, with its useful analysis opens doors to so many opportunities by providing a solid scientific base to information that was previously mere assumptions. If you are curious about the above applications of sentiment analysis and want to know more, have a look at our whitepaper on \u2018", "\u2019. If you want to know how web scraped news and article data can benefit your business, request a ", " with our solution architecture team today."]},
{"tite": "Navigating Compliance\u00a0When Extracting Web Scraped Alternative Financial Data", "date": "March 21, 2019 ", "author": "Sanaea Daruwalla", "blog_data": ["When it comes to using web data as alternative data for investment decision making, one topic rules them all: ", ".", "Regulatory compliance is such a pervasive issue in alternative data for finance, that it is often the number one barrier to investment firms using web data in their decision making processes. And matters aren\u2019t helped by the regulatory ambiguity.", "In this article, we\u2019re going to breakdown the regulatory compliance issues facing investment institutions, hedge funds and other financial institutions looking to use web data in their investment decision making processes and discuss some of the best practices we\u2019ve worked on with our clients to implement which have enabled them to extract their web data in a compliant manner.", ": The recommendations and commentary included in this guide do not constitute legal advice. The information contained in this article was garnered through Scrapinghub\u2019s experience working with our financial clients, and the independent research of our legal team. If you need specific legal advice regarding your use of web data as alternative data then you should consult a lawyer.", "When it comes to regulatory compliance and alternative data, everything revolves around ", ".", "As the usage of alternative data continues to grow, it is expected that regulatory interest in alternative data will only increase. Significantly increasing the compliance risks for firms who\u2019ve not ensured they have fully evaluated and managed the risks associated with their acquisition of alternative data and it\u2019s incorporation in their investment decision making processes.", "Generally speaking, the risks associated with alternative data can be broken into four categories:", "The various types of alternative data all have different levels of exposure to each of these risk categories. However, for the purposes of this article we\u2019re going to focus exclusively on the compliance areas associated with web scraped data that any firm considering using web data should be aware of.", "Web data extraction compliance is an evolving sector in the law and thus can pose some challenges when determining the legality of your scraping project. While there have been many cases related to web scraping in various different jurisdictions, the law remains unsettled and many of the case holdings are fact specific rather than espousing an overarching law related to web scraping. As such, clear guidance is difficult to provide to the industry as a whole, and particularly with regard to alternative financial data.", "It\u2019s this ambiguity, coupled with the scarcity of case law relating to asset management companies, which makes precisely identifying and mitigating the risks associated with using web data as part of an investment decision making process more challenging. This is compounded by the fact that many web scraping cases are associated with aspects of web scraping that are not highly applicable to financial use cases -- such as the extraction of web data to compete with the target website or redistributing the data in a manner that negatively impacts the market position of the data owner.", "In general, investors are seeking to gather web data to gain a better understanding of the wider trends impacting a market. Not to redistribute or compete with the original owner of the data. As a result, much of the current legal precedent for web scraping is of little relevance to alternative data for finance, requiring financial firms to dig a bit deeper into the case law for cases relevant to their use case. Of particular interest to financial firms would be cases involving breach of contract relating to terms of service, Computer Fraud and Abuse Act (CFAA), trespass, extraction of personal data, and overburdening the infrastructure of the target website during the data extraction process. However, as mentioned above, while many cases exist relating to these causes of action, no clear standard has emerged across the board.", "Despite this legal ambiguity, financial institutions have widely adopted web data into their investment decision making processes. In the absence of specific legal guidance the industry as a whole has managed to come to a general consensus on the specific compliance issues associated with web scraping and how they should be best dealt with.", "In the following sections we will discuss these conclusions for each of the four main risk categories: ", ", ", ", ", " ", ".", "One of the larger risks associated with the use of data extracted from the web for investment decision making is the risk of obtaining insider information and subsequently buying or selling shares based on that information, aka insider trading. Insider trading is the illegal use of non-public material information for profit.", "Non-public material information is any information that is not available to the general public. So, if the data you extract is not generally available to the public, your web data extraction could result in insider trading violations \u2013 which no financial firm wants to get involved with.", "The key to avoiding obtaining insider information by way of web scraping is to ensure that all the data scraped is information available to the general public. Typically, if the information is available on a public website that any person can go to and see, you are on safe footing. The risk of obtaining insider information increases when the information is not public \u2013 for example, information behind a login or paywall.", "With information behind a login or paywall there are clear restrictions over who can access the data, so there is a potential risk of obtaining insider information. However, logging in will not always present such a risk, as some sites allow any and every one to login, thereby mitigating the risk of getting inside information. Thus, if you plan to scrape data behind a login, it\u2019s imperative to ensure that you understand the nature of the data and whether it really is available to the general public.", "Information behind a paywall on the other hand is not generally available to the public, as it requires paying for access to the information, so unless you can verify that the information received is also generally available to the public elsewhere, you should avoid scraping information from behind a paywall.", "Another factor to consider when logging in is that you are typically expressly agreeing to terms and conditions and/or privacy policies when you register to login to a site. It is important to read these terms very closely, because many courts have found that by virtue of explicitly agreeing to those terms you are entering into a binding contract with the website. If the terms state that you may not scrape the site or use automated means to extract data from the site, your web scraping project may not only give rise to insider information issues, but also to breach of contract claims. As such, always ensure that you are working with a scraping provider that understands when and how terms must be reviewed.", ": at Scrapinghub our Legal Team looks at projects that require agreeing to terms and conditions on a case-by-case basis to ensure our client's web scraping is legally compliant.", "If you proceed with scraping data behind a login or paywall that is found to be non-public material information, you may be opening yourself up to prosecution for insider trading. As a result, it is imperative to take your review of the data extracted and means for extraction very seriously.", "If reviewing website terms and understanding the potential legal implications of agreeing to those terms is not a core competence to your company, we strongly recommend working with a data provider that has built-in compliance processes to manage this type of review.", "In recent years, personal data protection regulations have really come to the fore. Regulators are increasingly clamping down on companies collecting, storing and processing the personal information of citizens who haven\u2019t given their explicit consent to do so.", "Regulations such as the EU General Data Protection Regulation (GDPR) affect all companies including financial institutions and can lead to hefty fines. Under laws like GDPR, you typically need a lawful basis to process personal data, which can include consent, contractual agreement, or legitimate interest. Absent one of these lawful basises, you should not be scraping personal data. However, this analysis will vary from region to region, so please ensure you are familiar with the data protection laws in the region in which you operate before scraping personal data.", "Additionally, many personal data laws have notification and deletion requirements, so if you\u2019re unable to notify the individuals whose data you scrape and provide them with adequate deletion and other rights, you could be in violation of the relevant data protection laws in your region.", "For these above reasons, we always recommend either not scraping any personal data or anonymising personal data where feasible. Web crawlers should be designed so that they only extract the specific data that is valuable for the investment decision process, then verified to ensure there is in fact no personal data contained in the dataset during the QA process.", "If you\u2019d like an insider look at the four-layers of Scrapinghub\u2019s QA process and how you can build your own, then be sure to check out our ", ".", "If your crawlers can\u2019t be built to completely avoid personal data, the next best option is anonymisation. In our experience, our clients seeking alternative financial data want to obtain market and consumer trends, so the specific personal data that might be associated with those trends is not required. In these cases, anonymisation is your best friend. If you anonymise all personal data, it no longer falls under the remit of laws like GDPR and you don\u2019t have to worry about compliance.", "Furthermore, the possession of personal data also poses a headline risk for firms. There is the damage any negative press a firm would receive if the media found out that they were using personal data to make investment decisions, along with the hefty fines regulators will issue if they are found to illegally hold or use any such data.", "One final point to note is that web data can often be purchased in the form of off-the-shelf datasets. In these cases, asset managers will need to put internal processes in place to segregate this data from the rest of the organisation, and then identify and remove any personal data present before it is permitted to be used by investment teams. Working with a scraping company that already has personal data protection processes in place can significantly decrease the burden when obtaining off-the-shelf datasets, as you can be assured that the data you\u2019re receiving has been obtained in a compliant manner.", "The issue of copyright is also of concern in the case of alternative web data. Just because web data is publicly available on the internet doesn\u2019t mean that anyone can extract and store the data.", "In some cases the data itself might be copyrighted, and depending on how/what data you extract you could be found to have infringed the owner\u2019s copyright, creating additional risks for the users of this alternative data.", "Typical types of web data that are at risk of copyright are:", "For the purposes of web data, the most relevant data types are ", "and ", "as these often are the best sources of useful data for investment decision making.", "Copyright issues are sometimes surmountable if there is a valid exception to copyright within your use case. Some methods to achieve this are:", "The other copyright risk is if the website can claim database rights. A database is any organized collection of materials that permits a user to search for and access individual pieces of information contained within the materials. Database rights can create additional risks for the use of web data in investment decision making, if the data hasn\u2019t been extracted in a compliant manner.", "In the US, a database is protected by copyright when the selection or arrangement is original and creative. Copyright only protects the selection and organization of the data, not the data itself.", "In the EU, databases are protected under the Database Directive which offers much broader protection for EU databases. The Directive has two purposes: (1) protect IP, like in the US, and (2) protect the work and risk in creating the database.", "If you believe a data source might fall under database rights then decision makers should always consult with their legal team before scraping the data and ensure they either:", "Both copyright and database rights pose additional compliance risks for financial institutions considering using web data in their investment decision making processes, however, by following the simple guidelines outlined above this risks can be greatly reduced or negated. Furthermore, this type of analysis needs to be completed on a case-by-case basis, so it is always best to work with a web scraping company like Scrapinghub that has internal copyright compliance policies built into our workflows.", "As we\u2019ve already touched on, the data acquisition process itself presents some unique compliance risks for financial institutions -- extracting data behind logins, personal data, copyright, etc.", "However, there are some additional risks that legal counsel need to take into account when assessing the compliance risks of an off-the-shelf dataset or when developing their firm's own internal data extraction capabilities.", "Typically, these risks fall under the more traditional risks associated with web scraping but can pose additional risks for financial institutions due to the high profile nature of their business and the insider trading risks they have to manage.", "Here are some of the most important questions every legal counsel should be asking themselves and the data provider when evaluating a web datafeed:", "If the answer to any of these questions isn\u2019t a definite \u201cNo\u201d then by using this data you are exposing your firm to additional legal risks.", "When the firm is in full control of the data extraction process, they can take steps to ensure these issues never arise as they all fall under general web scraping best practices. However, if you are considering using an off-the-shelf dataset from a third party provider it can often be much harder to ascertain if the data was extracted safely using web scraping best practices.", "In a lot of cases, the data provider operates more so as a data marketplace -- collecting and organising a wide variety of alternative data types and often outsourcing part or all of their web data extraction to a third party. As a result, the level of oversight of a large historical dataset can be quite patchy.", "The only way to fully mitigate these risks is by directly controlling the data extraction process, either by moving the data extraction process in-house or partnering closely with a data extraction provider who has experience extracting alternative data for financial use cases. But beware, even some of the larger web data extraction companies have little to no compliance processes, so always ask about their compliance and best practices upfront.", "As we have seen, the compliance requirements for using web data in the investment decision making process can be quite demanding. However, the facts speak for themselves, web data is the most prevalent form of alternative data and can provide asset managers with a distinct informational advantage if used correctly.", "If the guidelines outlined in this article are followed, then there is no reason why web data can\u2019t contribute significant value to your firm without exposing yourself to undue compliance and regulatory risks.", "At Scrapinghub we have extensive experience developing data extraction solutions that overcome these challenges and mitigate the compliance risks associated with using web data in investment decision making.", "Our legal and engineering teams work with clients to evaluate the compliance associated with web scraping projects and develop data extraction solutions that enable them to reliably extract the data they need.", "If you have a need to start or scale your web data acquisition for your alternative data needs then our ", " are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."]},
{"tite": "St Patrick\u2019s Day Special: Finding Dublin\u2019s Best Pint of Guinness With Web Scraping", "date": "March 15, 2019 ", "author": "Ian Kerins", "blog_data": ["St Patrick\u2019s Day Special: Finding Dublin\u2019s Best Pint of Guinness With Web Scraping", "At Scrapinghub we are known for our ability to help companies make mission critical business decisions through the use of web scraped data.", "But for anyone who enjoys a freshly poured pint of stout, there is one mission critical question that creates a debate like no other\u2026", "\u201cWho serves the best pint of Guinness?\u201d", "So with St Patrick's day quickly approaching, we decided to turn our expertise in large scale data extraction to answering this mission critical question.", "Although this is a somewhat humorous question, the data extraction and analysis methods used are applicable to numerous high value business use cases and are used by the world\u2019s leading companies to gain a competitive edge in their respective markets.", "In this article, we\u2019re going to explore how to architect a web scraping and data science solution to find the best pint of Guinness in Dublin. But most importantly which Dublin pub serves the best pint of Guinness?", "For anyone who has ever enjoyed a pint of the black stuff, they know that the taste of a pint of Guinness is highly influenced by the skill of the person and the quality of the equipment they use.", "With that in mind, our first task is to identify where we can find web data that contains rich insights into the quality of a pub\u2019s Guinness and where the coverage levels are sufficient for all pubs in Dublin.", "After a careful analysis of our options - pub websites, social media, reviews, articles, etc. we decided customer reviews would be our best option. They provide the best combination of relevant high granularity data and coverage to answer this question.", "The next step would be to develop a web scraping infrastructure to extract this review data at scale using Scrapy. To do so we\u2019d need to create two separate types of spiders:", "We\u2019d also need to run these spiders on a web scraping infrastructure that can reliably extract the review data with no data quality issues. To do so, we\u2019d configure the web scraping infrastructure as follows:", "Due to data protection regulations such as GDPR, it is important that the extraction spiders don\u2019t extract any personal information of the customers who submitted the review. As a result, the data extraction spiders need to anomyonise the customer reviews.", "Once the unstructured review data was extracted from the site, the next step is to convert the text data into a collection of text documents, or \u201cCorpus\u201d, and pre-process the review data in advance of analysis.", "Natural Language Processing (NLP) techniques have difficulty modelling unstructured and messy text, preferring instead well defined fixed-length inputs and outputs. As a result, typically this raw data needs to be converted into numbers. Specifically, vectors of numbers. The more similar the words are, the closer the number assigned to the words are.", "The simplest way to convert a corpus to a vector format is the bag-of-words approach, where each unique word is represented by a unique number.", "To use this approach the review data first needs to be cleaned up and structured. Here are some of the common pre-processing steps that can be implemented using a library such as Python\u2019s NLTK, the most frequently used library in Python for text processing:", "The goal of this pre-processing step is to ensure the text corpus is clean and contains only the core words required for text mining.", "Once cleaned the review data then needs to be vectorised to enable analysis of the data. Here is an example review prior to it being vectorised:\u00a0", "Here was how the review should be represented once it has been vectorised using the bag-of-words approach. Each unique word is assigned a unique number, and the frequency of the words appearance recorded.", "Although there is no definitive method of achieving this goal, for the purposes of this project we decided not to overcomplicate things and instead do a simple analysis of the data to see what insights we can yield.", "One approach would be to filter the review data looking for the word \u201cguinness\u201d. This would enable us to identify all the reviews that specifically mention \u201cguinness\u201d, an essential requirement when trying to determine who pours the best pint of the black stuff.", "Next we need to create a way to determine if the mentioning of Guinness was done in a positive or negative context.", "One powerful method would be to build a classifier model using a labelled training dataset (30% of the overall dataset with reviews labelled as having positive or negative sentiment) developed with the Multinomial Naive Bayes library from Scikit-learn (a specialised version of Naive Bayes designed more for text documents) and apply our trained sentiment classifier model to the entire dataset. Categorising all the reviews as either positive or negative.", "To ensure the accuracy of these sentiment predictions, the results need to be analysed and compared to the actual reviews. Our aim is to have an accuracy of 90% and above.", "Finally, with a fully classified database of Guinness reviews we should now be in a position to analyse this data and determine which pub serves the best Guinness in Dublin.", "In this simple analysis project, we carried out analysis using the following assumptions and weighting criteria:", "Using this methodology we were able to get an interesting insight into the quality of Guinness in every bar in Dublin and find the best place to get a pint of the black stuff.", "So enough with the data science mumbo jumbo, what do our results say?", "\u00a0", ": Kehoes Pub - 9 South Anne Street", "Of the 74 reviews analysed, 36 display positive sentiment for pints of Guinness. 48.6% of all reviews. The highest ratio of reviews mentioning Guinness in a positive light and the highest number of total reviews mentioning Guinness in their reviews. A great sign that they serve the best Guinness in Dublin.", "To validate our results, the Scrapinghub team did our due diligence and sampled Kehoes\u2019 Guinness. We can safely say that those reviews weren\u2019t lying, great pint of stout!", "Worthy runners up\u2026", "John Kavanagh The Gravediggers - 1 Prospect Square", "Of the 54 reviews analysed, 25 display positive sentiment for pints of Guinness. 46.3% of all reviews.", "Mulligan\u2019s Pub - 8 Poolbeg St", "Of the 49 reviews analysed, 21 display positive sentiment for pints of Guinness. 42.9% of all reviews.", "So if you\u2019re looking for the best place to find a great pint of Guinness this Saint Patrick\u2019s Day, be sure to check out these great options.", "At Scrapinghub we specialize in turning unstructured web data into structured data. If you would like to learn more about how you can use web scraped data in your business then feel free to contact our", ", who will talk you through the services we offer startups right through to Fortune 100 companies.", "We always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on right now.", "Until next time\u2026", "Happy St Patrick's Day!\u00a0\u2618\ufe0f"]},
{"tite": "How to Architect a Web Scraping Solution: The Step-by-Step Guide", "date": "March 28, 2019 ", "author": "Ian Kerins", "blog_data": ["For many people (especially non-techies), trying to architect a web scraping solution for their needs and estimate the resources required to develop it, can be a tricky process.", "Oftentimes, this is their first web scraping project and as a result have little reference experience to draw upon when investigating the feasibility of a data extraction project.", "In this series of articles we\u2019re going to break down each step of Scrapinghub\u2019s four step solution architecture process so you can better scope and plan your own web scraping projects.", "At Scrapinghub we have a full-time team of solution architects who architect over 90 custom web scraping projects each week for everything from e-commerce and media monitoring, to lead generation and alternative finance use cases. So odds are if you are thinking of investigating a web scraping project our team has already architected a solution for something very similar.", "As a result, throughout this series we will be sharing with you the exact checklists and processes we use, along with some insider tips and rules of thumb that will make investigating the feasibility of your projects much easier.", "In this article, the first in the series, we\u2019re going to give you a high level overview of our solution architecture process so you replicate it for your own projects.", "The ultimate goal of the requirement gathering phase is to ", ", if possible to have zero assumptions about any variable so the development team can build the optimal solution for the business need.", "This is a critical process for us at Scrapinghub when working on customer projects so we can ensure we are developing a solution that meets their business need for web data, manage expectations and reduce risk in the project.", "However, the same is true if you are developing a web scraping infrastructure for your projects. Accurately capturing the project requirements will allow your development team to\u00a0ensure your web scraping project precisely meets your overall business goals.", "Here you want to capture two things:", "It is critical for you and/or your business that your data requirements accurately match your underlying business goals and your need for the data. A constant supply of high quality data can give your business a huge competitive edge in the market, but what is important is ", ".", "It is very easy to extract data from the web, what\u2019s difficult is extracting the right data at a frequency and data quality that makes it useful for your business processes.", "With every customer we speak with, we dive deep into their underlying business goal to better understand not only their specific data requirements, but also why they want the data and how it fits into the bigger picture. Because oftentimes, our team of solution architects are able to work with them to find the alternative or additional data sources for their specific requirements that better suit their business goals.", "However, when investigating the feasibility of any web scraping project you should always be trying to clarify:", "In this article (coming soon), we will walk you through the exact steps our team uses to gather project requirements and scope the best possible solution to meet them.", "The second step of any solution architecture process is to check if there are any legal barriers to extracting this data.", "With the increased level of awareness about data privacy and web scraping in the last number of years, ensuring your web scraping project is legally compliant is now a must. Otherwise you could land you or your company in a lot of bother.", "In this article (coming soon), we will share with you our exact legal assessment checklist that our solution architecture team uses to review every project request we receive. Our legal team has created a best practice guide for the solution architecture team to utilise so they know when to flag a project with legal for a review. Once flagged with legal, our legal team will review the project based on the criteria below, as well as others, to determine if we are able to proceed with the project.", "However, in general you need to be assessing your project against the following criteria:", "If your answers to any of the above questions raise concerns, you should be ensuring that a thorough legal review of the issue is conducted prior to scraping. Once you've completed this review then you are in a good position to move forward to assessing the technical feasibility and architecting your web scraping solution.", "Assuming your data collection project passed the legal review, the next step in the solution architecture process is to assess the technical feasibility of executing the project successfully.", "This is a critical step in our solution architecture process and a step most independent developers working on their own or for their company\u2019s projects overlook.", "There is a strong tendency amongst developers and business leaders to start developing their solution straight away. For simple projects this often isn\u2019t an issue, however, for more complex projects, developers can quickly discover that they run into a brick wall and can\u2019t overcome the challenges.", "We\u2019ve found that a bit of upfront testing and planning, can save countless wasted man hours down the line if you start developing a fully featured solution only to hit a technical brick wall.", "During the technical review phase, one of our solution architects will examine the website and run a series of small scale test crawls to evaluate the technical feasibility of developing a solution that meets the customers requirements (crawl speed, coverage and budgetary requirements).", "These tests are primarily designed to determine the difficulty of extracting data from the site, will there be any limitations on crawl speed & frequency, is the data easily discoverable, is there any post-processing or data science requirements, does the project require any additional technologies, etc.", "Once complete, this technical feasibility review gives the solution architect the information they need to firstly determine if the project is technically feasible and then what is the optimal architecture for the solution.", "In this article (coming soon), we\u2019ll give you a behind the scenes look at some of the tests we carry out when assessing the technical feasibility of our customers projects.", "The final step in the process is architecting the solution and estimating the technical and human resources required to deliver the project.", "Oftentimes, the solution need to be approached and scoped in phases, to balance the tradeoff of timeline, budget, and technical feasibility. Our team will propose the best first step to tackle your project while keeping the bigger goal in mind.", "Here you need to architect a web scraping infrastructure using the following building blocks:", "In this article (coming soon), we share the process we use to architect a solution, give examples of real solutions we have architected and the resources required to execute them.", "Once a solution has been architected and the resources estimated, our team has all the information they need to present the solution to the customer, estimate the cost of the project and draft a statement of work capturing all their requirements and the proposed solution.", "At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.", "Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.", "If you have a need to start or scale your web scraping project then our ", " experts are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0", "\u00a0"]},
{"tite": "Solution Architecture Part 2: How to Define The Scope of Your Web Scraping Project", "date": "April 05, 2019 ", "author": "Ian Kerins", "blog_data": ["In this the second post in our solution architecture series, we will share with you our step-by-step process for data extraction requirement gathering.", "As we mentioned in the first post in this series, the ultimate goal of the requirement gathering phase is to ", ", if possible to have zero assumptions about any variable so the development team can build the optimal solution for the business need.", "As a result, accurately defining project requirements most important part of any web scraping project.", "In this article we will discuss the four critical steps to scoping every web scraping project and the exact questions you should be asking yourself when planning your data extraction needs.", "The requirement gathering process can be broken into two parts: 1) understanding the business needs, and 2) defining the technical requirements to meet those needs.", "The business need is at the core of every web scraping project, as it clearly defines what objective do they want to achieve.", "Once, you\u2019ve clearly defined what you\u2019d like to achieve then it is possible to start gathering the technical requirements. ", "At the start of every solution architecture process, our team will put a huge amount of focus on trying to understand our customers underlying business needs and objectives. Because when we truly understand your objectives, we can assist you in selecting the best data sources for your project and be able to best optimise the project if there is any rescoping or trade offs required due to technical or budgetary constraints.", "Questions to ask yourself are:", "On the surface these questions may seem very simple, but we\u2019ve found that in a lot of cases customers come to use without a clear idea of what web data they need and how they can use it to achieve their business objectives. So the first step is to clearly identify what type of web data you need to achieve your business objectives.", "Once, everyone has a clear understanding of the business objectives, then it is time to define the technical requirements of the project i.e. how do we extract the web data we need.", "There are four key parts to every web scraping project:", "We will look at each one of these individually, breaking down why each one is important and the questions you should be asking yourself at each stage.", "The first step of the technical requirement gathering process is defining what data needs extracting and where can it be found.", "Without knowing what web data you need and where it can be found most web scraping projects aren\u2019t viable. As a result, getting a clear idea of these basic factors are crucial for moving forward with the project.", "Questions to ask yourself are:", "By asking yourself these questions you will be able to really start to define the scope of the project.", "The next step in the requirement gathering process is digging into extracting the data\u2026", "During this step our goal is to clearly capture what data do we want to extract from the target web pages. Oftentimes, there is vast amounts of data available on a page so the goal here is to focus in on the exact data the customer wants.", "One of the best methods to clearly capturing the scope of the data extraction is to take screenshots of the target web pages and mark with fields need to be extracted. Oftentimes during calls with our solution architects we will run through this process with customers to ensure everyone understands exactly what data is to be extracted.", "Questions to ask yourself are:", "A general rule of thumb is that the more data being extracted from a page, the more complex the web scraping project. Every new data type will require additional data extraction, data quality assurance checks, and maybe more technical resources in certain circumstances (i.e. a headless browser if a data type is rendered via javascript, or if multiple requests are required to access the target data).", "Once this step is complete we then look to estimate the scale of the project...", "Okay, by this stage you should have a very good idea of the type of data you want to extract and how your crawlers will find and extract it. Next, our goal is to determine the scale of the web scraping project.", "This is an important step as it allows you to estimate the amount of infrastructural resources (servers, proxies, data storage, etc.) you\u2019ll need to execute the project, the amount of data you\u2019re likely to receive and the complexity of the project.", "The three big variables when estimating the scale of your web scraping projects are the:", "After following steps 1 & 2 of the requirement gathering process, you should know exactly how many websites you\u2019d like data extracted from (variable 1). However, estimating the number of records that will be extracted (variable 2) can be a bit trickier.", "In some cases (often in smaller projects), it is just a matter of counting the number of records (products, etc.) on a page and multiply by the number of pages. In certain situations the website will even list the number of records on a website.", "However, there is no one size fits all solution for this question. Sometimes it is impossible to estimate the number of results you will extract, especially when the crawl is extracting hundreds of thousands or millions of records. In cases like these often the only way to know for sure how many records the crawl will return is by actually running the crawl. You can guesstimate, but you\u2019ll will never know for sure. ", "The final factor to take into account is how often would you like to extract this data? Daily, weekly, monthly, once off, etc? ", "Also, do you need the data extraction completed within a certain time window to avoid changes to the underlying data?", "It goes without saying that the more often you extract the data, the larger the scale of the crawl. If you go from extracting data monthly to extracting data daily you are in effect multiplying the scale of the crawl by a factor of 30.", "In most circumstances, increasing the scale from monthly to daily won\u2019t have much of an effect other than increasing the infrastructural resources required (server bandwidth, proxies, data storage, etc.). ", "However, it does increase the risk that your crawlers will be detected and banned, or put excessive pressure on the websites servers. Especially, if the website typically doesn\u2019t receive a large volume of traffic each day. ", "Sometimes the scale of the crawl is just too big to complete at the desired frequency without crashing a website or requiring huge infrastructural resources.Typically this is only an issue when extracting enormous quantities of data on an hourly or less basis, such as monitoring every product on an e-commerce store in real-time. During the technical feasibility phase we normally test for this. ", "The other factor to take into account when moving from a once-off data extraction to a recurring project is how do you intend to process the incremental crawls? ", "Here you have a number of options:", "The crawlers can be configured to do this or else they can just extract all the available data during each crawl and you can post-process it to your requirements afterwards.", "Finally, the last step of the project scoping process is defining how do you want to interact with the web scraping solution along with how do you want the data delivered.", "If you are building the web scraping infrastructure yourself, you really only have one option: you\u2019re managing the data, the web scraping infrastructure and the underlying source code. ", "However, when customers work with Scrapinghub (or other web scraping providers) we can offer them a number of working relationships to best meet their customers needs. Here are some of Scrapinghub\u2019s most common working relationships:", "The next question is in what format do you want the data and how you would like the data delivered. These questions are largely dependant on how you would like to consume the data and the nature of your current internal systems. There are a huge range of options for both, but here are some examples:", "These considerations are typically more relevant when you are working with an external web scraping partner.", "So there you have it, they are the four steps you need to take to define the scope of your web scraping project. In the next article in the series we will share with you how to take that project scope and conduct a legal review of the project. ", "At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.", "Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.", "If you have a need to start or scale your web scraping project then our ", " are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."]},
{"tite": "Scrapinghub\u2019s New AI Powered Developer Data Extraction API for E-Commerce & Article Extraction", "date": "April 09, 2019 ", "author": "Ian Kerins", "blog_data": ["Today, we\u2019re delighted to announce the launch of the beta program for\u00a0Scrapinghub\u2019s new AI powered developer data extraction API for automated product and article extraction.", "After much development and refinement with alpha users, our team have refined this machine learning technology to the point that data extraction engine is capable of automatically identifying common items on product and article web pages and extracting them without the need to develop and maintain individual web crawlers for each site.", "Enabling developers to easily turn unstructured product and article pages into structured datasets at a scale, speed and flexibility that is nearly impossible to achieve when manually developing spiders.", "With the AI enabled data extraction engine contained within the developer API, you now have the potential to extract product data from 100,000 e-commerce sites without having to write 100,000 custom spiders for each.\u00a0", "As result, today we\u2019re delighted to announce the launch of the developer API's public beta.", "If you are interested in e-commerce or media monitoring and would like to get early access to the data extraction developer API then be sure to ", "When you sign up to the beta program you will be issued an API key and documentation on how to use the API. From there you are free to use the developer API for your own projects and retain ownership of the data you extracted when the beta program closes.", "What's even better, the beta program is completely free. You will be assigned a daily/monthly request quota which you are free to consume as you wish.", "The beta program will run until July 9th, so if you\u2019d like to be involved then be sure to ", " as places are limited.", "Once you\u2019ve been approved to join the beta program and have received your API key, using the API is very straightforward.", "Currently, the API has a single endpoint: ", ". A request is composed of one or more queries where each query contains a URL to extract from, and a page type that indicates what the extraction result should be (product or article).", "Requests and responses are transmitted in JSON format over HTTPS. Authentication is performed using HTTP Basic Authentication where your API key is the username and the password is empty.", "To make a request simply send a POST request to the API along with your API key, target URL and pageType (either article or product):", "Or, in Python:", "To facilitate query batching (see below) API responses are wrapped in a JSON array. Here is an article from our blog that we want to extract structured data from:", "And the response from the article extraction API:", "As mentioned previously the developer API is capable of extracting data from two types of web pages: ", "and ", "pages.", "The product extraction API enables developers to easily turn product pages into structured datasets for e-commerce monitoring applications.", "To make a request to the product extraction API, simply set the \u201cpageType\u201d attribute to \u201cproduct\u201d, and provide the URL of a product page to the API. Example:", "The product extraction API is able to extract the following data types:", "All fields are optional (can be null), except for url and probability.", "The article extraction API enables developers to easily turn articles into structured datasets for media monitoring applications.", "To make a request to the article extraction API, simply set the \u201cpageType\u201d attribute to \u201carticle\u201d, and provide the URL of an article to the API. Example:", "The article extraction API is able to extract the following data types:", "Similarly to the product extraction API, all article extraction fields are optional (can be null), except for url and probability.", "Both the product and article extraction API offer the ability to submit multiple queries (up to 100) in a single API request:", "The API will return the results of the extraction as the data extraction receives them, so query results are not necessarily returned in the same order as the original query.", "If you need an easy way to associate the results with the queries that generated them, you can pass an additional \"meta\" field in the query. The value that you pass will appear as a \"userMeta\" field in the corresponding query result. For example, you can create a dictionary keyed on the \"meta\" field to match queries with their corresponding results:", "If you would like to learn more about the developer API\u2019s functionality and how you can use it for your specific projects then check out the API documentation (will be sent to you when you sign up)."]},
{"tite": "ScrapyRT: Turn Websites Into Real-Time APIs", "date": "May 14, 2019 ", "author": "Pawel Miech", "blog_data": ["If you\u2019ve been using Scrapy for any period of time, you know the capabilities a well-designed Scrapy spider can give you.", "With a couple lines of code you can design a scalable web crawler and extractor that will automatically navigate to your target website and extract the data you need. Be it e-commerce, article or sentiment data.", "The one issue that traditional Scrapy spiders poses however, is the fact that in a lot of cases spiders can take a long time to finish their crawls and deliver their data if it is a large job. Making Scrapy spiders unsuitable for near real-time web scraping. A growing and very useful application for many data aggregation and analysis efforts.", "With the growth of data based services and data-driven decision making, end users are increasingly looking for ways to extract data on demand from web pages instead of having to wait for data from large periodic crawls.", "And that\u2019s where ScrapyRT comes in\u2026", "Originally evolving out of a ", " project in 2014, ", " (Scrapy Realtime) is an open source Scrapy extension that enables you to control Scrapy spiders with HTTP requests.", "Simply send your Scrapy HTTP API a request containing the ", " (with URL and callback as parameters) and the API will return the extracted data by the spider in real-time. No need to wait for the entire crawl to complete.", "Usually, spiders are run for long periods of time, and proceed step by step, traversing the web from a starting point and extracting any data that matches their extraction criteria.", "This mode of operation is great if you don\u2019t know the location of your desired data. However, if you know the location of the data then there is a huge amount of redundancy if the spider has to complete all the intermediary steps.", "ScrapyRT allows you to schedule just one single request with spider, parse it in callback, and get response returned immediately as JSON instead of having the data saved in a database.", "By default spider\u2019s start_requests spider method is not executed and the only request that is scheduled with spider is Request generated from API params.", "ScrapyRT\u2019s architecture is very simple. It is a web server written in Python Twisted tied with custom Crawler object from Scrapy.", "Twisted is one of the most powerful Python asynchronous frameworks, so was a natural choice for ScrapyRT as Twisted works great for asynchronous crawling and Scrapy uses Twisted for all HTTP traffic. Ensuring easy integration with Scrapy.", "Once added to your project, ScrapyRT runs as a web service, retrieving data when you make a request containing the URL you want to extract data from and the name of the spider you would like to use.", "ScrapyRT will then schedule a request in Scrapy for the URL specified and use the \u2018foo\u2019 spider\u2019s parse method as a callback. The data extracted from the page will be serialized into JSON and returned in the response body. If the spider specified doesn\u2019t exist, a 404 will be returned.", "The ScrapyRT web server is customizable and modular. You can easily override GET and POST handlers. This means that you can add your own functionality to the project - you can write your own ", " inheriting from main ScrapyRT handlers, for example you can write code to return response in XML or HTML instead of JSON, and add it to configuration.", "One thing to keep in mind is that ScrapyRT was not developed with long crawls in mind.", "Remember: after sending a request to ScrapyRT you have to wait for the spider to finish before you get response.", "So if the request requires the spider to crawl an enormous site and generates 1 million requests in callback, then ScrapyRT isn\u2019t the best option for you as you will likely have to sit in front of a blank screen waiting for your crawl to finish and return the item.", "One possible way of solving this problem would involve modifying ScrapyRT so that it could use websockets or HTTP push notifications - this way API could send items as they arrive in API.", "Currently data from spider is returned in response to initial request - so after sending each request you have to wait for the response until data is returned by spider. If you expect that your spider will generate lots of requests in callback but you actually don\u2019t need all of them you can limit the amount of requests by passing max_requests parameter to Scrapy.", "If you would like to learn more about ScrapyRT or contribute to the open source project, then check out the ScrapyRT ", " and ", ".", "At Scrapinghub we specialize in turning unstructured web data into structured data. If you have a need to start or scale your web scraping projects then our ", " is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!\u00a0"]},
{"tite": "Looking Back at 2018", "date": "December 21, 2018 ", "author": "Ian Kerins", "blog_data": ["What a year 2018 has been for Scrapinghub!!", "It\u2019s hard to know where to start\u2026", "This year has seen tremendous growth at Scrapinghub, setting us up to have a great 2019.", "Here are some of the highlights of 2018\u2026", "Data continues to be an important element of our customers business strategy and to support this we saw continued growth in the amount of data we help them extract from over 108.6 billion pages in 2018, an average of over 9 billion pages per month.", "What is more, the number of items we extracted from those pages rose by 102% from 103 billion in 2017 to 209 billion in 2018. Signifying the growing appetite for every more new data types amongst our customers.", "To deliver this level and scale of service for our customers we continued to invest heavily in our core technologies ensuring continued reliability and robustness of Scrapy Cloud and Crawlera.", "Building on the improvements we made in 2017, we again focused on improving the technical capabilities of the platform to ensure the 1M Developers who rely on our tools can get the data they need.", "Putting this into numbers we saw another year of growth in the platform with nearly 120,000 developers signing up to use Scrapy Cloud.", "What\u2019s more behind the scenes we\u2019ve been working on an overhaul of the platforms user interface which will be released in early 2019. We\u2019re super excited!!!", "Crawlera has gone from strength to strength in 2018. This year Crawlera processed over 90 billion successful requests, up from 60.5 billion in 2017.", "\u00a0", "\u00a0", "This year, Shubbers were active as ever in the open source, remote working and developer communities.", "Scrapinghub participated in and were sponsors of the Running Remote conference in Bali and attended the ", " conference in Ireland, along with attending numerous other remote working conferences and meetups.", "Sanaea Daruwalla, Scrapinghub\u2019s Head of Legal, was invited to speak at the annual IAPP Europe Data Protection Congress in Brussels, to ", ".", "And always, our developers were active participants and contributors to the open source community along with attending countless developer conferences and meetups throughout the year.", "It seemed like not a week went by without us welcoming a new member to the team on Slack. Which is actually not an exaggeration, as over the course of 2018 the number of Shubbers has grown 47%.", "An increase of 69 people, which is more than one per week. So it was no wonder it felt like we were welcoming new people to the team every day!", "Meaning that we are positioned to support our ever growing customers base in 2019 and continue to advance the technologies for web scrapers around the world.", "But doesn\u2019t mean we\u2019re planning on stopping here...", "We are still looking to hire world-class people in all areas of our business, from developers and data scientists, to salespeople, marketers and support functions. So if you\u2019re interested in working with one of the best remote working companies around then check out ", ".", "With such a fast growing fully distributed team, we continued our efforts to ensure our culture and company values were alive and well across the organisation.", "We re-launched our values after much collaboration and input from fellow Shubbers, expanded our Shubber praise and recognition programme, created cross functional teams to support global collaboration within a remote environment, set up the Shubber Voice to allow people to voice their ideas for the company, and WaterCooler channels to enable Shubbers to talk and gossip whilst being thousands of kilometres apart.", "But no year is complete without our signature Shubber GetTogethers\u2026", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "This November 120 Shubbers travelled from all over the world to Lisbon to meet up, make new friends and discuss the company direction.", "We held a two-day Town Hall where every Scrapinghub team got a chance to share the progress they\u2019ve made over the last year and their plans for the coming year.", "\u00a0", "\u00a0", "And of course, it wasn\u2019t all work and no play. There was plenty of time for workshops and team bonding to strengthen the Scrapinghub team.", "\u00a0", "2018 was the year we completed our senior leadership team, putting experienced leaders in charge of each aspect of Scrapinghub\u2019s key areas. Our leadership team is:", "\u00a0", "And we welcomed...", "Ensuring we have a 50/50 gender split amongst our Senior Leadership team. All in all, having our SLT in place gives Scrapinghub a solid foundation to grow and expand in the years to come.", "With the ever-increasing use of big data and artificial intelligence by the world\u2019s leading companies there is a huge appetite for reliable sources of high-quality web data, so 2019 is shaping up to be a great year for Scrapinghub.", "Not only that, we also have numerous new products and product enhancements in the works that will increase our capabilities and ensure we can better serve our customers in 2019.", "If you\u2019d like to be part of this journey then ", ", so see if any of our open roles takes your fancy. Until next year...Happy Scraping!\u00a0"]},
{"tite": "From The Creators Of Scrapy: Artificial Intelligence Data Extraction API", "date": "April 11, 2019 ", "author": "Ian Kerins", "blog_data": ["To accurately extract data from a web page, developers usually need to develop custom code for each website. This is manageable and recommended for tens or hundreds of websites and where data quality is of the utmost importance, but if you need to extract data from thousands of sites, or rapidly extract data from sites that are not yet covered by pre-existing code, this is often an insurmountable challenge.", "The complex and resource intensive nature of developing code for each individual website, acts as a bottleneck severely curtailing the scope of companies data extraction and analysis capabilities.", "Nowhere has this need for real time data extraction at scale being more needed than in e-commerce and media monitoring. Where the ability to monitor products on any online e-commerce store or monitor news from thousands of media outlets would take a company\u2019s business intelligence capabilities to a completely new level.", "Scrapinghub\u2019s new", " has been specifically designed for real-time e-commerce & article extraction at scale, and we\u2019re now ", ".", "At the core of the ", " able to extract data from a web page without the need to design custom code. Through the use of deep learning, computer vision and Crawlera, Scrapinghub\u2019s advanced proxy management solution, the data engine is able to automatically identify common items on product and article web pages and extract them without the need to develop and maintain extraction rules for each site.", "With this AI technology, developers and companies now have the ability to extract product data from e-commerce sites without having to write custom data extraction code for each website.", "As with any machine learning based solution, the coverage and accuracy of the output is open to more inaccuracies compared to custom developed code.", "However, after much testing and refinement with alpha users, our data science team have improved our machine learning technology and operational processes to the point that the data extraction engine is capable of yielding commercially viable data quality for users.", "Key to this success, has been Scrapinghub\u2019s 10+ year experience being at the forefront of web scraping technologies and extracting over 8 billion pages per month. This experience and scale has enabled us to overcome a lot of the technical challenges faced by AI enabled data extraction engines and design a solution that is viable for commercial applications.", "Ideally suited for developers the API offers a flexible and highly scalable data extraction engine for large scale data analysis and visualisation applications. Especially:", "The AI enabled web scraping technology used as part of the API has the potential to unlock the web's full potential, turning the web into the world\u2019s largest structured database.", "Now instead of having to manually develop and maintain code for each new website, you can simply configure your applications to send it\u2019s queries to the developer API and receive structured data ready for analysis in response.", "Not only does this capability enable developer teams to build highly scalable data extraction capabilities, it also enables data science teams to rapidly prototype and test the value of data science projects, and stands as a backup to your existing custom built code if they were ever to break.", "Currently there are two versions of the API designed for two separate use cases:", "Although we are initially focused on providing the API for product and article extraction, overtime we plan to expand the types of data the API can automatically extract to include company/people profile data, real estate, reviews, etc. Further enhancing the accessibility of the web\u2019s data.", "If you are interested in e-commerce or media monitoring and would like to get early access to the data extraction developer API then be sure to sign up to the public beta program", "When you sign up you will be issued an API key, along with documentation on how to use the API. From there you are free to use the developer API for your own projects and retain ownership of the data you extracted when the beta program closes.", "What's even better, there is zero cost involved with the beta program. You will be assigned a daily/monthly request quota which you are free to consume as you wish."]},
{"tite": "Web Data Analysis: Exposing NFL Player Salaries With Python", "date": "May 09, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["Football. From throwing a pigskin with your dad, to crunching numbers to determine the probability of your favorite team winning the Super Bowl, it is a sport that's easy to grasp yet teeming with complexity. From game to game, the amount of complex data associated with every team - and every player - increases, creating a more descriptive, timely image of the League at hand.", "By using web scraping and data analytics, the insights that we, the fans, crave - especially during the off-season - become accessible. Changing agreements, draft picks, and all the juicy decisions made in the off-season generate a great deal of their own data.", "By using a few simple applications and tricks we can see the broader patterns and landscapes of the sport as a whole. Salary data is especially easy to scrape, and for the purposes of this article we\u2019ll build a scraper to extract this data from the web, then teach you how to use simple data analytics to answer any questions we might have.", "Amid this sea of data, interesting insights appear, like the correlation between age and salary, who gets the biggest and smallest paycheck, or which position brings in the most cash. To get the information we need to make these conclusions, we\u2019re going to extract NFL players\u2019 salary data from a public source. Then, using that data, we\u2019ll create adhoc reports to unearth patterns hidden in the numbers.", "In this article, guest writer Attila T\u00f3th, founder of ScrapingAuthority.com, will give you a bird\u2019s eye view of how to scrape, clean, store, and analyze unstructured public data.\u00a0In addition to the code, we\u2019ll demonstrate our thought process as we go through each step. Whether you want to build a scrapy spider or only work on data analysis, there\u2019s something here for everybody.", "The site we\u2019re going to scrape is ", ", a site that has all kinds of information about NFL player contracts and salaries.", "For this project we will use the Scrapy web scraping framework for data extraction; a MySQL database to store said data; a pandas library to work with data, and the tried-and-true matplotlib for charting.", "We will use the follow process to complete the project:", "Here is example URL of the quarterback page we want to extract data from:", "However, we will be extracting data for all positions.", "Our spiders will be designed to extract data from the following fields: ", ", ", ".", "As always, we first need to inspect the website to be able to extract the target data.\u00a0", "As we can see, each field is inside a separate ", "tag which makes it easy to extract the data. An important thing to keep in mind is that all the fields inside a row tag belong to only one item in our spider. Now let\u2019s figure out how to actually fetch these fields.", "\u00a0Let\u2019s kick off with starting a new Scrapy project:", "In order to fetch the data with Scrapy, we need an item defined in the ", " file. We will call it ContractItem:", "\u00a0Now we\u2019re done with defining the item, let's start coding the spider.", "There are two challenges we need to overcome when designing our spider:", "There are multiple ways to implement this spider, however, for this project we decided to use an approach which is the most straightforward and simple.", "First, create a new spider python file and inside the file a class for our spider:", "Then define ", " and ", ":\u00a0", "Now we need to implement the ", " function. Inside this function we usually populate the item. However, in this case we first need to make sure that we\u2019re grabbing data from all the position pages (quarterback, running-back, etc.).", "In the parse function we are doing exactly that: finding the URLs and requesting each of them so we can parse them later:", "So what does our parse function actually do here?", "The parse function iterates over each dropdown element (positions). Then inside the iteration we extract the URL for the actual page where we want to get data from. Finally, we request this page and pass the position info as metadata for the ", " function, which will be coded next.", "The ", " function will need to deal with actual data extraction and item population. As we previously learned from inspecting the website, we know that each table row contains one item. So we\u2019ll iterate over each row to populate each item. Easy right?", "But remember that we want the data to be written into a database at the end. To simplify that, we\u2019re using item loaders and i/o processors to clean the extracted data.", "The next step is to implement the input processors which will clean our freshly extracted data.", "Let\u2019s make use of item loader input and output processors. There are two main tasks that the processors need to accomplish: ", " and ", ". Removing html tags is important because we don\u2019t want messy data in our database.", "The conversion part is important because our database table, which we will write into, will have number and text columns as well. And so we need to make sure that we are inserting actual numbers as number fields and cleaned text data as text fields.", "The fields that need processing (other than removing html tags) include: a", ".", "As all of these fields are number fields they need further processing:", "At this point our extracted data is clean and ready to be stored inside a database. But before that, we need to remember that we likely have plenty of duplicates because some players hold multiple positions. For example, a player can be a safety and a defensive-back at the same time. So this player would be in the database multiple times. We don\u2019t want redundant data corrupting our data set, so we need to find a way to filter duplicates whilst scraping.", "The solution for this is to write an item pipeline which drops the actual item if the player has already been scraped. Add this pipeline to our ", " file:", "This pipelines works by continuously saving scraped player names in", ", which is a set. If the player has already been processed by the spider it raises a warning message and simply drops (ignores) the item. If the currently processed player has not been processed yet then no need to drop the item.", "One of the best ways to write a database pipeline in Scrapy is as follows:", "2. Create new pipeline class and a constructor to initiate db connection.", "3. Override", "function to get db settings.", "4. Insert item into database", "5. Close db connection when the spider finishes", "We\u2019ve extracted data. Cleaned it. And saved it into a database. Finally, here comes the ", " part: trying to get some actual insights into NFL salaries.", "Let\u2019s see if we find something interesting. For each report, we\u2019re going to use pandas dataframes to hold the queried data together.", "Now, let\u2019s create some ad hoc reports using the database we\u2019ve generated to answer some simple questions, like \u201cWho is the youngest player?\u201d and \u201cWhen will the longest active contract end?\u201d", "To do so, let\u2019s fetch all the data into a dataframe. We will need to create a new dictionary based on the dataframe to get min, max, mean and median values for all the data fields. Then create a new dataframe from the dict to be able to display it as a table.", "This table contains some basic descriptive information about our data.", "Here lets query the data set to identify which players have the highest and lowest salaries:", "The minimum yearly salary for an NFL player is $480,000 (in 2019 season it\u2019s going to be $15,000 more). Tanner Lee, 24, QB for the Jags, is the only player who got this amount of money, in his rookie season. For some unknown reason, other teams stick to $487,500 when it comes to minimum payment.", "The top five players with the biggest paycheck are all QBs, which is not surprising. Aaron Rodgers is the 1st with a yearly average salary of $33,500,000. The youngest in the top five is Jimmy Garoppolo, 28, with a solid $28,000,000 average yearly salary. Except Kirk Cousins, they all have long-term contracts (4+ years remaining).", "Next lets see which players have the longest contracts until they become a free-agent:", "There are four players whose teams decided they want to tie their players down to long term contracts. 3 out of 4 are defensive players. 2 out of 4 are linemen. The top 3 players are 28 or 29 years old. Landon Collins is the only player who is younger, he\u2019s only 25 but has been in the NFL since 2015.", "Next, let's look at the contract length distribution across players to see what is the average active contract length:", "\u00a0", "What this chart really shows is that currently, a huge amount of players\u2019 contracts will end next year and if nothing happens before then they\u2019ll become free agents. Almost 1,000 players have a year remaining in their contracts. More than 600 have 2 years left. Less than 400 players have 3 years left, and just a handful of players have 4, 5 or 6 year contracts.", "To dig a little deeper we then looked at the correlation between a players age and their contract length:", "This graph shows the correlation between a player\u2019s age and the remaining years of his contract (contract length). There\u2019s a downhill trend between age 22 and 26, which probably suggests that most of players of this age still have their rookie contracts. At age 26 they start negotiating new contracts hence the uphill climb. The average rookie contract is a 3-year deal but after that an average player receives a 1 or 2-year contract.", "Most of the players are either 24, 25 or 26 years old. 26 is the average and median as we mentioned it before.", "\u00a0", "Some quick remarks:", "Now, this is a really simple chart. It appears that the more years left in a player\u2019s contract (the longer the contract,) the more average yearly salary he gets. There's something special that happens after the 3 year mark because there\u2019s a huge pay rise if the contract is longer than 3 years.", "We covered website inspection, data extraction, cleaning, processing, pipelines and descriptive analysis. If you\u2019ve read this far I hope this article has given you some practical tips on how to leverage web scraping. Web scraping is all about giving you the opportunity to gain insights and make data-driven decisions. Hopefully this tutorial will inspire future projects and exciting new insights.", "At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.\u00a0If you have a need to start or scale your web scraping project then our", "are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.", "Thanks for reading!", "The full source code of this tutorial is available on Github", "-----------------------------------------------------------------------------------------------------------------------------", "\u00a0", "Founder of ", " where he teaches web scraping and data engineering. Expertise in designing and implementing web data extraction and processing solutions.", "\u00a0", "\u00a0", "\u00a0", "\u00a0"]},
{"tite": "Solution Architecture Part 3: Conducting a Web Scraping Legal Review", "date": "May 23, 2019 ", "author": "Sanaea Daruwalla", "blog_data": ["In this the third post in our solution architecture series, we will share with you our step-by-step process for conducting a legal review of every web scraping project we work on.", "At Scrapinghub, it\u2019s absolutely critical that our services respect the rights of the websites and companies whose data we scrape. Scraping, as a process, is not illegal - however, the data you extract, the manner in which you extract the data, and what exactly you\u2019re scraping all need to be held to rigorous legal standards to ensure legal compliance.", "In ensuring that your solution architecture follows both legal guidelines as well as industry best practices, we\u2019ve established a checklist for your ease and to protect the reputation and integrity of web scraping as a practice. Personal and commercial data regulations are in flux across the world, and given the inherently international nature of the internet, establishing clearly legal practices within your solutions should be considered an executive priority.", "In this article we will discuss the three critical legal checks you need to make when reviewing the legal feasibility of any web scraping project and the exact questions you should be asking yourself when planning your data extraction needs.", "Data comes in all shapes and sizes. However, before we start extracting this data, we need to determine the exact status and legality of extracting this data for each project.", "There are three forms of data that can be present a legal risk if extracted:", "1. ", "2. ", "3. ", "However, the first step of the legal review process is to identify the use case for the data - i.e. what will the data be used for and do you have the data owners explicit consent to extract, store and use their data.", "The ultimate use case of the data can have a large bearing on the legal status of scraping the data from a website, particularly in the case of personal data which we will discuss later.", "So the first step of any legal review process is to define:", "Once this has been defined, you will be in a position to carry out your legal checks.", "Personal data, or personally identifiable information (PII) as it is technically known, is any data that could be used to directly or indirectly identify a specific individual. With the increased awareness and regulation governing how personal data is used, extracting personal data has resulted in increasingly stringent data protection regulations coming into force - the General Data Protection Regulation, or GDPR, is a prime example.", "First, you need to check whether you plan to extract any form of personal data. Common examples include:", "If you\u2019re not extracting any personal data, then you can move onto the next step of the legal review. However, if you are extracting any of the personal data types listed above then you need to investigate the data protection regulations associated with this data.", "Every legal jurisdiction (US, EU, etc.) has different regulations governing personal data. So the next step is to identify which jurisdiction do the owners of this personal data reside: the EU, US, Canada, etc.", "For a detailed step-by-step process for evaluating the legal regulations of the personal data you want to extract then be sure to check out our ", ".", "Copyrighted data generally describes content owned by businesses and individuals with explicit control over its reproduction and capture. Just because web data is publicly available on the internet doesn\u2019t mean that anyone can extract and store the data.", "In some cases the data itself might be copyrighted, and depending on how/what data you extract you could be found to have infringed the owner\u2019s copyright, creating additional risks for the users of this data.", "First, you need to check whether you plan to extract any form of data that are at risk of being subject to copyright. Common examples include:", "If you are extracting any of these forms of web data, then you need to determine if you will violate copyright by extracting and using the data in your projects.", "Cases like these need to be evaluated on a case-by-case basis as copyright issues often aren\u2019t black and white like personal data issues, they are sometimes surmountable if there is a valid exception to copyright within your use case. Some methods to achieve this are:", "Database rights is a subset of copyright, that needs further explanation on it\u2019s own. A database is any organized collection of materials that permits a user to search for and access individual pieces of information contained within the materials.", "Database rights can create additional risks for the use of web data in your projects, if the data hasn\u2019t been extracted in a compliant manner.", "In the US, a database is protected by copyright when the selection or arrangement is original and creative. Copyright only protects the selection and organization of the data, not the data itself.", "In the EU, databases are protected under the Database Directive which offers much broader protection for EU databases. The Directive has two purposes: (1) protect IP, like in the US, and (2) protect the work and risk in creating the database.", "If you believe a data source might fall under database rights then decision makers should always consult with their legal team before scraping the data and ensure they either:", "Copyright can be a tricky topic, so it is always best to talk to a qualified legal professional prior to scraping potentially copyrightable data for your projects. At Scrapinghub, every web scraping project request we receive is reviewed for copyright issues by our legal team prior to commencing the project. Ensuring our clients know they are extracting data in a legally compliant manner.", "Extracting data from a website that first requires you to login to access the data can raise potential legal issues. In most situations, logging it requires you to accept the terms and conditions of the website which might explicitly state that automatic data extraction is prohibited.", "If this is the case, you should review the terms and conditions to determine whether you would be in breach of the T&C\u2019s by extracting data from the website. As the terms and conditions of some of these websites can sometimes be quite intricate, it is advisable that you have them reviewed by an experienced legal professional prior to scraping data from behind the login.", "In order to maintain compliance with today\u2019s data regulations, it\u2019s incredibly important to keep your legal team up-to-date and to ensure data protection specialists regularly monitor your scraping operation. Legal checks are integral to keeping compliant and demonstrate goodwill - furthermore, by performing consistent legal assessments of your projects, you can streamline the scraping process and make absolutely certain that your scraping remains respectful and productive.", "As we have seen, there is more to web scraping than just the technical implementation of a project. There are numerous legal compliance requirements that need to be taken into account when deciding if a web scraping project is viable.", "If the guidelines outlined in this article are followed, then there is no reason why you can\u2019t extract data from the web without exposing yourself to undue compliance and regulatory risks.", "At Scrapinghub we have extensive experience developing data extraction solutions that overcome these challenges and mitigate the compliance risks associated with using web scraped data in your business.", "If you have a need to start or scale your web scraping projects then our ", " are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."]},
{"tite": "The Web Data Extraction Summit 2019", "date": "September 26, 2019 ", "author": "Attila T\u00f3th", "blog_data": ["The Web Data Extraction Summit was held last week, on 17th September, in Dublin, Ireland. This was the first-ever event dedicated to web scraping and data extraction. We had over 140 curious attendees, 16 great speakers from technical deep dives to business use cases, 12 amazing presentations, a customer panel discussion and unlimited Guinness.", "Our goal with this event was to share not just our own expertise with the audience but also provide a platform for external industry experts and actual web data users to share their knowledge and experience with web data extraction. Based on the feedback we got during and after the event, it was an absolute success! The people who came learnt a lot and were able to connect with others in the industry.", "Here is a quick overview of the day and some highlights from the talks:", "To kick-off the day, Shane Evans, CEO at Scrapinghub talked about how web data extraction is impacting businesses and gave us his insights on how the industry will evolve.", "Right after, Cathal Garvey, Data Scientist, took the stage to provide some insights into what data types, use cases, applications and emerging trends. All these based on scraping 9 billion pages per month for thousands of Scrapinghub customers.", "The third talk was about legal compliance and GDPR in web scraping. Kate O\u2019Brien, Legal Counsel, talked about best practises guidelines to make sure you and your web scraper are compliant. The audience had quite a few questions about the ", " and others. It\u2019s obviously a hot topic nowadays.", "Attila Toth, Technology Evangelist and Pawel Miech, Technical Team Lead, talked about ", " and problems you need to solve if you extract data from the web at scale. Also providing actual tips for data extraction developers, like how to find data fields most effectively in an HTML or how to get through captchas. A takeaway from the presentation is that the web is like a jungle and you should not expect that each website will follow the standards.", "Then, Bryan O\u2019Brien, Product Manager, showed us how the new product from Scrapinghub ", ", an AI-enabled automatic data extraction API for article and e-commerce data extraction is changing the way we extract data from the web today. AutoExtract uses artificial intelligence and machine learning to make ", " and getting ", " from pages a breeze. No more need for data field selectors or XPath.", "In the next talk the two biggest topics for any successful web scraping projects were discussed - proxies and anti-ban best practises. Akshay Philar, Head of Development, and Tomas Rinke, Systems Engineer, gave us a better understanding of how anti-ban systems work and ", ". They also exposed their best actionable tips on how to scale web scraping projects with proxies.", "Going further than the usual use cases that are associated with web scraping we were able to show something new. Something that is not only interesting but actually making the world a better place. Amanda Towler (Hyperion Gray) & David Schroh (Uncharted Software) showed how web data extraction is used to fight human trafficking. They talked about how they are using web data extraction to locate potential victims of human trafficking and identify those who exploit them. In the process, they open-sourced ", ".", "Mikhail Korobov, Head of Data Science, gave an entertaining and super educational talk about how machine learning can be used in web scraping. It is also how our AI web scraping tool, ", ", works under the hood. Reassuring, that it works on any website and the algorithm doesn\u2019t need a \u201csample\u201d of the website structure before data extraction.", "Andrew Fogg, Founder of import.io, talked about how web data is used to identify trends and inform business decisions. And took us through some unusual business questions that can be answered with web data.", "Then, we had some of our customers - Just-Eat, OLX, Revuze and Eagle Alpha - in a panel discussion on how they are leveraging web data and what are some of their challenges when it comes to web data extraction. One takeaway from this panel was that getting the data is crucial but it\u2019s also important to be able to get actionable insights from the data.", "Or Lenchner, CEO at Luminati, talked about the importance of data quality. He showed us how to tweak your web scraper to avoid common traps websites give you and what to look for on a web page to recognize bad quality data.", "In the last talk, we learnt about data democratization from Juan Riaza, Software Developer at Idealista. He gave us technical insights into the usage of Apache Airflow, Scrapy Cloud and Apache Spark together to build out a company-wide data infrastructure.", "It was a busy one-day event. Packed with interesting insights from industry and technology experts, customers and web data users. Our speakers gave their expertise and suggestions on all of the different web data extraction challenges. Proxies, anti-ban, legal, scalability, machine learning. On a higher level, we also had several talks about use cases.", "The people who came to this event had the opportunity to learn from several web data extraction experts, hear amazing stories about what web data can do, connect with like-minded people and had a great time in the Guinness Storehouse. We expect to organize the event next year, ", " if you want to be kept informed or ", " at the event next year."]},
{"tite": "Do What is Right Not What is Easy!", "date": "December 13, 2018 ", "author": "Sanaea Daruwalla", "blog_data": ["I was recently invited to speak at the IAPP Europe Data Protection Congress in Brussels about web scraping and GDPR. The panel also included Claire Fran\u00e7ois of Hunton Andrews Kurth and Peter Brown from the Information Commissioner\u2019s Office (ICO). For more information you can check out my blog about this topic ", ".", "\u00a0", "\u00a0", "There are only two legal basises for scraping personal data (1) consent or (2) legitimate interest. While consent is rare in web scraping cases, it\u2019s the cleaner of the two options, so much of the panel discussion at the IAPP Congress was spent on legitimate interest. In reality, legitimate interest will typically be the only legal basis at your disposal when scraping personal data, so is there a compliant way to use legitimate interest as a legal basis when web scraping?? Maybe . . . sometimes . . . if you\u2019re really careful.", "\u00a0", "Where no other legal basis is available, many companies are turning to legitimate interest. Legitimate interest can be used where the use case for the personal data is a use that the data subject would reasonably expect and have a minimal privacy impact. When determining if this is the case, this three-factor test can be utilised:", "Identify the legitimate interest (for example, Recital 47 of the GDPR states that \u201c...the processing of personal data for direct marketing purposes may be regarded as carried out for a legitimate interest.\u201d);", "Show that processing is necessary to achieve that legitimate interest; and", "Balance the legitimate interest against the individual\u2019s rights.", "\u00a0", "Some ideas considered during our panel discussion:", "Conducting a Data Processing Impact Assessment (DPIA)", "Review the use case for the data to determine if it aligns with the data subject\u2019s original purpose for sharing the data", "Territorial scope -- consider where the scraping is taking place and the location of the company that is conducting the scraping. Remember, GDPR only applies if:", "If the privacy policy of the website scraped lists categories of third parties that may access the personal data and you fall within those categories", "Obtaining consent after scraping the data.", "\u00a0", "It was great to hear the ICO\u2019s recommendation, given that they are the ones enforcing GDPR. The ICO was clear that they don\u2019t have any specific recommendations on web scraping, but you can look to their recommendations on \u201cInvisible Processing\u201d to get some guidance. Invisible Processing is the \u201cprocessing of personal data that has not been obtained direct from the data subject in circumstances where the controller considers that compliance with Article 14 would prove impossible or involve disproportionate effort.\u201d The ICO considers Invisible Processing \u201chigh risk\u201d and thus requires a DPIA to be conducted prior to such processing.", "A DPIA is an assessment that helps you analyse, identify, and minimise the data protection risks of a project, to ensure compliance with GDPR. The ICO provides a step-by-step list for conducting a DPIA, which includes:", "There are also various data protection software packages on the market, which walk you through a step-by-step DPIA process. At Scrapinghub, if we were to utilise the DPIA approach, it would be our preference to conduct it within the data protection software we use, so that we\u2019re conducting the most robust and thorough analysis possible.", "\u00a0", "Attending and speaking at the IAPP Congress helped to get web scraping on the minds of some of the leading data protection experts in the world, and we\u2019re hopeful that this will turn into direct guidance from organisations like the ICO about web scraping. In the meantime, Scrapinghub will continue to advocate for fair scraping of public data, and will continue to guide our customers to help them lawfully scrape personal data.", ": I am a lawyer, but I am not your lawyer and the recommendations in this post do not constitute legal advice. The commentary and recommendations outlined are based on Scrapinghub\u2019s experience helping our clients (startups to Fortune 100\u2019s) maintain GDPR compliance whilst scraping 7 billion web pages per month. If you want legal advice regarding your specific situation then you should consult a lawyer."]},
{"tite": "Visual Web Scraping Tools: What to Do When They Are No Longer Fit For Purpose?", "date": "May 30, 2019 ", "author": "Ian Kerins", "blog_data": ["Visual web scraping tools are great. They allow people with little to no technical know-how to extract data from websites with only a couple hours of upskilling, making them great for simple lead generation, market intelligence and competitor monitoring projects. Removing countless hours of manual entry work for sales and marketing teams, researchers, and business intelligence team in the process.", "However, no matter how sophisticated the creators of these tools say their visual web scraping tools are, users often run into issues when trying to scrape mission critical data from complex websites or when scraping the web at scale.", "In this article, we\u2019re going to talk about the biggest issues companies face when using visual web scraping tools like Mozenda, Import.io and Dexi.io, and what they should do when they are no longer fit for purpose.", "First, let\u2019s use a commonly known comparison to help explain the pros and cons of visual web scraping tools versus manually coding your own web crawlers.", "If you have any experience of developing a website for your own business, hobby or client projects, odds are you have come across one of the many online tools that say you can create visually stunning and fully featured websites using a simple-to-use visual interface.", "When we see their promotional videos and the example websites their users have \u201ccreated\u201d on their platforms we believe we have hit the jackpot. With a few clicks of a button, we can design a beautiful website ourselves at a fraction of the cost of hiring a web developer to do it for us. Unfortunately, in most cases these tools never meet our expectations.", "No matter how much they try, visual point and click website builders can never replicate the functionality, design and performance of a custom website created by a web developer. Websites created by visual website builder tools are often slow, inefficient, have poor SEO and severely limit the translation of design requirements into the desired website. As a result, outside of very small business websites and rapid prototyping of marketing landing pages, companies overwhelming have professional web developers design and develop custom websites for their businesses.   ", "The same is true of visual point and click web scraping tools. Although the promotional material of many of these tools make it look like you can extract any data from any website at any scale, in reality this is often never true.  ", "Like visual website builder tools, visual web scraping tools are great for small and simple data extraction projects where lapses in data quality or delivery aren\u2019t critical, however, when scraping mission critical data from complex websites at scale then they quickly suffer some serious issues often making them a bottleneck in companies data extraction pipelines and a burden on their teams. ", "With that in mind we will look at some of these performance issues in a bit more detail... ", "Visual point and click web scraping tools suffer from similar issues that visual website builders encounter. Because the crawler design needs to be able to handle a huge variety of website types/formats and isn\u2019t being custom developed by an experienced developer, the underlying code can sometimes be clunky and inefficient. Impacting the speed at which visual crawlers can extract the target data and make them more prone to breaking.", "Oftentimes, these crawlers make additional requests that aren\u2019t required, render JavaScript when there is no need, and increase the footprint of the crawler increasing the likelihood of your crawlers being detected by anti-bot countermeasures.", "These issues often have little noticeable impact on small scale and infrequent web scraping projects, however, as the volume of data being extracted increases, users of visual web scrapers often notice significant performance issues in comparison to custom developed crawlers.", "Unnecessarily, putting more strain on the target websites servers, increasing the load on your web scraping infrastructure and make extracting data within tight time windows unviable.", "Visual web scraping tools also suffer from increased data quality and reliability issues due to the technical limitations described above along with their inherent rigidity, lack of quality assurance layers and the fact their opaque nature makes it harder to identify and fix the root causes of data quality issues. ", "These issues combine to reduce the overall data quality and reliability of data extracted with visual web scraping tools and increase the maintenance burden.  ", "Another drawback of visual web scraping tools is the fact that they often struggle to handle modern websites that make extensive use of JavaScript and AJAX. These limitations can make it difficult to extract all the data you need and simulate user behaviour adequately.", "It can often also be complex to next to impossible to extract data from certain types of fields on websites, for example: hidden elements, XHR requests and other non-HTML elements (for example PDF or XLS files embedded on the page).", "For simple web scraping projects these drawbacks might not be an issue, but for certain use cases and sites they can make extracting the data you need virtually impossible.", "Oftentimes, the technical issues described above aren\u2019t that evident for smaller scale web scraping projects, however, they can quickly become debilitating as you scale up your crawls. Not only do they make your web scraping processes more inefficient and buggy, they can stop you from extracting your target data entirely.", "Increasingly, large websites are using anti-bot countermeasures to control the way automated bots access their websites. However, due to the inefficiency of their code, web crawlers designed by visual web scraping tools are often easier to detect than properly optimised custom spiders. ", "Custom spiders can be designed to better simulate user behaviour, minimise their digital footprint and counteract the detection methods of anti-bot countermeasures to avoid any disruption to their data feeds.", "In contrast, the same degree of customisation is often impossible to replicate with crawlers built using visual web scraping tools without getting access to and modifying the underlying source code of the crawlers. Which can be difficult to do as it is often proprietary to the visual website builder.  ", "As a result, often the only step you can take is to increase the size of your proxy pool to cope with the increasing frequency of bans, etc. as you scale.", "If you are using a visual web scraping tool with zero issues and have no plans to scale your web scraping projects then you might as well just keep using your current web scraping tool. You likely won\u2019t get any performance boost from switching to custom designed tools.", "Although current visual web scraping tools have come along way, currently they often can\u2019t replicate the accuracy and performance of custom designed crawlers, especially when scraping at scale. ", "In the coming years, with the continued advancements in artificial intelligence these crawlers may be able to match their performance. However for the time being, if your web scraping projects are suffering from poor data quality, crawlers breaking, difficulties scaling, or want to cut your reliance on your current providers support team then you should seriously consider building a custom web scraping infrastructure for your data extraction requirements. ", "In cases like these, it is very common for companies to contact Scrapinghub to migrate their web scraping projects from a visual web scraping tool to a custom web scraping infrastructure.  ", "Not only are they able to significantly increase the scale and performance of your web scraping projects, they no longer have to rely on proprietary technologies, have no vendor lock-in, and have more flexibility to get the exact data they need with no data quality or reliability issues. ", "Removing all of the bottlenecks and headaches companies normally face when using visual web scraping tools.", "If you think it is time for you to take this approach with your web scraping, then you have two options:", "At Scrapinghub, we can help you with both options. We have a ", " to help development teams build, scale and manage their spiders without all the headaches of managing the underlying infrastructure. Along with a range of ", " where we develop and manage your custom high performance web scraping infrastructure for you. ", " If you have a need to start or scale your web scraping projects then our ", " is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!"]},
{"tite": "The Challenges E-Commerce Retailers Face Managing Their Web Scraping Proxies", "date": "January 08, 2019 ", "author": "Ian Kerins", "blog_data": ["These days web scraping amongst big e-commerce companies is ubiquitous due to the advantages that data-based decision making can bring to remain competitive in such a tight-margin business.", "E-commerce companies are increasingly using web data to fuel their competitor research, dynamic pricing and new product research.", "For these e-commerce sites, their most important considerations are: the ", " and it\u2019s ability to return the data they need at the ", ".", "As a result, these e-commerce sites face big challenges managing their proxies so that they can reliably scrape the web without disruption.", "In this article, we\u2019re going to talk about those challenges and how the best web scrapers get around them.", "The sheer number of the requests being made (upwards of 20 million ", "requests per day) is a huge challenge for companies. With millions of requests per day, companies also need thousands of IPs in their proxy pools to cope with the request volume.", "Not only do they need a large pool size, but a pool that contains a wide range of proxy types (location, datacenter/residential, etc.) to enable them to reliably scrape the precise data they need.", "However, managing proxy pools of this scale can be very time-consuming. Developers and data scientists often report spending more time managing proxies and troubleshooting data quality issues than analyzing the extracted data.", "To cope with this level of complexity, to scrape the web at this scale you will need to implement a robust intelligence layer to your proxy management logic.", "When scraping the web at a relatively small scale (couple thousand pages per day), you can get away with a simple proxy management infrastructure if your spiders are well designed and you have a large enough pool.", "However, when you are scraping the web at scale, this simply just won\u2019t cut it. Very quickly you\u2019ll run into the following challenges when building a large scale web scraper.", "As a result, companies need to implement a robust proxy management logic to rotate IPs, select geographical specific IPs, throttle requests, identify bans and captchas, automate retries, manage sessions, user agents and blacklisting logic to prevent your proxies from getting blocked and disrupting their data feed.", "The problem is most solutions on the market are only selling proxies or proxies with simple rotation logic at best. So often times companies need to build and refine this intelligent proxy management layer themselves. Which requires significant development.\u00a0", "The other option is to use a proxy solution that takes care of all the proxy management for you. More on this later.", "As is often the case with e-commerce product data, the prices and specifications of products vary depending on the location of the user.", "As a result, to get the most accurate picture of a products pricing or feature data, companies often want to request each product from different locations/zip codes. This adds another layer of complexity to an e-commerce web scraping proxy pool, as you now need a proxy pool that contains proxies from different locations and has implemented the necessary logic to select the correct proxies for the target locations.", "At lower volumes, it is often ok to just manually configure a proxy pool to only use certain proxies for specific web scraping projects. However, this can become very complex as the number and complexity of the web scraping projects increases. That is why an automated approach to proxy selection is key when scraping at scale.", "As stated at the start of this article, the most important consideration in the development of any proxy management solution for large-scale e-commerce web scraping is that it is robust/reliable and returns high-quality data for analysis.", "Oftentimes, the data these e-commerce companies are extracting is mission critical to the success of the businesses and their ability to remain competitive in the marketplace. As a result, any disruptions or reliability issues with their data feed is a huge area of concern for most companies conducting large-scale web scraping.", "Even a disruption of a couple hours will likely prevent them from having up to date product data for the setting product pricing for the next day.", "The other issue is cloaking, the practice of e-commerce websites feeding incorrect product data to requests if they believe them to be from web scrapers. This can cause huge headaches for the data scientists working in these companies as there will always be a question mark over the validity of their data.", "Growing a seed of doubt in their minds as to whether they can make decisions based on what the data is telling them.", "This is where having a robust and reliable proxy management infrastructure along with an ", " in place really helps. Not only does it remove a lot of the headaches of having to manually configure and troubleshoot proxy issues, it also gives companies a high degree of confidence in the reliability of their data feed.", "Ok, we\u2019ve discussed the challenges of managing proxies for enterprise web scraping projects, however, how do you overcome these challenges and build your own proxy management system for your large scale web scraping projects?", "In reality, enterprise web scrapers have two options when it comes to building their proxy infrastructure for their web scraping projects.", "One solution is to build a robust proxy management solution in-house that will take care of all the necessary IP rotation, request throttling, session management and blacklisting logic to prevent your spiders being blocked.", "There is nothing wrong with this approach, provided that you have the available resources and expertise to build and maintain such an infrastructure. To say that a proxy management infrastructure designed to handle 300 million requests per month (the scale a lot of e-commerce sites scrape at) is complex is an understatement. This kind of infrastructure is a significant development project.", "For most companies their #1 priority is the data, not proxy management. As a result, a lot of the largest e-commerce companies completely outsource proxy management using a single endpoint proxy solution.", "Our recommendation is to go with a proxy provider who can provide a single endpoint for proxy configuration and hide all the complexities of managing your proxies. Scraping at scale is resource intensive enough without trying to reinvent the wheel by developing and maintaining your own internal proxy management infrastructure.", "This is the approach most of the large e-commerce retailers take. Three of the worlds top five largest e-commerce companies use ", " as their primary proxy solution, the smart downloader developed by Scrapinghub, that completely outsources their proxy management. In total, Crawlera processes 8 billion requests per month.", "The beauty of Crawlera is that instead of having to manage a pool of IPs, your spiders just send a request to Crawlera's single endpoint API where Crawlera retrieves and returns the desired data.", "Under the hood, Crawlera manages a massive pool of proxies, carefully rotating, throttling, blacklists and selecting the optimal IPs to use for any individual request to give the optimal results at the lowest cost. Completely, removing the hassle of managing IPs and enabling users to focus on the data, not proxies.", "The huge advantage of this approach is that it is extremely scalable. Crawlera can scale from a few hundred requests per day to millions of requests per day without any additional workload from the user. Simply increase the number of requests you are making and Crawlera will take care of the rest.", "Crawlera also comes with global support. Clients know that they can get expert input into any issue that may arise 24 hours per day, 7 days a week no matter where they are in the world.", "If you'd like to learn more about Crawlera then, be sure to ", ".", "As you have seen there are a lot of challenges associated with managing proxies for large-scale web scraping projects. However, it is a surmountable challenge if you have adequate resources and expertise to implement a robust proxy management infrastructure. If not then you should seriously consider a single endpoint proxy solution such as Crawlera.", "For those of you who are interested in scraping the web at scale but are wrestling with the decision of whether or not you should build up a dedicated web scraping team in-house or outsource it to a dedicated web scraping firm then be sure to check out our guide, ", "."]},
{"tite": "The Predictive Power of Web Scraped Product Data For Institutional Investors: A GoPro Case Study", "date": "January 24, 2019 ", "author": "John Larson", "blog_data": ["Investors understand the importance of high-quality information. It minimizes risk, empowers decision-making, and enables investors of all sizes to obtain alpha - like the old adage, knowing is often half the battle.", "Knowing this, alternative data providers wield vast, untraditional datasets derived from hundreds of millions of sources, not only enabling asset managers to consistently obtain alpha, but granting them a competitive foresight that makes access to such data so highly desirable.", "Truer still, traditional data channels simply cannot produce the same actionable insights as alternative data. The rise of big data and advanced machine learning technologies in the past decade speaks to the critical nature of information and to the strange reality that now faces us: we can parse it with extreme efficiency, but we need more of it, and from more sources.", "In October 2015, ", " a provider of alternative financial data, published a report on action camera manufacturer GoPro (NASDAQ:GPRO). Using data aggregated from US electronics websites and more than 80 million sources, the company\u2019s publication highlighted weaknesses in GoPro\u2019s revenue for the third quarter of that year, signalled by weak demand for GoPro products and a negative mix shift towards lower-end products. While 68% of stock recommendations insisted investors buy, Eagle Alpha predicted otherwise, correctly observing that the company would miss its Q3 targets.", "They were right. How?", "It wasn\u2019t intuition, but better data, and a lot more of it. When it comes to alternative data, data quality is everything. The existence of \u201cReturn on Data\u201d as a term used by asset managers and Chief Data Officers is itself telling: investment research is paradigmatically transforming towards a better utilization of data and with tremendous potential. If competitors rely solely upon traditional data, those using alternative data are capable of seeing around corners, assessing risk and determining optimal stock positions based on information unknown to others.", "GoPro is a particularly interesting example, given its former status as a stock market darling - a company whose stocks soared to an all-time high of $98.47 in October 14. From its IPO in June 2014, the stocks rose sharply promoting investor confidence and expectations of strong revenue growth for a company innovating cameras, drones, and aspiring to be a lifestyle media company.", "\u00a0", "\u00a0", "Disappointing Q3 2015 results pointed to the weakness of traditional analysts\u2019 methods of anticipating a company\u2019s performance. GoPro reported Q3 earnings of $400 million, falling $30 million short of the $430 million expected. The company projected a Q4 between $500-550 million, which is 17% lower YoY. Part of the reason alternative data, in particular web scraped pricing data, was ahead of the curve is the fact GoPro\u2019s pricing adjustment of the Hero 4 from $399 to $299 likely negatively impacted sales. Why buy a product or buy a company\u2019s new release if you think the company will adjust the price by an astounding 33%?", "In the Q3 earnings call, CEO Nicholas Woodman remarked \u201cWhile we experienced strong year-over-year growth, this quarter marks the first time as a publicly traded company that we delivered results below the expectations that we outlined in our guidance\u201d(", "). One of the factors Woodman pointed to for the weak Q3 performance was the price adjustment of the Hero 4 to $299 that was in response to the camera\u2019s underperformance. This adjustment resulted in \u201c$19 million of price protection.\u201d", "The earnings call closely mirrors Eagle Alpha\u2019s own proactive outlook on GoPro\u2019s pricing mistakes and weakening demand for these higher priced models in the face of lower priced competition. The key here is alternative pricing data gives investors the leg up against those using earnings reports as their investing GPS, so to speak.", "As online retail burgeons to record highs, what better place to look for consumer sentiment than data from online retail giants like Amazon and Best Buy? Amazon was responsible for 44% of all U.S e-commerce sales in 2017 (CNBC). Taking advantage of key metrics like average selling price, quantities sold in a given month and frequency of price drops gives an investor much more insight into the competitive landscape of an industry like adventure cameras.", "While the competitive landscape and average selling price for adventure cameras transformed, so did GoPro\u2019s fate. Extracting average selling price (ASP) and consumer interest from web crawling on electronics websites is key to how alternative data providers stay one step ahead of those relying on traditional data. For example, web scraped product data below shows hows ASP fell as market saturation rose - a combination that can dethrone companies that were once at the helm of their respective industries.", "Average selling price spiked in Q4 2014 and early 2015, and companies like GoPro with high-end adventure cameras hit record high stock prices and investor sentiment continued to strengthen. As we can see, from this period of November 2014 - Mar 15, ASP continued to rise while product count stayed relatively steady. Identifying this pattern of market saturation is a key way in which alternative data investors can see risk before traditional data flags a company.", "Source Eagle Alpha.", "To asset managers, missing a trend is likely missing the boat on an opportunity for alpha. From Q4 2013 to Q4 2015, the wearable action camera market experienced a price segmentation convergence, where the low, mid and high-end price segments eventually collapsed on themselves.", "Using sophisticated web scraping technology to aggregate and index millions of points of information from across the web, Eagle Alpha was able to capture valuable insight into consumer preferences and, unfortunately for GoPro, a decreasing interest and average selling price (ASP) for wearable action cameras.", "GoPro tried to focus on entry-level products as the compression of mid-range and higher-end products intensified, but this strategy ignored other critical issues within the market, namely that overall demand was diminishing and average selling price trends were highly negative. The company has struggled to make a comeback.", "\u00a0", "Scraping price data from almost a hundred million sources enables alternative data providers to provide strong directional indicators for a number of measurements related to a company\u2019s overall health and success. Web scraped product data is an especially powerful tool for predicting company fundamentals and is thus extremely useful in the investment decision-making process. It\u2019s also extremely useful to business owners, executives and key decision makers, as it informs strategic decisions which enable more intelligent and shorter sales cycles, competitive advantages, improved revenue per customer and heightened internal operational efficiency to name a few.", "Incomplete or low-quality data, on the other hand, pose a legitimate risk to investors as it skews or modifies interpretations which could otherwise lead to profitable outcomes. In traditional investment research, missing data or misrepresentative data is often a dead-end without highly specialized tools and access. Alternative data providers, like Eagle Alpha and Scrapinghub, put a premium on data quality and employ sophisticated crawling algorithms to ensure the capture and verification of all available past and present information - it\u2019s one thing to ensure data quality with a moderate amount of information, and another thing entirely to mandate quality when working with hundreds of millions of points of information. Leveraging world-class technology with expert crawl engineers and data scientists, alternative data providers, are able to predict the success or failure of product launches and actually determine public sentiment toward a particular brand or iteration.", "\u00a0", "\u00a0", "Price data extracted from the web and major retailers was key in predictions of GoPro\u2019s weakening performance for Q3 2015. Observations of lower demand and average sales price from third-party retailers like Amazon, Best Buy, and other major retailers gives hedge funds and wall street traders the ability to better forecast how a company or product\u2019s pricing is shaping revenue in real time.", "By utilizing the broad spectrum of pricing information available on the web, investors making use of alternative financial data are able to see the Average Selling Price fluctuations before the company itself expresses concerns or releases negative earnings reports. Aggregated pricing data is more readily available than it was in the past, providing more current corporate revenue and insight into industry competition.", "Today, GoPro faces mounting competition and struggles to find a way to bounce back from its peak in 2014 and early 2015. Amazon and other retailers show how lower priced alternatives often edge out GoPro.", "If you\u2019d like to see the underlying data for Eagle Alpha\u2019s GoPro prediction, or any of the other 40 example alternative data use cases then be sure to check out their 124-page report: ", ".", "Looking forward to 2019, projected spending on alternative data and web scraped pricing information is higher than ever before. As hedge funds and investment firms purchase more alternative data, the key to staying ahead of the curve is going to be knowing how to use this data.", "If you\u2019d like to believe there is the alpha in web data and would like to incorporate it into your investment decision-making process, then be sure to reach out to ", " team for a free consultation. We provide web data to numerous hedge funds, investment banks and alternative data providers so our team can help you architect a solution for you to get the web data you need."]},
{"tite": "The Rise of Web Data in Hedge Fund Decision Making & The Importance of Data Quality", "date": "February 01, 2019 ", "author": "Ian Kerins", "blog_data": ["Over the past few years, there has been an explosion in the use of alternative data sources in investment decision making in hedge funds, investment banks and private equity firms.", "These new data sources, collectively known as \u201calternative data\u201d, have the potential to give firms a crucial informational edge in the market, enabling them to generate alpha.", "Although investors are now using countless alternative data sources, satellite, weather, employment, trade data, etc. by far the leading alternative data source are the various forms of web data - web scraped data, search trends and website traffic.", "Web data is unique in that there is a vast ocean of rich and up to date signalling data lying within arms reach on the web. However, it is locked away in not easily accessible unstructured data formats.", "In this article, we\u2019re going to discuss the most popular form of alternative data, web-scraped data, and share with you the most important factor that firms need to be taken into account when building a robust alternative financial data feed for their investment decision making processes: ", ".", "When it comes to using data in multi-million dollar (or billion dollar) investment decisions, the ability to validate your investment hypothesis via benchmarking and backtesting are crucial.", "What this means for web scraped data is that web data doesn\u2019t start to become truly valuable until you have a complete historical dataset.", "The key here is the word \u201ccomplete\u201d.", "As we will discuss in the Data Quality section below, data completeness and quality plays a huge roll in the value and usefulness of any data sources. Without a complete historical data set, it is nearly impossible for firms to validate their investment thesis prior to committing to a multi-million (or billion) dollar investment decision.", "Their investment thesis must be rigorously stress tested to evaluate the soundness of their underlying assumptions, the predicted risk and return from the investment, and then benchmarked versus other competing investment theses competing for the same pool of investment money.", "The most effective way of evaluating how an investment thesis would have faired in past situations is by stress testing it with historical data. Making the need for complete historical data extremely important.", "There are two approaches taken to obtain the historical data firms need:", "One approach is to purchase off-the-shelf data sets from alternative data vendors. The completeness and value of these datasets can be easily validated with some internal analysis, however, they suffer greatly from commoditization.", "As these datasets are openly for sale, everyone has can get access to the same data sources. Significantly reducing the informational edge one firm can get over another from the resulting data. The ability to generate alpha with the data will be largely dependant on the competencies of the internal data analysis and investment teams, and the other proprietary data they can combine these off-the-shelf datasets with.\u00a0", "The other and increasingly popular option is for firms to create their own alternative finance web data feeds and build their own historical datasets. This approach has it\u2019s pro\u2019s and con\u2019s as well.", "The huge advantage to firms creating their own web data feeds is it gives them access to unique data that their competitors won\u2019t have. Having their own internal data extraction capabilities expands exponentially the number and completeness of the investment theses their team can develop. Enabling them to develop investment theses that give them a unique edge over the market. However, the primary downside to building internal data feed is the fact that they are typically an investment for the future. Firms likely won\u2019t use the extracted data straight away (depending on the data type they might) as they need to build a backlog of historical data.", "As we\u2019ve seen there is a huge need for web data in investment decision making, however, as we\u2019ve noted it is all highly dependant on the quality of the underlying data.", "By far the most important element of a successful web scraping project in alternative data in finance is data quality.", "Without high quality and complete data, web data is oftentimes useless for investment decision-making. It is simply too unreliable and risky to base investment decisions on incomplete or low-quality data.", "This poses a huge challenge to any hedge fund data acquisition team, as the accuracy and coverage requirements they face often far exceed the requirements of your typical web scraping project.", "The reason for this heightened need for data quality is the fact that any breaks or corruptions in the data oftentimes corrupt the whole dataset. Making it unusable for investment decision making.", "If there is a break in the data feed, interpolating between the available data points might induce errors that would corrupt the output of any analysis of the data. Potentially leading to a misguided investment decision.", "As a result, unless you can be confident in the accuracy of the interpolation, any break in the data feed can severely disrupt the usability of the data.", "It is because of this need for high quality and reliable data that alternative finance web scraping teams need to double down on the core fundamentals of building a robust web scraping infrastructure: crawler/extractor design, proxy management and data quality assurance.\u00a0", "Crawler and extractor design plays a crucial role in the quality and reliability of an alternative data feed for finance. As the name suggests the crawler and extractor are parts of the web scraping system that locates and extracts the target data from the website.", "As a result, any inaccuracies here are extremely hard (sometimes impossible) to correct in post-processing. If the extracted raw data is incomplete, incorrect or corrupted then without other independent data sources to supplement, interpolate and validate the data, the underlying raw data can be rendered unusable. Making crawler and extractor design the #1 focus when building a web data extraction infrastructure for alternative finance data.", "It is outside the scope of this article to detail how to develop robust crawlers and extractors, however, we will discuss some high-level points to keep in mind when designing your crawlers and extractors.", "With the importance of the resulting data to investment decision making, nothing beats having experienced crawl engineers when designing and building crawlers and extractors.", "Each website has its own quirks and challenges, from sloppy structures to javascript, to anti-bot countermeasures and difficulty navigating to the target data. Having experienced engineers enables your team to predict the challenges your crawlers and extractors are going to face well in advance of the problems manifesting themselves. Enabling you to develop a robust data feed from day one and building historical datasets, instead of spending weeks (or months) troubleshooting and refining a data feed yielding unreliable data.", "How the web crawlers and extractors are configured is also very important. We\u2019ve touched on it in ", ", however, to build on those points. When building your web scraping infrastructure you need to separate your data discovery and data extraction spiders. Along with this, your crawlers need to be highly configurable and designed to enable crawls to be stopped and resumed easily without any data loss. It\u2019s inevitable with website changes and anti-bot challenges that your crawlers will stop yielding perfect data quality. As a result, your crawlers need to be highly configurable, able to detect/cope with foreseen edge cases and be structured in a way that enables them to be stopped and resumed mid-crawl.", "The most important factor in ensuring the ", "of your data feed is ensuring you can reliably access the data you need no matter the scale. As a result, a robust proxy management solution is an absolute must.", "Nothing influences the reliability of requesting the underlying web pages more than your proxy management system. If your requests are constantly getting blocked that introduces a very high risk that there will be gaps in your data feed.", "It is very common for web scraping teams to run into severe banning issues as they move spiders from staging to full production. At scale blocked requests can quickly become a troubleshooting nightmare and huge burden on your team.", "You need to use a robust and intelligent proxy management layer that is able to rotate IPs, select geographical specific IPs, throttle requests, identify bans and captchas, automate retries, manage sessions, user agents and blacklisting logic to prevent your proxies from getting blocked and disrupting their data feed.", "You\u2019ve two options here, you can either use high-quality proxies and develop this proxy management infrastructure in-house or use a tailor built proxy management solution like ", ".", "Managing proxies is not a core competency or high ROI task for hedge funds so our recommendation is to use robust and well maintained off-the-shelf proxy management solutions like Crawlera, and let you focus on using the data in your investment decision-making processes.", "For a deeper look at the challenges of managing proxies at scale and the solutions to those problems then be sure to check out our article: ", ".", "Lastly, your firm's web scraping infrastructure must include a highly capable and robust data quality assurance layer that can detect data quality issues in real-time so they can be fixed immediately to minimise the likelihood that there will be any breaks in the data feed.", "Obviously, a completely manual QA process simply won\u2019t cut here as it would never be able to guarantee the required quality levels at scale.", "You need to implement a hybrid automated/manual QA process that is able to monitor your crawlers in real-time, detect data accuracy and coverage issues, correct minor issues and flag major issues for manual inspection by your QA team.", "At Scrapinghub, we use a four-layer QA process to ensure our alternative finance clients have confidence in the quality of their data. It is because of this four-layer QA process that we can confidently give our clients written data quality and coverage guarantees in their service level agreements (SLAs).", "If you\u2019d like an insider look at the four-layers of our QA process and how you can build your own, then be sure to check out our ", ".", "As you have seen there are a lot of challenges associated with extracting high-quality alternative finance data from the web. However, with the right experience, tools and resources you can build a highly robust web scraping infrastructure to fuel your investment decision making process with high-quality web data and gain an informational edge over the market.", "For those of you who are interested in extracting web data for their investment decision making processes but are wrestling with the decision of whether or not you should build up a dedicated web scraping team in-house or outsource it to a dedicated web scraping firm then be sure to check out our guide, ", ".", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0"]},
{"tite": "Why We Created Crawlera? The World\u2019s Smartest Web Scraping Proxy Network", "date": "February 07, 2019 ", "author": "Ian Kerins", "blog_data": ["Let\u2019s face it, managing your proxy pool can be an absolute pain and the biggest bottleneck to the reliability of your web scraping!\u00a0", "Scrapinghub had the same issues for years until we hit our breaking point and decided to solve this problem forever.", "At the time, Scrapinghub was about 3 years in business, providing web scraping consultancy services to companies looking to outsource their data extraction.", "We had established ourselves as the leading provider of web scraping consultancy services, however, increasingly our crawl engineers were running into big proxy issues as the scale and complexity of the projects grew.", "We\u2019d start a project, but our development timelines we\u2019re constantly being delayed because after deploying our spiders we\u2019d experience constant proxy issues.", "First, we\u2019d configure a proxy pool and tell the client that everything was working. Then within a couple of days, everything would be broken. The proxy pool would no longer return requests at the target RPM (requests per minute).", "We\u2019d then acquire new proxies, increase the pool size, rotate the proxies and create a new pool to route the requests through. This would work for a while but before long we were back where we started. A proxy pool full of banned IPs and our crawlers unable to make successful requests.", "It was a never ending cycle of swapping and rotating proxies. We couldn\u2019t reliably predict how long it would take us to get a project into full production, leading to frustrated engineers and plenty of hard conversations with customers.", "We were getting sick of firefighting proxies issues, but then one project came along that forced us to say \u201cenough was enough\u201d and commit to fixing this problem permanently...", "The client wanted us to build a web scraping infrastructure to scrape product data from 20 e-commerce sites, about 1 million requests per day. Which in 2011 was a big deal!.", "Everything started off great. We developed the spiders, ran a number of pilot crawls and delivered the proof of concept data to the customer.", "However, as was all too common we ran into serious problems scaling the crawls.", "Although our spiders were well designed and configured to crawl at a polite speed, when we moved the project from proof of concept to production our proxies were being banned at an alarming rate.", "We started the normal process of switching out proxies to try and get the crawlers back up and running.", "However, eventually it got to the point that we couldn\u2019t scale the crawl anymore as we couldn\u2019t put out the proxy fires fast enough.", "Initially, we told the client that we\u2019d have the issue fixed in 1 or 2 days \u201cas it was just a matter of swapping out the banned IPs\u201d.", "However, the days kept ticking by and we still hadn\u2019t found a permanent solution.", "Finally, nearly a month later. We fixed it!", "The solution\u2026", "We found that without an intelligent proxy management layer, our requests were continuously being blocked and our proxies burned. Leaving us constantly scrambling to find new proxies and get our crawlers back up and running again.", "However, when managed intelligently we could reliably scrape the web with little fear of our IPs being banned and the accompanying development/crawl delays.", "This breakthrough was a game changer for us. With this new proxy management layer, we were able to exponentially scale our crawls and completely remove the headache of managing proxies.", "Once configured for a project this new proxy management layer would automatically select the best proxy to use for the target website and manage all the proxy rotation, throttling, blacklisting, etc. ensuring that we could reliably extract the data we need.", "All without any manual intervention from our crawl engineers!", "As we continued to scale, people were constantly asking us how were we managing our proxies as they were facing the same reliability issues we encountered as they scaled their web scraping. It was at this point Crawlera was born...", "In 2012, we decided to make this technology available to everyone in the form of ", ", a proxy management solution specifically designed for web scraping.", "Crawlera enabled web scrapers to reliably crawl at scale, managing thousands of proxies internally, so you they didn\u2019t have to.", "They never needed to worry about rotating or swapping proxies again.", "Users loved Crawlera! It removed the frustrations their engineers had with managing their web scraping proxies.", "With Crawlera, instead of having to manage a pool of IPs the user's spiders send the request directly to Crawlera's single endpoint API.", "Crawlera then selects the best IP and proxy configuration (user agents, request delay, etc.) for that particular website to retrieve the target data.", "If a request is blocked, Crawlera then automatically selects the next best IP and reconfigures the proxy configuration before making another request. This process continues until Crawlera is able to obtain a successful request or a predefined request limit has been reached.", "\u00a0", "\u00a0", "All this functionality happened under the hood. The user just made the request to Crawlera\u2019s API and Crawlera would take care of everything else. Enabling users to focus on the data, not proxies.", "\u00a0", "\u00a0", "Crawlera achieved this by managing a massive pool of proxies, carefully rotating, throttling, blacklists and selecting the optimal IPs to use for any individual request to give the optimal results at the lowest cost. Completely, removing the hassle of managing IPs.", "The huge advantage of this approach is that it is extremely reliable and scalable. Crawlera can scale from a few hundred requests per day to millions of requests per day without any additional workload on your part.", "Better yet, as Crawlera was built by web scrapers for web scrapers we know they only care about successful requests, not the number of proxies. As a result with Crawlera you only pay for successful requests that return your desired data, not IPs or the amount of bandwidth you use.", "This is a huge benefit for users for Crawlera as they can accurately predict the cost of their proxy solution as they scale.", "For Scrapinghub having Crawlera at our disposal was a game changer for our business. Now our crawl engineers could focus on what they really enjoyed, building crawlers and delivering accurate reliable data for our customers. Not constantly having to put out proxy fires just to get their data feeds up and running again. Leading to happier and more motivated teams, and happy customers.", "Since its original launch, Crawlera has undergone numerous redesigns and improvements to keep pace with the changes in web scraping technologies and cope with the ever more complex challenges experienced when scraping the web.", "We\u2019ve added highly targeted geographical support (city granularity), residential IPs, headless browser support, custom user agents, to name a few of the features. Making Crawlera the most feature rich and robust proxy solution for web scraping.", "Today, Scrapinghub offers Crawlera in two flavours:", "Crawlera also comes with global support. Clients know that they can get expert input into any proxy issue that may arise 24 hours per day, 7 days a week no matter where they are in the world. Giving them immense peace of mind, knowing that they will never be left alone if they can\u2019t get access to the data they need.", "At Scrapinghub all our products are 100% designed with web scraping in mind. We are committed to helping the web scraping community extract they need to grow their businesses.", "If you tired of troubleshooting proxy issues and would like to give Crawlera a try ", ", or schedule a ", ".", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0"]},
{"tite": "A Sneak Peek Inside Crawlera: The World\u2019s Smartest Web Scraping Proxy Network", "date": "February 15, 2019 ", "author": "Ian Kerins", "blog_data": ["\u201cHow does Scrapinghub Crawlera work?\u201d is the most common question we get asked from customers who after struggling for months (or years) with constant proxy issues, only to have them disappear completely when they switch to Crawlera.\u00a0", "Today we\u2019re going to give you a behind the scenes look at Crawlera so you can see for yourself why it is the world\u2019s smartest web scraping proxy network and the only solution that allows you to completely forget about proxies, and focus on the data instead.", "Simply make a HTTP request to Crawlera\u2019s single endpoint API, and Crawlera will return a successful response. No need to worry about managing proxies.", "Crawlera achieves this by managing thousands of proxies so that you don\u2019t have to: carefully rotating, throttling, blacklisting and selecting the optimal IPs to use for any individual request to give the optimal results with the maximum efficiency.", "Completely removing the hassle of managing IPs and allowing you to focus on the data, not proxies.", "From day one, ", ". It has been optimised to ensure users can reliably extract the data they need at scale, without having to worry about bans or failed requests.", "Crawlera completely removes the hassle of managing proxy pools and all the headaches that go with it - rotating proxies, defining ban rules for each website, managing user agents, throttling, etc.", "Without Crawlera you would make a request to your own proxy pool and hope that it is successful.", "\u00a0", "\u00a0", "Instead, make a request to the Crawlera API and it takes care of the rest.", "\u00a0", "\u00a0", "Once, the HTTP request is sent to the API, Crawlera\u2019s intelligent IP management layer kicks in. This layer automatically determines the target domain and selects the optimal proxy pool and request configuration to yield the best results. Configuring the delay between requests, user agents, cookies, etc. to ensure successful requests.", "However, when the request is blocked is where Crawlera\u2019s capabilities really shine through.", "Instead of just returning a failed request to the client, Crawlera will change the proxies, request delays, user agents, etc. as required to return a successful request to the client.", "\u00a0", "Instead of having to manually design, build and configure your own proxy management infrastructure, with Crawlera all the individual elements that make a great proxy management solution tick are available out of the box and completely automated for your requests.", "Crawlera automatically finds the sweet spot between crawl speed and reliability, the two most important factors for any web scraping professional.", "As a web scraper, in most cases we want to extract our target data as fast as possible. However, with high speed comes an increasing risk that the requests will be blocked. As a result, finding the crawl speed sweet spot is our #1 priority.", "If you designed your own proxy management solution, you embark on a tedious process of trial and error to find this elusive crawl speed sweet spot. Which is very time consuming and often doesn\u2019t yield the optimal results.", "In contrast, Crawlera is optimised for successful requests and automatically finds this sweet spot for you. Crawlera constantly throttles the request delays to ensure the maximum successful request throughput for your crawl. With the precise delay being determined by the target website, the estimated load on the website, and it\u2019s ban history. Crawlera selects the optimal throttling configuration to give you the maximum request reliability.", "Key to Crawlera\u2019s functionality is its ban detection functionality, comprising of a constantly updated database of over 130+ ban types, response codes and captchas, developed as a result of Crawlera serving over 8 billion requests per month.", "Crawlera monitors every request for bans, even those that are not immediately obvious from the response code. When detected, Crawlera automatically retries the request with a different IP (while also modifying the delay) and puts the banned IP in what we call \u201cJail Time\u201d: A state where that same proxy won\u2019t be used to make a request to that same website for a variable period of time.", "Even so, you are able to add your own custom ban rules to cope with specific project requirements and challenges.", "Depending on your project requirements your spiders may need to store and manage cookies so that they can navigate to the target data and parse the data.", "With Crawlera, these cookie and session requirements will be handled for you, eliminating the need to handle them on the crawler side.", "However, you can easily disable this functionality to allow you to manually configure your crawler to manage cookies and sessions if your project requirements necessitate custom cookie and session handling.", "Crawlera also uses browser like headers to give your requests more human-like behaviour.", "When a request is received, Crawlera will select the most appropriate header from a browser header pool removing the need for you to worry about header configuration and rotation.", "In certain circumstances, you might like to manually control the headers being used with your requests. As a result, Crawlera\u2019s header functionality can be disabled by simply passing it a single command with your API request.", "With the increasing complexity of modern websites (most require 50 to 150 single requests to load) scraping efficiently at scale can become a real challenge.", "To help combat this problem, Crawlera comes with headless browser support. Enabling you to easily integrate headless browsers such as Splash and Puppeteer with your crawlers, significantly reducing the time it takes to get successful requests.", "If you\u2019d like to learn more about Crawlera\u2019s full feature set then be sure to check out ", ".", "Despite having all this advanced functionality available out of the box, where Crawlera really shines though is the reliability and predictability it gives you when scraping at scale.", "If you start a web scraping project, often times you can get away with a simple proxy pool at the start but as you scale you will have to develop all the functionality listed above or else suffer from constant reliability issues.", "In contrast, from day one Crawlera has been built with scraping at scale in mind.", "Crawlera\u2019s #1 priority is to give you maximum request reliability. As a result, Crawlera has been painstakingly designed to ensure it will consistently return successful requests to your crawlers no matter the website being scraped.", "And if your request volume increases, you don\u2019t need to worry about getting more proxies or reconfiguring your proxy management logic. Instead, you simply increase your allocated number of concurrent requests and Crawlera will handle the rest. Giving users great predictability of their web scraping costs as they scale.", "Ensuring that developers and companies don\u2019t need to invest huge amounts of time, engineering resources and money into developing their own scalable proxy management solutions. Allowing you to focus on the data instead.", "If you\u2019re tired of troubleshooting proxy issues and would like to give Crawlera a try, ", ", or schedule a call with our ", ". Scrapinghub offers Crawlera in two flavours:", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0", "\u00a0"]},
{"tite": "Meet Spidermon: Scrapinghub\u2019s Battle Tested Spider Monitoring Library [Now Open Sourced]", "date": "March 01, 2019 ", "author": "Renne Rocha", "blog_data": ["Your spider is developed and we are getting our structured data daily, so our job is done, right?", "Absolutely not! Website changes (sometimes very subtly), anti-bot countermeasures and temporary problems often reduce the quality and reliability of our data.", "Most of these problems are not under our control, so we need to actively monitor the execution of our spiders. Although manually monitoring a dozen spiders is doable, it becomes a huge burden if you have to monitor hundreds of spiders collecting millions of items daily.", "Spidermon is Scrapinghub\u2019s battle-tested extension for monitoring Scrapy spiders that we\u2019ve now made available as a ", " Spidermon makes it easy to validate data, monitor spider statistics and send notifications to everyone when things don't go well in an easy and extensible way.", "Installing Spidermon is just as straightforward as any other Python library:", "Once installed, to use Spidermon in your project, you first need to enable it in the settings.py file:", "To start monitoring your spiders with Spidermon the key concepts you need to understand are the ", "and the ", ".", "A ", "is similar to a Test Case. In fact, it inherits from ", ". TestCase, so you can use all existing unittest assertions inside your monitors. Each ", "contains a set of test methods that will ensure the correct execution of your spider.", "A ", "groups a set of ", "classes to be executed at specific times of your spiders execution. It also defines the actions (e.g., e-mail notifications, reports generation, etc) that will be performed after all monitors are executed.", "A", "can be\u00a0executed when your spider starts, when it finishes or periodically while spider is running. For each ", "you also can specify a list of actions that may be performed if all monitors pass without errors, if some monitor fail or always.", "For example, if you want to monitor whether your spider extracted at least 10 items, then you would define a monitor as follows:", "Monitors need to be included in a ", "to be executed:", "Include the previous defined monitor suite in project settings, and every time the spider closes, it will execute the monitor.", "After executing the spider, spidermon will present the following information in your logs:", "If the condition specified in your monitor fails, then spidermon will output this information in the logs:", ", so you can test it with your own spider.", "A useful feature of Spidermon is its ability to verify the content of your extracted items and confirm that they match against a defined data schema. Spidermon allows you to do this using two different libraries (you can choose which one fits better in your project): ", " and ", ".", "With the JSON Schema you can define required fields, field types, expressions to validate the values included in the item and much more.", "Schematics is a validation library based on ORM-like models. You can define Python classes using its built-in data types and validators, but they can be easily extended.", "To enable item validation, simply enable the built-in item pipeline in your project:", "\u00a0A JSON Schema looks like this:", "This schema is equivalent to the schematics model shown in the\u00a0", ". An item will be validated as correct if the required fields 'quote', 'author' and 'author_url' are filled with valid string content.", "To activate a data schema, simply define the schema in a json file and include it in your project settings. From there Spidermon will be able to use it during your spider execution and validate it:", "After that, any item returned in your spider will be validated against this schema.", "However,\u00a0 it is important to note that item validation failures will not appear automatically in monitors results. These results will be added to the spider stats, so you will need to create your own monitor to verify the results according to your own rules.", "For example, this monitor will only pass if no items have validation errors:", "When something goes wrong with our spiders, we want to be notified (e.g., by e-mail, on Slack, etc) so we can take corrective actions to solve the problem. To accomplish this, Spidermon has the concept of ", ", that are executed according to the results of your spider execution.", "Spidermon contains a set of built-in actions that makes it easy to be notified in different channels like e-mail (through Amazon SES), Slack, reports and Sentry. However, you can also specify your own custom actions so you can design your own notifications to suit your specific project requirements.", "Creating a custom action is straightforward. First you declare a class inheriting from spidermon.core.actions. then implement your business logic inside _run_action_ method:", "\u00a0To enable an action, you need to include it inside a ", ":", "Spidermon has some built-in actions for common cases which will require a few settings to be added to your project. You can see which ones are ", ".", "Spidermon\u2019s complete documentation can be found ", ". See also the \u201c", "\u201d section where we present an entire sample project using Spidermon.", "If you would like to take a deeper look at how Spidermon fits into Scrapinghub\u2019s data quality assurance process, the exact data validation tests we conduct and how you can build your own quality system, then be sure to check our whitepaper: ", ".", "\u00a0", "At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!"]},
{"tite": "Proxy Management: Should I Build My Proxy Infrastructure In-House Or Use AN Off-The-Shelf Proxy Solution?", "date": "February 21, 2019 ", "author": "Ian Kerins", "blog_data": ["Proxy management is the thorn in the side of most web scrapers. Without a robust and fully featured proxy infrastructure, you will often experience constant reliability issues and hours spent putting out proxy fires - a situation no web scraping professional wants to deal with. We, web scrapers, are interested in extracting and using web data, not managing proxies.", "In this article, we\u2019re going to tackle the great proxy question: should you build your own proxy infrastructure in-house or use an off-the-shelf proxy solution?", "But first, let\u2019s talk about...", "Although every individual web scraping project is different, proxy requirements remain remarkably similar. Your proxy infrastructure needs to be able to ", ". Anything else is a suboptimal proxy solution.", "To achieve this, at a minimum your proxy infrastructure needs to contain a sufficient number of proxies to process the desired number of requests per minute and the ability to rotate the proxies to lower the risk of bans.", "However, most web scrapers quickly discover that this rudimentary proxy infrastructure simply won\u2019t cut it at any reasonable level of scale. Very quickly the list of requirements grows even longer to enable your crawlers to reliably retrieve the data they need:", "As a result, web scrapers need to design robust management logic within their proxy infrastructure to ensure it can reliably rotate IPs, select geographical specific IPs, throttle requests, identify bans and captchas, automate retries, manage sessions, user agents and blacklisting logic.", "Turning an axillary part of your web scraping project into a large development and maintenance undertaking.", "When it comes to choosing a proxy management solution you really only have two options:", "First, let\u2019s look at your first option\u2026", "A common approach a lot of developers take when first getting started scraping the web is building their own proxy management solution from scratch.", "This approach often works very well when scraping simple websites at small scales. With a relatively simple proxy infrastructure (pool of IPs, simple rotation logic & throttling, etc.) you can achieve a reasonable level of reliability from such a solution.", "However, when scaling their web scraping or if they start scraping more complex websites they often find they increasingly start running into proxy issues. Commencing the arduous process of troubleshooting the proxy issue, obtaining more IPs, upgrading the proxy management logic, etc.", "It is rare for developers to build a extremely robust proxy infrastructure from the get-go. Typically, it is an iterative process of running into proxy issues and patching together an adequate solution to get the crawlers back up and running.", "Over time the sophistication and robustness of the proxy infrastructure does improve, however, not without sucking in significant development resources and countless late nights trying to fix the latest proxy issue.", "In recent times, at Scrapinghub we\u2019ve increasingly noticed the trend of companies looking to jump to straight to large scale web scraping as a result of the ever-growing appetite for web data in business decision making and data-driven products.", "In cases like these, it would be a massive understatement to say building a proxy management infrastructure designed to handle millions of requests per month is complex. Building this kind of infrastructure is a significant development project. Requiring months of development hours and careful planning.", "The thing is, for most developers and companies proxy management is at the bottom of their list of priorities. You are interested in extracting the target data as efficiently and quickly as possible so you can get on with their main interests - analysing and making decisions based on the data, incorporating the data into their products and services, and growing their businesses.", "In nearly every situation web scrapers have very little to gain by building their own proxy management infrastructure from scratch, other the learning experience of developing the proxy management logic or saving a small amount of money on the direct costs of proxies (oftentimes, the indirect engineering costs far outweigh the direct savings).", "That is why we always recommend to our community that they should at the very least outsource some element of their proxy management infrastructure. Be it obtaining their proxies from a provider that also offers proxy rotation or other configurations, or our recommended method using a proxy management API that completely removes the hassle of managing proxies.", "When it comes to web scraping, especially scraping at scale, our recommendation is to use a proven fully featured off-the-shelf proxy management solution.", "It will save your team countless weeks in development time, allow you to start extracting the data you need immediately and dramatically increase the reliability of your crawlers.", "Developing crawlers, post-processing and analysing the data is time intensive enough without trying to reinvent the wheel by developing and maintaining your own internal proxy management infrastructure.", "By using an off-the-shelf proxy management solution you can get access to a highly robust & configurable proxy infrastructure from day 1. No need to spend weeks delaying your data extraction building your proxy management system and troubleshooting proxy issues that will inevitably arise.", "If you are interested in using an off-the-shelf proxy management solution then we strongly recommend that you consider ", ", the complete proxy solution developed by Scrapinghub.", "Crawlera is the world's smartest proxy network built by and for web scrapers. Instead of having to manage a pool of IPs, your crawler just sends a request to Crawlera's single endpoint API and gets a successful response in return.", "Crawlera manages a massive pool of proxies, carefully rotating, throttling, blacklists and selecting the optimal IPs to use for any individual request to give the optimal results at the lowest cost. Completely, removing the hassle of managing IPs.", "Users love Crawlera because of the fact completely removes the hassle of managing proxies, freeing them up to work on more important areas of their business.", "Not only that, using Crawlera makes your web crawlers extremely reliable (", ").", "The huge advantage of using Crawlera is that it is extremely scalable. Crawlera can scale from a few hundred requests per day, to millions of requests per day without any additional workload from the user. Simply increase the number of requests you are making and Crawlera will take care of the rest.", "If you\u2019d like to learn more about how Crawlera only returns successful responses to it\u2019s users, then be sure to check out \"", " to get an inside look on how Crawlera works.", "Ok which approach is the best option for you?", "To help you make that decision, we\u2019ve outlined some questions you should be asking yourself when picking the best proxy solution for your needs:", "Your answers to these questions will quickly help you decide which approach to proxy management best suits your needs.", "If you\u2019re tired of troubleshooting proxy issues and would like to give Crawlera a try, ", "\u00a0or schedule a call with our ", ". Scrapinghub offers Crawlera in two flavours:", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might. So please leave a comment below with what you thought of the article and what you are working on."]},
{"tite": "Looking Back at 2017", "date": "December 31, 2017 ", "author": "Scott Garcia", "blog_data": ["\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0"]},
{"tite": "Spidermon: Scrapinghub\u2019s Secret Sauce To Our Data Quality & Reliability Guarantee", "date": "March 05, 2019 ", "author": "Ian Kerins", "blog_data": ["If you know anything about Scrapinghub, you know that we are obsessed with data quality and data reliability.", "Outside of building some of the most powerful web scraping tools in the world, we also specialise in helping companies extract the data they need for their mission-critical business requirements. Most notably companies who:", "In both scenarios having a reliable data extraction infrastructure that delivers high-quality data is their ", ".", "When your business's success is dependant on maintaining reliable high-quality data, any disruption to the quality of your data feeds along with their reliability can be devastating and can have huge consequences for your business.", "Imagine you are the provider of product and price monitoring services for the world's largest consumer brands (think Nike, Unilever, P&G). Your customers rely on your data-driven services to fuel their competitor research, dynamic pricing and new product research. If their data feeds stop working or suddenly start to experience data quality issues you are in serious hot water.", "Your customers will be flying blind, unable to make data-driven product pricing or positioning decisions until the data quality issue is resolved. Potentially costing them hundreds of thousands or even millions of dollars in lost revenue.", "Not to mention the technical headaches you will face when trying to diagnose and rectify the underlying problems, the reputational or contractual risk to your business would be enormous. Even a single occurrence of a serious data quality issue could kill your business.", "As a result, data quality and reliability is a burning need for many companies.", "To ensure our clients feel safe relying on Scrapinghub to deliver their data, we decided to develop Spidermon, our - until recently - proprietary library for monitoring Scrapy spiders.", "Spidermon is Scrapinghub\u2019s battle-tested ", "for monitoring Scrapy spiders. Over the last 3 years, Spidermon has been central to our ability to consistently deliver the most reliable data feeds on the market.", "In fact, we are so confident in our capabilities that we guarantee data quality and reliability in all our customer service level agreements (SLAs).", "Spidermon is a Scrapy extension for monitoring Scrapy spiders. It provides a suite of data validation, stats monitoring, and notification tools that enable Scrapinghub to quickly develop robust spider monitoring functionality for our client\u2019s spiders.", "Until recently Spidermon was a proprietary internal technology, however, as open source is our DNA, Scrapinghub\u2019s co-founders Shane Evans and Pablo Hoffman were adamant that we open source the technology to help developers scrape the web more efficiently at scale. So the decision was made to open source Spidermon.", "If you would like to learn more about how you can integrate Spidermon into your own web scraping projects then be sure to check out ", " and ", "Spidermon is central to Scrapinghub\u2019s four-layer data quality assurance process:", "The monitoring functionality Spidermon provides powers Scrapinghub\u2019s automatic and manual QA activities.", "Once integrated with a spider, Spidermon continuously runs in the background monitoring the data extraction process for potential sources of data quality or reliability issues.", "Spidermon verifies the scraped data against a schema that defines the expected structure, data types and value restrictions. Ensuring that if a spider ceases to extract all the target data correctly, the underlying problem can be immediately identified and resolved without the data quality issue ever reaching the client's data infrastructure.", "Spidermon also monitors the spider execution for bans, errors and item coverage drops, among other aspects of a typical spiders execution process that may indicate the early signs of a reliability issue. Ensuring that our engineers can investigate the causes of the reliability issue before gaps appear in the data feed (which is extremely important for some of our clients - especially investment asset managers who need perfect data integrity to enable them to backtest their investment theses).", "What makes Spidermon so powerful is the fact it can provide these technical capabilities extremely efficiently at enormous scale. Scrapinghub extracts data from over 8 billion pages per month, with Spidermon monitoring the spider execution for every single one.", "If an issue is ever detected, a QA engineer is immediately alerted to the issue, where in they can then diagnose and resolve the underlying issue.", "If you would like to take a deeper look at Scrapinghub\u2019s four-layer data quality assurance process, the exact data validation tests we conduct and how you can build your own quality system, then be sure to check our whitepaper: ", ".", "\u00a0", "We can safely say that without Spidermon, Scrapinghub would never have been able to scale our operation to the size we\u2019ve achieved whilst simultaneously improving the quality and reliability of our data feeds as our work has continued to scale.", "At Scrapinghub we specialize in turning unstructured web data into structured data. If you have a need to start or scale your web scraping projects then our ", " is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.", "At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!\u00a0"]},
{"tite": "A Faster, Updated Scrapinghub", "date": "November 05, 2017 ", "author": "Scott Garcia", "blog_data": ["We\u2019re very excited to announce a new look for Scrapinghub!", "We\u2019ve been improving your experience by streamlining common workflows, and integrating with common tools (like our ", ").", "Today\u2019s release is another step in that direction. Here is what\u2019s new:", "\u00a0", "We hope you like the new look! Most things are in the same place as before, so we hope it\u2019s a seamless transition.", "\u00a0", "For those new to the platform there\u2019s an improved onboarding experience that gets you to running your spiders within minutes of signing up.", "For everyone else, things are now simplified with your organization(s) and user profile now contained within a single dropdown menu:", "\u00a0", "A more intuitive and simplified workflow for deploying your spiders (including from Github).", "\u00a0", "Perhaps our most requested improvement: much quicker page load times!", "\u00a0", "As always, we love your feedback! Please let us know what you think by ", ".", "\u00a0"]},
{"tite": "Want to Predict Fitbit\u2019s Quarterly Revenue? Eagle Alpha Did It Using Web Scraped Product Data", "date": "June 07, 2018 ", "author": "Ian Kerins", "blog_data": ["\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0"]},
{"tite": "How Data Compliance Companies Are Turning To Web Crawlers To Take Advantage of the GDPR Business Opportunity", "date": "May 30, 2018 ", "author": "Ian Kerins", "blog_data": []},
{"tite": "A Sneak Peek Inside What Hedge Funds Think of Alternative Financial Data", "date": "June 19, 2018 ", "author": "Ian Kerins", "blog_data": []},
{"tite": "Data for Price Intelligence: Lessons Learned Scraping 100 Billion Products Pages", "date": "July 02, 2018 ", "author": "Ian Kerins", "blog_data": ["Web scraping can look deceptively easy these days. There are numerous open-source libraries/frameworks, visual scraping tools and data extraction tools that make it very easy to scrape data from a website. However, when you want to scrape websites at scale things start to get very tricky, very fast. Especially when it comes to ", ", where scale and quality matters a lot.", "In this series of articles, we will share with you the lessons we\u2019ve learned scraping over 100 billion product pages since 2010, give you an\u00a0", "Founded in 2010, Scrapinghub is one of the leading data extraction companies and the author of Scrapy, the most robust and popular web scraping framework available today. Currently, Scrapinghub scrapes over 8 billion pages per month (3 billion of which are product pages) for many of the largest e-commerce companies in the world.\u00a0", "Unlike your standard web scraping application, scraping e-commerce product data at scale has a unique set of challenges that make web scraping vastly more difficult.", "At its core, these challenges can be boiled down to two things: ", " and ", ".", "As time is usually a limiting constraint, scraping at scale requires your crawlers to scrape the web at very high speeds without compromising data quality. This need for speed makes scraping large volumes of product data very challenging.", "\u00a0", "\u00a0", "It might be obvious and it might not be the sexiest challenge, but sloppy and always changing website formats is by far the biggest challenge you will face when extracting data at scale. Not necessarily because of the complexity of the task, but the time and resources you will spend dealing with it.", "If you have spent any length of time building crawlers for e-commerce stores you\u2019ll know that there is a epidemic of sloppy code on e-commerce stores. There\u2019s more to it than HTML well-formedness or the occasional character encoding problem. We\u2019ve run into all sorts of colorful issues over the years - misused HTTP response codes, broken JavaScripts, or misuse of Ajax:", "Sloppy code like this can make writing your spider a pain, but can also make visual scraping tools or automatic extraction tools unviable.", "When scraping at scale, not only do you have to navigate potentially hundreds of websites with sloppy code, you will also have to deal with constantly evolving websites. A good rule of thumb is to expect your target website to make changes that will break your spider (drop in data extraction coverage or quality) every 2-3 months.", "That mightn\u2019t sound like too big a deal but when you are scraping at scale, those incidents really add up. For example, one of Scrapinghub\u2019s larger e-commerce projects has ~4,000 spiders targeting about 1,000 e-commerce websites, meaning they can experience 20-30 spiders failing per day.", "Variations in website layouts from regional and multilingual websites, A/B split testing and packaging/pricing variants also create a world of problems that routinely break spiders.", "Unfortunately, there is no magic bullet that will completely solve these problems. A lot of the time it just a matter of committing more resources to your project as you scale. To take the previous project as a example again, that project has a team of full-time 18 crawl engineers and 3 dedicated QA engineers to ensure the client always has reliable data feed.", "With experience however, your team will learn to create ever more robust spiders that can detect and deal with quirks in your target websites format.", "Instead of having multiple spiders for all the possible layouts a target website might use, it is best practice to have only one product extraction spider that can deal with all the possible rules and schemes used by different page layouts. The more configurable your spiders are the better.", "Although these practices will make your spiders more complex (some of our spiders are thousands of lines long), it will ensure that your spiders are easier to maintain.", "\u00a0", "\u00a0", "The next challenge you will face is building a crawling infrastructure that will scale as the number of requests per day increases, without degrading in performance.", "When extracting product data at scale a simple web crawler that crawls and scrapes data serially just won\u2019t cut it. Typically, a serial web scraper will make requests in a loop, one after the other, with each request taking 2-3 seconds to complete.", "This approach is fine if your crawler is only required to make <40,000 requests per day (request every 2 seconds equals 43,200 request per day). However, past this point you will need to transition to a crawling architecture that will allow you to scrape millions of requests per day with no decrease in performance.", "As this topic warrants a article onto itself, in the coming weeks we will publish a dedicated article discussing the how to design and build your own high throughput scraping architecture. However, for the remainder of this section we will discuss some of the higher level principles and best practices.", "As we\u2019ve discussed, speed is key when it comes to scraping product data at scale. You need to ensure that you can find and scrape all the required product pages in the time allotted (often one day). To do this you need to do the following:", "To scrape product data at scale you need to separate your product discovery spiders from your product extraction spiders.", "The goal of the product discovery spider should be for it to navigate to the target product category (or \u201cshelf\u201d) and store URLs of the products in that category for the product extraction spiders. As the product discovery spider adds product URLs to the queue the product extraction spiders scrape the target data from that product page.", "This can be accomplished with the aid of a crawl frontier such as ", ", the open source crawl frontier developed by Scrapinghub. While Frontera was originally designed for use with Scrapy, it\u2019s completely agnostic and can be used with any other crawling framework or standalone project. In this guide, we share how you can\u00a0", ".", "As each product category \u201cshelf\u201d can contain anywhere from 10 to 100 products and extracting product data is more resource heavy than extracting a product URL, discovery spiders typically run faster than product extraction spiders. When this is the case, you need to have multiple extraction spiders for every discovery spider. A good rule of thumb is to create a separate extraction spider for each ~100,000 page bucket.", "\u00a0", "\u00a0", "Scraping at scale can easily be compared to Formula 1 where you goal is to shave every unnecessary gram of weight from your car and squeeze that last fraction of a horsepower from the engine all in the name of speed. The same is true for web scraping at scale.", "When extracting large volumes of data you are always on the lookout for ways to minimise the request cycle time and maximise your spiders performance of the available hardware resources. All in the hope that you can shave a couple milliseconds off each request.", "To do this your team will need to develop a deep understanding of the web scraping framework, proxy management and hardware you are using so you can tune them for optimal performance. You will also need to focus on:", "When scraping at scale you should always be focused on solely extracting the exact data you need in as few requests as possible. Any additional requests or data extraction slow the pace at which you can crawl a website. Keep these tips in mind when designing your spiders:", "\u00a0", "If you are scraping e-commerce sites at scale you are guaranteed to run into websites employing anti-bot countermeasures.", "For most smaller websites their anti-bot countermeasures will be quite basic (ban IPs making excess requests). However, larger e-commerce websites such as Amazon, etc. make use of sophisticated anti-bot countermeasures such as Distil Networks, Incapsula, or Akamai, that make extracting data significantly more difficult.", "With that in mind the first and most essential requirement for any project scraping product data at scale is to use proxy IPs. When scraping at scale you will need a sizeable list of proxies, and will need to implement the necessary IP rotation, request throttling, session management and blacklisting logic to prevent your proxies from getting blocked.", "Unless you already have or are willing to commit a sizeable team to manage your proxies you should outsource this part of the scraping process. There are a huge number of proxy services available who provide varying levels of service.", "However, our recommendation is to go with a proxy provider who can provide a single endpoint for proxy configuration and hide all the complexities of managing your proxies. Scraping at scale is resource intensive enough without trying to reinvent the wheel by developing and maintaining your own internal proxy management infrastructure.", "This is the approach most of the large e-commerce companies use. A number of the worlds largest e-commerce companies use ", ", the smart downloader developed by Scrapinghub, that completely outsource their proxy management. When your crawlers are making 20 million requests per day, it makes much more sense to focus on analysing the data not managing proxies.", "Unfortunately, just using a proxy service won\u2019t be enough to ensure you can evade bot countermeasures on larger e-commerce websites. More and more websites are using sophisticated anti-bot countermeasures that monitor your crawlers behaviour to detect that it isn\u2019t a real human visitor.", "Not only do these anti-bot countermeasures make scraping e-commerce sites more difficult, overcoming them can significantly dent your crawlers performance if done incorrectly.", "A large proportion of these bot countermeasures use javascript to determine if the request is coming from a crawler or a human (Javascript engine checks, font enumeration, WebGL and Canvas, etc.).", "However as mentioned previously, when scraping at scale you want to limit your usage of scriptable headless browsers such as Splash or Puppeteer, that render any javascript on the page as they are very heavy on resources and slow the speed at which you can scrape a website.", "This means that to ensure you can achieve the necessary throughput from your spiders to deliver daily product data you often need to painstakingly reverse engineer the anti-bot countermeasures used on the site and design your spider to counteract them without using a headless browser.", "From a data scientists perspective the most important consideration of any web scraping project is the quality of the data being extracted. Scraping at scale only makes this focus on data quality even more important.", "When extracting millions of data points every single day, it is impossible to manually verify that all your data is clean and intact. It is very easy for dirty or incomplete data to creep into your data feeds and disrupt your data analysis efforts.", "This is especially true when scraping products on multiple versions of the same store (different languages, regions, etc.) or separate stores.", "Outside of a careful QA process during the design phase of the building the spider, where the code of the spider is peer reviewed and tested to ensure that it is extracting the desired data in the most reliable way possible. The best method of ensuring the highest possible data quality is the development of a automated QA monitoring system.", "As part of any data extraction project you need to plan and develop a monitoring system that will alert you of any data for inconsistencies and spider errors. At Scrapinghub we\u2019ve developed machine learning algorithms designed to detect:", "All of which we will discuss in a later article dedicated to automated quality assurance.", "As you have seen scraping product data at scale creates its own unique set of challenges. Hopefully, this article has made you more aware of the challenges you will face and how you should go about solving them.", "However, this is just the first article in this series so if you are interested in reading the next articles as soon as they are published be sure to sign up to our email list.", "For those of you who are interested in scraping the web at scale but are wrestling with the decision of whether or not you should build up a dedicated web scraping team in-house or outsource it to a dedicated web scraping firm then be sure to check out our guide, ", ".", "At Scrapinghub we specialize in turning unstructured web data into structured data. If you would like to learn more about how you can use web scraped product data in your business then feel free to ", " who will talk you through the services we offer startups right through to Fortune 100 companies.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might. So please leave a comment below with what you thought of the article and what you are working on.", "\u00a0", "\u00a0"]},
{"tite": "GDPR Compliance For Web Scrapers: The Step-By-Step Guide", "date": "July 25, 2018 ", "author": "Ian Kerins", "blog_data": ["Unless you\u2019ve been living under a rock for the past few months you know that the EU\u2019s General Data Protection Regulation (GDPR) is upon us.", "It is the most comprehensive data protection law ever been introduced, fundamentally changing the way companies can use the personal data of their customers and prospects.", "There are countless articles and guides about how GDPR will affect your company\u2019s marketing efforts, lead generation, etc. and the changes you\u2019ll need to make to ensure your company is in full compliance with the law.", "But when it comes to web scraping....nothing.", "Which is strange given that web scraping has traditionally been the backbone of many companies marketing, lead generation and market intelligence efforts.", "To shed some light on this grey area, I sat down with Sanaea Daruwalla, Head of Legal at Scrapinghub, to get her insights on how Scrapinghub ensures our clients are scraping personal data in a GDPR compliant way.", "In this guide I will share with you:", "Before we get started though, I want to highlight a quick disclaimer.", ": I am not a lawyer, and the recommendations in this guide do not constitute legal advice. Our Head of Legal is a lawyer, but she\u2019s not your lawyer, so none of her opinions or recommendations in this guide constitute legal advice from her to you. The commentary and recommendations outlined below are based on Scrapinghub\u2019s experience helping our clients (startups to Fortune 100\u2019s) maintain GDPR compliance whilst scraping 7 billion web pages per month. If you want assistance with your specific situation then you should consult a lawyer.", "Now with the technicalities out of the way, let\u2019s talk about how you should evaluate your web scraping project for GDPR compliance.", "This is the very first and most obvious question you should be asking yourself when you are instigating a web scraping project.", "The General Data Protection Regulation, or GDPR as it is more commonly known, only applies to personal data. Which is defined as any personally identifiable information (PII) that could be used to directly or indirectly identify a specific individual. Examples of personal data include a person's:", "If you aren\u2019t scraping personal data, then GDPR does not apply. However, if you are scraping personal data then move to step 2.", "If you are scraping personal data then the next question you need to ask yourself is whether or not you are scraping the personal data of EU citizens or residents (note that the GDPR actually covers the EEA, which includes all EU countries, plus Iceland, Liechtenstein, and Norway, so it\u2019s a bit broader than just the EU).", "GDPR is an EEA specific regulation, so it only applies to EU citizens. If you are scraping the personal information of residents of other countries (ex. US, Canada, Australia, etc.) then GDPR may not apply. You just need to comply with the data protection laws in the jurisdiction that you scraping personal data from.", "Ok, now we are starting to get into the nuts and bolts of GDPR. We now know we are scraping personal data and there will be EU citizens affected. The next question we need to ask yourselves is:", "Under GDPR to use or hold the personal data of any EU citizen a company must comply with one or more of the following legal reasons for storing or using their personal data, otherwise they will be in breach of the regulation. The five types of lawful reasons are:", "When a client comes to Scrapinghub looking to scrape the personal data of EU residents we take it on a case by case basis because it is vital that you can prove that you have a lawful reason to scrape that data.", "The most common legal reasons in the case of web scraping are ", " and ", ".", "First, let\u2019s take a look at consent...", "For most web scrapers, demonstrating that you have consent from the individual to scrape their personal data will be the main (and often only) method in which you can lawfully scrape the personal data from EU residents.", "Prior to the commencement of GDPR, there was a lot of discussion within the web scraping community on whether an EU resident had to implicitly give their consent for companies to scrape their personal data if it was available on public websites (no login required to see the data).", "The argument was that by uploading personal data to a public site you are giving consent for that data to be viewed and stored by 3rd parties.", "However, after in-depth review of this argument by Sanaea (Head of Legal at Scrapinghub) and external legal experts contracted by Scrapinghub we concluded that this interpretation of the regulations wasn\u2019t compliant with GDPR.", "As a result, to scrape the personal data of EU residents you now need to demonstrate that you have the explicit consent of the individual before scraping their personal data.", "A lot of web scrapers mightn\u2019t like this position, but after a careful review of all the guidance documents provided by the commission Scrapinghub believes that adopting this policy is the only one that is guaranteed to prevent you and your company falling foul to GDPR.", "Obviously, this interpretation of the GDPR regulations will significantly curtail most web scraping projects focused on extraction of the personal information of EU residents for lead generation, market analysis, etc.", "However, it will still enable some companies to scrape the personal data of EU citizens if they have obtained their explicit content to do so. An example of this would be companies like Mint.com, where users give Mint consent to log into their online banking accounts and retrieve their banking transactions so that they can be tracked and displayed in a more user friendly format on Mint.com.", "Next, we\u2019ll look at using \u201clegitimate interest\u201d as the your lawful reason for scraping the personal data of EU citizens.", "The other likely lawful reason available to web scrapers is if they can demonstrate they have a legitimate interest in scraping/storing/using this personal data.", "Although this lawful reason is viable for web scrapers, for most companies it will be very difficult for them to demonstrate that they have a legitimate interest in scraping someone's personal data.", "In most cases, only governments, law enforcement agencies, etc. will have what would be deemed a to have a legitimate interest in scraping the personal data of its citizens as they will typically be scraping people's personal data for the public good.", "As mentioned in Step 3, when a client approaches Scrapinghub looking to scrape the publicly available personal data of EU residents we take it on a case by case basis and work with the client to ensure that this data is being extracted in a GDPR compliant manner.", "During this stage not only do we look at the companies lawful reason for scraping personal data we also look at the type of personal data they want to extract, the extent of the proposed data collection and how they plan to use the data post extraction.", "There are a number of reasons for taking this approach:", "Under the GDPR regulation, there are certain types of data that are classed as \u201csensitive\u201d . These include any type of personal data that could indicate a person's:", "Scraping sensitive data means that you are subject to additional rules and require specific consent to be given for this data to be scraped and stored. Therefore, unless you have clear explicit consent and legitimate reason to scrape this data you should avoid scraping it.", "A important part of GDPR is that companies should only store and process as much data as is required to successfully accomplish a given task.", "Given web scrapings ability to extract large quantities of data from a website there is sometimes the desire to capture as much data as possible as it might be useful in the future. Obviously, this mindset isn\u2019t in line with the new GDPR regulations.", "As a result, when Scrapinghub is evaluating a scraping project we often work with client companies to minimise the amount of personal data they extract from a website and to define retention periods to ensure they comply with GDPR. You should adopt a similar evaluation process for your own scraping projects to ensure you comply with GDPR\u2019s minimisation requirements.", "Even if you can argue that you have a legitimate interest in this data or have the users consent to extract and store their personal data, under GDPR you need to have a clear and legal reason for doing so and be able to demonstrate that it will be used for legitimate business purposes.", "If the proposed scraping project doesn\u2019t raise any red flags after being evaluated on these criteria then we will generally commence the scraping project.", "As the GDPR regulation defines IP addresses as personally identifiable information you need to ensure that any EU residential IPs you use as proxies are GDPR compliant.", "This means that you need to ensure that the owner of that residential IP has given their explicit consent for their home or mobile IP to be used as a web scraping proxy.", "If you own your own residential IPs then you will need to handle this consent yourself. However, if you are obtaining residential proxies from a 3rd party provider then you need to ensure that they have obtained consent and are in compliance with GDPR prior to using the proxy for your web scraping project", "That is everything you need to know about any future web scraping projects, however, what does GDPR mean for personal data that you may have extracted from websites previously?", "Luckily for us you just need to use the same process as outlined above to ensure the GDPR compliance of any old web scraping projects:", "GDPR is perhaps the most impactful data protection law ever passed, and it will change the way data is extracted from websites forever."]},
{"tite": "What I Learned as a Google Summer of Code student at Scrapinghub", "date": "September 12, 2018 ", "author": "Lam Chau Tung Nguyen", "blog_data": ["Google Summer of Code (GSoC) was such a great experience for students like me. I learned so much about open source communities as well as contributing to their complex projects. I also learned a great deal from my mentors, Konstantin and Cathal, about programming and software engineering practices. In my opinion, the most valuable lesson I got from GSoC was what it was like to be a Software Engineer, which prepared me to continue the pursuit of my dream career in technology.", "GSoC is a program hosted by Google for students to spend their summer contributing to open source projects from various organizations. Fortunately, my proposal for a project called Scrapy in Python Software Foundation got accepted. Before I applied for the program, I had no idea what open source was, so I decided to take some small steps to get started on contributing. I spent 3 months studying smaller projects within the Scrapy organization to familiarize myself with the code. I found the application easier after I got used to contributing. I also received a lot of help from people within the community, especially from Konstantin, who was to be my GSoC mentor. From my experience, it was extremely important to have a willingness to learn and ask for help when anyone wanted to start participating in an open source project.", "In the beginning, I did not know what to expect at GSoC due to the mixed review from students. I figured since it is an individual project for each student, their experiences must be different from one another. I also wondered how a student-built project could have significant impact on the community. However, I came out of GSoC with a completely different perspective. I learned that it was me who would make or break my project, and I was responsible to push myself harder. I also believed my project was a success because I got other contributors\u2019 attention and, hopefully, I inspired more students to do the same thing!", "The first week of GSoC was the most confusing part of the program, because my original project proposal was too generic. It took more effort than I anticipated to build the product I planned on originally. However, it was okay, because it was a common mistake among students, since most of us never worked on a large scale project before.", "I was first assigned to profile the Scrapy project spider to identify the components that had significant run time to implement speed improvement. It is a meticulous process, so I had to spend so much time on it. I was bored because I thought I did not join GSoC to analyze graphs. However, I learned that everything had to start small. I could spend less time on profiling and started coding instead, but I would be building something meaningless, since I wasn\u2019t sure which component to be optimized. I believe it is an analogy to building a skyscraper with an insecure foundation.", "After a lot of time profiling, I finally found out what component needed optimization, the URL parsing library from CPython urllib. I then had to profile again. I was getting impatient, since I couldn\u2019t get my hands on coding yet. The potential libraries fell into 2 types: those that didn\u2019t improve anything at all and those that were fast but not compatible with Scrapy. I felt hopeless from being stuck on the same problem for such a long time. However, we, student developers, will have to experience that eventually. I took a deep breath and continued with the task that I was assigned. Eventually, I decided to build a library from scratch to replace urllib, and I named the project Scurl (", ").", "After a month developing Scurl, I got it to a stable stage. However, the Chromium source I used was from another project on Github, so Scurl would not last long if the Chromium source could not be updated. What if Chromium would release a patch to the components that the library uses? Or the source code would change completely in 20 years? My project would be thrown away.", "Chromium is a gigantic project. Building the Chromium Source on an average machine is slower than a snail running half a mile (quoted from ", "). Working with just 2 components of the project was really difficult. Since I did not have any prior experience working on Chromium\u2019s source, or C++, I spent a lot of time trying to track which source files I needed for my project. It was taking me too long to figure it out, and I thought of giving up several times. However, giving up was not an option, since I already spent two and a half months for the project. Despite the struggle, I learned how to update the Chromium source code, which allowed others to maintain this library with ease.", "Overall, GSoC gave me a chance to be a better software developer. Not only did I have an opportunity to hone my programming skills, I also trained my mind to be ready for the career path that I chose. I am truly grateful for the experience that I had. I want to thank my mentors, Konstantin and Cathal for their help and support, and I hope that I have inspired others to go out of their comfort zones to do the same!", "Special thanks to Tram Nguyen and Samuel Coveney for helping me edit this article!"]},
{"tite": "How You Can Use Web Data to Accelerate Your Startup", "date": "November 10, 2016 ", "author": "Cecilia Haynes", "blog_data": ["In just the US alone, there were ", " running or starting a new business in 2015. With this fiercely competitive startup scene, business owners need to take advantage of every resource available, especially given a ", ". Enter web data. Web data is abundant and those who harness it can do everything from keeping an eye on competitors to ensuring customer satisfaction.", "You can get web data through a process called ", ". Since websites are created in a human readable format, software can\u2019t meaningfully analyze this information. While you could manually (read: the time-consuming route) input this data into a format more palatable to programs, ", " automates this process and eliminates the possibility of human error.", "If you\u2019re new to the world of web data or looking for creative ways to channel this resource, here are three real world examples of entrepreneurs who use scraped data to accelerate their startups.", "Max Robinson", "CBDiablo UK", "The key to staying ahead of your competitors online is to have excellent online visibility, which is why we invest so much in paid advertising (Google Adwords). But it occurred to me that if you aren't offering competitive prices, then you're essentially throwing money down the drain. Even if you have good visibility, users will look elsewhere to buy once they've seen your prices.", "Although I used to spend hours scrolling through competitor sites to make sure that I was matching all of their prices, it took far too long and probably wasn't the best use of my time. So instead, I started scraping websites and exporting the pricing information into easily readable spreadsheets.", "This saves me huge amounts of time, but also saves my copywriter time as they don't have to do as much research. We usually outsource the scraping, as we don't really trust ourselves to do it properly! The most important aspect of this process is having the data in an easily readable format. Spreadsheets are great, but even they can become too muddled up with unnecessary information.", "Chris McCarron", "Founder of GoGoChimp", "We use a variety of different sources and data to get our clients more leads and sales. This is really beneficial to our clients that include national and international brands who all use this information to specifically target an audience, boost conversions, increase engagement and/or reduce customer acquisition costs.", "Web data can help you know which age groups, genders, locations, and devices convert the best. If you have existing analytics already in place, you can enrich this data with data from around the web, like reviews and social media profiles, to get a more complete picture. You\u2019ll be able to use this enriched web data to tailor your website and your brand's message so that it instantly connects to who your target customer is.", "For example, by using these techniques, we estimate that our client ", " will increase their annual revenue by $450,000.", "Mike Catania", "CTO of PromotionCode.org", "The coupon business probably seems docile from the outside but the reality is that many sites are backed by tens of millions of dollars in venture capital and there are only so many offers to go around. That means exclusive deals can easily get poached by competitors. So we use scraping to monitor our competition to ensure they're not stealing coupons from our community and reposting them elsewhere.", "Both the IT and Legal departments use this data--in IT, we use it more functionally, of course. Legal uses it as research before moving ahead with cease and desist orders.", "And there you have it. Real use cases of web data helping\u00a0companies with\u00a0competitive pricing, competitor monitoring, and increasing conversion for sales. Keep in mind that\u00a0it\u2019s not just about having the web data, it\u2019s also about quality and using a reputable company to provide you with the information you need to increase your revenue.", "Please share any other ways that web data has helped you in the comments below."]},
{"tite": "Data Quality Assurance for Enterprise Web Scraping", "date": "September 27, 2018 ", "author": "Ian Kerins", "blog_data": ["When it comes to web scraping, one key element is often overlooked until it becomes a big problem.", "That is ", ".", "Getting consistent high quality data when scraping the web is critical to the success of any web scraping project, particularly when scraping the web at scale or extracting mission critical data where accuracy is paramount.", "Data quality can be the difference between a project being discontinued or it giving your business a huge competitive edge in a market.", "In this article we\u2019re going to talk about data quality assurance for web scrapers, give you a sneak peak into some of the tools and techniques Scrapinghub has developed and share with you some big news as we are open sourcing one of our most powerful quality assurance tools. These QA processes enable us to verify the quality of our clients\u2019 data at scale, and confidently give all our clients data quality and coverage guarantees.", "From a business perspective, the most important consideration of any web scraping project is the quality of the data being extracted. Without a consistent high quality data feed your web scraping infrastructure will never be able to help your business achieve its objectives.", "Today, with the growing prevalence of big data, artificial intelligence and data driven decision making, a reliable source of rich and clean data is a major competitive advantage. Compounding this is the fact that many companies are now directly integrating web scraped data into their own customer-facing products, making real-time data QA a huge priority for them.", "Scraping at scale only magnifies the importance of data quality. Poor data accuracy or coverage in a small web scraping project is a nuisance, but usually manageable. However, when you are scraping hundreds of thousands or millions of web pages per day, even a small drop in accuracy or coverage could have huge consequences for your business.", "At the commencement of any web scraping project, you always need to be thinking about how you are going to achieve the high levels of data quality you need when scraping the web.", "We know that getting high quality data when scraping the web is often of critical importance to your business, but what makes it so complex?", "This is a combination of factors really:", "- The first and most important aspect of data quality verification is clearly defined requirements. Without knowing what data you require, what the final data should look like and what accuracy/coverage level you require, it is very hard to verify the quality of your data. Quite often companies come to Scrapinghub not having clear data requirements laid out, so we need to work with the client to properly define what are these requirements. We find that a good question to ask is:", "In order to make your data quality targets realistic and achievable, it is important that you specify your requirements clearly and that they be \u201ctestable\u201d, particularly when one or more of the following is true:", " The beauty of web scraping is that it has a unmatched ability to scale very easily compared to other data gathering techniques. However, data QA often isn\u2019t able to match the scalability of your web scraping spiders, particularly when it involves only manual inspection of a sample of the data and visual comparison with the scraped pages.", " Perhaps the biggest cause of poor data coverage or accuracy is changes to the underlying structure of all or parts of the target website. With the increasing usage of A/B split testing, seasonal promotions and regional/multilingual variations, large websites are constantly making small tweaks to the structure of their web pages that can break web scraping spiders. As a result, it is very common for the coverage and accuracy of the data from your spiders to degrade over time unless you have continuous monitoring and maintenance processes in place.", "- Verifying the semantics of textual information, or the meaning of the data that is being scraped, is still a challenge for automated QA as of today. While ourselves and others are developing technologies to assist in the verification of the semantics of the data we extract from websites, no system is 100% perfect. As a result, manual QA of the data is often required to ensure the accuracy of the data.", "At a high level, your QA system is trying to assess the quality/correctness of your data along with the coverage of the data you have scraped.", "Depending on the scale, number of spiders, and the degree of complexity of your web scraping requirements, there are different approaches you can take when developing an automated quality assurance system for your web scraping.", "Due to the number of clients we scrape the web for and the wide variety of web scraping projects we have in production at any one time, Scrapinghub have experience with both approaches. We\u2019ve developed bespoke project-specific automated test frameworks for individual projects with unique requirements. We rely principally though on the generic automated test framework we\u2019ve developed that can be used to validate the data scraped by any spider.", "When used alongside Spidermon (more on this below), this framework allows us to quickly add a quality assurance layer to any new web scraping project we undertake.", "The other key component of any web scraping quality assurance system is a reliable system for monitoring the status and output of your spiders in real-time.", "A spider monitoring system allows you to detect sources of potential quality issues immediately after spider execution completes.", "At Scrapinghub we\u2019ve developed Spidermon, which allows developers (and indeed other stakeholders such as QA personnel and project managers) to automatically monitor spider execution. It verifies the scraped data against a schema that defines the expected structure, data types and value restrictions. It can also monitor bans, errors, and item coverage drops, among other aspects of a typical spider execution. In addition to the post-execution data validation that Spidermon performs, we often leverage real-time data-validation techniques, particularly for long-running spiders, which enables the developer to stop a spider when it is detected that it is scraping unusable data.", "This brings us to the big news to we have to announce. Scrapinghub is delighted to announce that in the coming weeks we are going to open source Spidermon, and making it an easy use add-on for all your Scrapy spiders. It can also be used with spiders developed using other Python libraries and frameworks such as BeautifulSoup.", "\n", "\n", "\n", "To ensure the highest data quality from our web scraping, Scrapinghub applies a four-layer QA process to all the projects we undertake with clients.", "Only after passing through all four of these layers is the dataset then delivered to the client.", "To get a detailed behind the scenes look at how Scrapinghub\u2019s quality system works, the exact data validation tests we conduct and how you can build your own quality system, then click on the image below to download our Web Scraping Quality Assurance Guide.", "As you have seen, there is often quite a bit of work in ensuring your web scraping projects are actually yielding the high quality data you need to grow your business. Hopefully, this article has made you more aware of the challenges you will face and how you could go about solving them.", "At Scrapinghub we specialize in turning unstructured web data into structured data. If you would like to learn more about how you can use web scraped data in your business then feel free to contact our Sales team, who will talk you through the services we offer startups right through to Fortune 100 companies.", "At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on right now.", "Happy scraping!"]},
{"tite": "Interview: How Up Hail uses Scrapy to Increase Transparency", "date": "October 06, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Avi sat down with Cecilia and shared how he and his team use Scrapy and web scraping to help users find the best rideshare and taxi deals in real time.", "Fun fact, ", " was named one of ", ".", " Thanks for meeting with me! Can you share a bit about your background, what your company is, and what you do?", ": We are team ", " and we are a search engine for ground transportation like taxis and ride-hailing services. We are now starting to add public transportation like trains and buses, as well as bike shares. We crawl the web using ", " and other tools to gather data about who is giving the best rates for certain destinations.", "Scrapy for the win", "There\u2019s a lot of data out there, especially public transportation data on different government or public websites. This data is unstructured and a mess and without APIs. Scrapy\u2019s been very useful in gathering it.", " How has your rate of growth been so far?", " Approximately 100,000 new users a month search our site and app, which is nice and hopefully we will continue to grow. There\u2019s a lot more competition now than when we started, and we\u2019re working really hard to be the leader in this space.", "Users come to our site to compare rates and to find the best deals on taxis and ground transportation. They are also interested in finding out if the different service providers are available in their cities. There are many places in the United States and across the world that don\u2019t have these services, so we attract those who want find out more information.", "We also crawl and gather a lot of different product attributes such as economy vs. luxury, shared vs. private, how many people each of these options fit, whether they accept cash, and whether you can book in advance.", "Giving users transparency on different car services and transportation options is our mission.", " By the way, where are you based?", " We\u2019re based in midtown Manhattan in a place called ", ". This is run by a very notable web designer and author named ", " who has been gracious enough to host us. He also runs A Book Apart, An Event Apart, and A List Apart, which are some of the most popular communities for web developers and designers.", " You have really found some creative applications for Scrapy. I have to ask, why ", "? What do you appreciate about it?", " A lot of the sites that we're crawling are a mess. Especially the government transit ones and local taxi companies. As a framework, Scrapy has a lot of features built in right out the box that are useful for us.", " Is there anything in particular that you're like, \u201cI'm obsessed with this aspect of Scrapy?\u201d", " We're a Python shop and Scrapy is the Python library for building web crawlers. That's primarily why we use it. Of course, Scrapy has such a vibrant ecosystem of developers and it's just easy to use. The ", " and it was super simple to get up and started. It just does the job.", "We're grateful that you make such a wonderful tool [Note: We are the ", " and ", " of Scrapy] that is free and open source to startups like us. There's a lot of companies in your space that are charging a lot of money and making it cost prohibitive to use.", " That's really great to hear! We're all about open source, so keeping Scrapy forever free is a really important aspect of this approach.", " So tell me a bit more about why you\u2019re a Python shop?", " Our application runs on the ", " and we're using Python libraries to do a lot of the back-end work.", " Dare I ask why you\u2019re using Python?", " One of the early developers on the project is a Xoogler, and Python is one of Google's primary languages. He really inspired us to use Python and we just love the language because it's the philosophy of readability, brevity, and making it simple and powerful enough to get the job done.", "I think developer time is scarce and Python makes it faster to deploy, especially for a startup that needs to ship fast.", " May I ask you've used our Scrapy Cloud Platform to deploy Scrapy crawlers?", " We haven't tried it out yet. We just found out about Scrapy Cloud, actually.", " Really? Where did you hear about us?", " I listen to a Python podcast [", "] which was with Pablo, one of your co-founders. I didn't know about how Scrapy originated from your co-founders. When I saw your name in the Collision Conference app, I was like, \"Oh, I know these guys from the podcast! They're maintainers of Scrapy.\" Now that we know about Scrapy Cloud, we'll give it a try.", "We usually run Scrapy locally or we'll deploy Scrapy on an EC2 instance on Amazon Web Services.", " Yeah, Scrapy Cloud is our ", " that lets you build, deploy, and scale your Scrapy spiders. We\u2019ve actually just included support for Docker. Definitely let me know what you think of Scrapy Cloud when you use it.", " Definitely, I'll have to check it out.", " Where are you hoping to grow within the next five years?", " That's a very good question. We're hoping to, of course, expand to more regions. Right now, we're in the United States, Canada, and Europe. There's a lot of other countries that have a tremendous population that we're not covering. We'd like to add a lot more transportation options into the mix. There's all these new things like on-demand helicopters and we want to just show users going from point A to point B all their available options. We're kind of like the Expedia of ground transportation.", "Also, we're adding a lot of interesting new things like a ", ". We're scoring how rideshare-friendly a city is. New York and San Francisco, of course, get 10s, but maybe over in New Jersey, where there are less options, some cities will get 6 or 7. It depends on how many options are available. Buffalo, New York, for example, doesn't have Uber or Lyft and they would probably get like a 1 because they only have yellow taxis. This may be useful for users that are thinking of moving to a city and want to know how accessible taxi and rideshares are. We want to give users even more information about taxis and transportation options.", " It seems that increasing transparency is a large part of where you want to continue to grow.", " The transportation industry is not as transparent as it should be. We've heard stories at the Expo Hall [at Collision Conference] of taxi drivers ripping off tourists because they don't know in advance what it's going to cost. By us scraping these sites, taking the rate tables, and computing the estimates, they can just fire up our app and have a good idea of what it's going to cost.", " Is your business model based on something like Expedia\u2019s approach?", " Similar. We get a few dollars when we sign up new users to the various providers. We're signing up a few thousand users a month. While it\u2019s been really good so far, we need to grow it tremendously and we're looking for other business models. Also, advertising on the site has been good for us as well, but, of course, it's limited. Don't want to be too intrusive to our users by being overly aggressive with ads, so we're trying to keep it clean there.", " Within the next few months we hope to launch a public API to outside developers and other sites. We've talked with a lot of other vendors here at the expo like travel concierge apps and the like that want to bring in our data.", " Oh, that's great! Seems to be the makings for a lot of cross-platform collaboration.", " We've gathered a lot of great data, thanks to Scrapy and other crawling tools, and we hope to make it available for others to use.", "In fact, I specifically reached out to you to tell you how awesome Scrapy was.", " Well I\u2019m thrilled you did! And I\u2019m so glad we also got to talk about Python and how you use it in your stack.", " Definitely. We are heavily using Python to get the job done. We think it's the right tool for the job and for what we're doing.", "Team Up Hail at Collision Conference 2016", "."]},
{"tite": "An Introduction to XPath: How to Get Started", "date": "October 27, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["With XPath, you can extract data based on text elements' contents, and not only on the page structure. So when you are scraping the web and you run into a hard-to-scrape website, XPath may just save the day (and a bunch of your time!).", "This is an introductory tutorial that will walk you through the basic concepts of XPath, crucial to a good understanding of it, before diving into more ", ".", " You can use the ", " to experiment with XPath. Just paste the HTML samples provided in this post and play with the expressions.", "Consider this HTML document:", "XPath handles any XML/HTML document as a tree. This tree's root node is not part of the document itself. It is in fact the parent of the document element node (", " in case of the HTML above). This is how the XPath tree for the HTML document looks like:", "As you can see, there are many node types in an XPath tree:", "Distinguishing between these different types is useful to understand how XPath expressions work. Now let's start digging into XPath.", "Here is how we can select the title element from the page above using an XPath expression:", "This is what we call a ", ". It allows us to specify the path from the ", " (in this case the root of the tree) to the element we want to select, as we do when addressing files in a file system. The location path above has three ", ", separated by slashes. It roughly means: ", ". The context node changes in each step. For example, the ", " node is the context node when the last step is being evaluated.", "However, we usually don't know or don\u2019t care about the full explicit node-by-node path, we just care about the nodes with a given name. We can select them using:", "Which means:", ". In this example, ", " is the ", " and ", " is the ", ".", "In fact, the expressions we've just seen are using XPath's ", ". Translating ", " to the full syntax we get:", "So, ", " in the abbreviated syntax is short for ", ", which means ", ". This part of the expression is called the ", " and it specifies a set of nodes to select from, based on their direction on the tree from the current context (downwards, upwards, on the same tree level). Other examples of ", " are: parent, child, ancestor, etc -- we\u2019ll dig more into this later on.", "The next part of the expression, ", ", is called a ", ", and it contains an expression that is evaluated to decide whether a given node should be selected or not. In this case, it selects nodes from all types. Then we have another axis,", ", which means ", ", followed by another node test, which selects the nodes named as ", ".", "So, the ", " defines where in the tree the ", " should be applied and the nodes that match the node test will be returned as a ", ".", "You can test nodes against their name or against their type.", "Here are some examples of name tests:", "And here are some examples of node type tests:", "We can also combine name and node tests in the same expression. For example:", "This expression selects the text nodes from inside ", " elements. In the HTML snippet shown above, it would select \"This is the first paragraph.\".", "Now, ", ". Consider this HTML document:", "Say we want to select only the first ", " node from the snippet above. We can do this with:", "The expression surrounded by square brackets is called a ", " and it filters the node set returned by ", " (that is, all ", " nodes from the document) using the given condition. In this case it checks each node's position using the ", " function, which returns the position of the current node in the resulting node set (notice that positions in XPath start at 1, not 0). We can abbreviate the expression above to:", "Both XPath expressions above would select the following element:", "Check out a few more predicate examples:", "So, a location path is basically composed by steps, which are separated by ", " and each step can have an axis, a node test and a predicate. Here we have an expression composed by two steps, each one with axis, node test and predicate:", "And here is the same expression, written using the non-abbreviated syntax:", "We can also ", " multiple XPath expressions in a single one using the union operator ", ". For example, we can select all ", " and ", " elements in the document above using this expression:", "Now, consider this HTML document:", "Say we want to select only the ", " elements whose link points to an HTTPS URL. We can do it by checking their ", " ", ":", "This expression first selects all the ", " elements from the document and for each of those elements, it checks whether their ", " attribute starts with \"https\". We can access any node attribute using the ", " syntax.", "Here we have a few additional examples using attributes:", "We've seen only two types of axes so far:", "But there's ", " and we'll see a few examples. Consider this HTML document:", "Now we want to extract only the first paragraph after each of the titles. To do that, we can use the ", " axis, which selects all the siblings after the context node. Siblings are nodes who are children of the same parent, for example all children nodes of the ", " tag are siblings. This is the expression:", "In this example, the context node where the ", " axis is applied to is each of the ", " nodes from the page.", "What if we want to select only the text that is right before the ", "? We can use the ", " axis:", "In this case, we are selecting the first text node before the ", " footer (", ").", "XPath also allows us to select elements based on their text content. We can use such a feature, along with the ", " axis, to select the parent of the ", " element whose text is \"Footer text\":", "The expression above selects ", ". As you may have noticed, we used ", " here as a shortcut to the ", " axis.", "As an alternative to the expression above, we could use:", "It selects, from all elements, the ones that have a ", " child which text is \"Footer text\", getting the same result as the previous expression.", "You can find additional axes in the XPath specification: ", "XPath is very powerful and this post is just an introduction to the basic concepts. If you want to learn more about it, check out these resources:", "And ", ", because we will post a series with more XPath tips from the trenches in the following months."]},
{"tite": "Why Promoting Open Data Increases Economic Opportunities", "date": "October 19, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Coming fresh off his talk on \"", "\", Cecilia was thrilled to chat with Tyrone all about the democratization of data and how open data can help anyone build innovative products and services.", ": Thanks for meeting with me! I saw your talk and I thought you would be the the perfect person to reach out to. Since you're in government, you're approaching data in a different way than the business or tech world. What is your take on open data?", ": Data within startups and companies is proprietary. I have this big issue with data ownership, data privacy, and data security and many companies feeling that because they collected and are stewards for data, they immediately have ownership rights.", "For example, who does the data belong to if you were in a hospital and the hospital takes down your information for an evaluation? When a\u00a0hospital generates data on your condition in the process of delivering care, you likely believe that that data is still yours. However hospitals don\u2019t assume that.", ": I actually didn't know that. That's troubling.", ": I mean it's basically their proprietary intellectual property at that point where they now have the right to sell it based upon the terms and conditions that you actually agreed to.", "It's the same thing that happens when you use something like a Fitbit.", "I looked at your hand and was just like, \"That data is not yours.\"", "\"We want to reduce the barrier of entry for people working on and with data.\"", ": What is the government\u2019s approach to data?", ": So the government is more focused on the power of open data and how do we actually increase the accessibility and usability of it.", "This includes exploring how to enable public-private data partnerships, and, in the process, help government be more data-driven in how it\u2019s run. What I've observed is that the Department of Commerce, for example, has highly valuable data sets.", "A quick example is ", ", which is the National Oceanic and Atmospheric Administration. Commerce has twelve bureaus and NOAA is a bureau within Commerce. NOAA provides information for the weather industry globally.", "It's all free, but no one really knows this. It is technically all open, but it's very difficult to find and it's very difficult to actually understand.", "And there are some companies that have leveraged this information by investing in understanding it and making it clean and accessible. That's why you have theweather.com, that's why you have the weather channel, that's all NOAA.\u00a0Even worse, NOAA collects around 20-30 terabytes of data per day. They even have satellites monitoring the sun\u2019s surface. They have sensors monitoring sounds underwater, you name it, they monitor it.\u00a030 terabytes a day, but they only actually release 2 terabytes of that data and it's only a fraction of those 2 terabytes that funds the world\u2019s weather system.", ": Oh, so is that why weather predictions are unreliable?", ": No, no, that's not the data's fault. That is on the analytical models on top of the data.", "If you had access to more data, and you had a better understanding of the nuances of collection like what you have to filter out and what overlaps, then you can actually get better models. The prediction models are actually better now than they were like three years ago and current three-day predictions are pretty spot on.\u00a0If we go farther than that, then okay, not so reliable\u2026", ": What other data sources can benefit companies?", ": The Census Bureau has this thing called the American Community Survey which basically documents the daily lives of all Americans. So, if you want to know anything, they have tens of thousands of features, which means tens of thousands of descriptors on the lives of Americans.", "Every single study that you see that actually talks about how Americans are living, or whatever else, that's all from the Census Bureau. These studies don't recognize the Census, they don't like give attribution back to the Census. But there is nowhere else the data can come from.", "Say I wanted to get access to senior citizens over 65 who collected social security benefits and who used to commute 10 miles to their job. Almost any attribute you could actually think of, you could find this demographic right now with open data.", ": And it's all completely available?", ": It's all open. There is a project called the ", " that we're doing at the department where we produce tutorials that show you:", "Here's a valid data set, here is a story as to why you should care about the data set, here is how to get it, here is how it\u2019s processed, here is how to actually make some visualizations from it, here is how to actually analyze it. Go.", ": The democratization of data is such a big deal to us as well. It\u2019s why we ", " and products, and why we made an ", ", so that anyone can engage with web data.", "One of our goals is to enable data journalists, data scientists, everyday people to be able to use our tools to seek out the information they need.", ": Commerce is really dedicated to this goal as well. That\u2019s why we have a startup now within Commerce called ", " whose mission it is to support all the bureaus on their data initiatives.", "We want to fundamentally and positively change the way citizens and businesses interact with the data products from Commerce. We recognize the problems are tied to\u00a0marketing, access, and usability.", "The Data Service commits to having everything in the open, everything transparent as much as possible. If you want to see everything we're working on right now, it's on ", ".", "Take a look at the ", " since we have a bunch of tutorials on everything from census data to data from ", " which is the standard's organization that has everything from internet security standards to time standards, you name it.", "We also have satellite information. So there is a satellite that was launched, I think it was October 2011, called the Delta 2. It had on it this device called the Visible Infrared Imaging Radiometer Suite, VIIRS, which actually monitors all human activity as it goes around.", "So a bunch of scientists have been looking at this VIIRS data set that no one knows exists and figured out that it's a really good proxy for a lot of amazing stuff. For example, you could actually use satellite imagery to predict population very simply. You could even use it to predict the number of mental health related arrests in San Francisco. You could also use it to figure out economic activity in a particular place.", ": So do you incorporate machine learning into analyzing the data?", ": We've got the platform and we have examples that show you how to use machine learning with the data sets. If you want to use machine learning algorithms on a data set, you can find everything you need. If you want to use the data sets with something else in a really straight forward way to do straight mapping, for example, then you have it on our platform.", ": This is actually really helpful to me because we have partnerships with ", ", a company that specializes in predictive analytics, along with ", ", a machine learning company that works on text analysis.", "We\u2019re always looking for new ways to highlight our collaboration, so we\u2019ll have to check out VIIRS.", ": What is your role in the Department of Commerce?", ": I'm the Deputy Chief Data Officer. I'm one of three people that leads this Commerce Data Service and the office itself is the lead for the data pillar across Commerce.", "The Secretary has a strategic plan that has five initiatives that everyone has to tie into, data is one of them and we're responsible for making sure that data is successful.", ": Have you found it really challenging so far?", ": The support from the Secretary and the senior staff at Commerce has been amazing. The challenge has actually been that we are not in the private sector. Since it is a little bit different delivering products in government than it is in private industry.", "In the private industry, you're focused on clicks, and buys, and elastic problems where it's all about growing and shrinking some base. Whereas with government, it's more of the hardest, most difficult problems that can considered baseline needs like, \u201cI'd like to have health care. I'd like not to be homeless.\u201d", "These are problems that you know no company that will actually tackle because there is no profit motive, but these should be basic intrinsic rights for anyone who lives in the US. These are the problems that the government has to handle, and we have to produce amazing data products to make sure that we approach them in the right way.", ": So your goal is to create products that allow people to access data more easily?", ": Our approach is two-fold: One, we're building the products to help people engage with the services that we're offering. And two, we're building a platform that's an enabler. I hope that the platform is something that citizens can use to help solve local issues.", "The Commerce Department's mission is to create conditions for economic growth and opportunity. We want to empower citizens to take this data and build businesses and create more jobs.", "That\u2019s why we want to open as much data as possible and just encourage and engage with people so that they can build great things.", ": So open data is a critical part of your strategy?", ": The more data you have, the more you can shed light on issues. However, you can\u2019t let the data speak for itself because you have to recognize that there is bias in data. If you recognize the bias first, you can try and filter out for it, and if you can't, chuck it and use a different data source.", "It\u2019s important to have a data source that is real, legitimate, and sound so you can find a signal and get meaningful information out of it. It\u2019s helpful if you have a purpose, or a direction, or a question you're asking. Then you can actually say, \"I want to see spending trends. I want to see who's spending X on Y.\" And just do an analysis of this one feature over time.", ": How do you determine the difference between\u00a0good data vs bad data?", ": There is no good or bad since data is a product of the collection process and the people that handle it. It\u2019s more about the people that clean, process, and provide it.", ": So there is a lot of importance in having a reliable group who gathers the data?", ": There is a lot of value in having the people responsible for ETL (Extract, Transform, Load) who can create a data set that is a gold standard. They reduce biases as much as possible, and they minimize errors as much as possible.", "It\u2019s important that they're honest with the upstream consumer about the problems with any data sets they provide. If you're really honest about it, then somebody else can know what are the right techniques to use on the data. Otherwise people might just use it willy-nilly and not know that it shouldn\u2019t be used for that purpose or in a particular way.", "So the good and bad thing, there is no dichotomy, it's all data and the interpretation of it.", ": Do you have any advice to people who are looking to get into open data or data security in this industry?", ": I'd say just go in with a problem or a question, something that's burning in your heart that you want to solve. And then figure out what data sets you can use.", "You have a backpack of methods and technologies available and it all starts with the question or problem you're fundamentally trying to solve. You need to understand the user, the problem, and the context in which you have to deliver something.", "That determines what tools you need to actually use to solve that problem, not the other way around. Don\u2019t approach this with, \u201cI have a hammer, I'm going to smash everything with it.\u201d", " ", "Learn more about\u00a0how ", "\u00a0in your business and take a look at how anyone can ", ", no coding required."]},
{"tite": "How to Build your own Price Monitoring Tool", "date": "November 24, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Computers are great at repetitive tasks. They don't get distracted, bored, or tired. ", " is how you should be approaching tedious tasks that are absolutely essential to becoming a successful business or when carrying out ", ". Price monitoring, for example, is a practice that every company should be doing, and is a task that readily lends itself to automation.", "In this tutorial, I\u2019ll walk you through how to create your very own ", " and monitoring tool from scratch. While I\u2019m approaching this as a careful shopper who wants to make sure I\u2019m getting the best price for a specific product, you could develop a similar tool to ", " using similar\u00a0methods.", "Price monitoring is basically knowing how your competitors price their products, how your prices fit within your industry, and whether there are any fluctuations that you can take advantage of.", "When it comes to mission critical tasks like price monitoring, it\u2019s important to ensure accuracy, obtain up-to-date information, and have the capacity for massive scale. By pricing your products perfectly, you can make sure that your competitors aren\u2019t undercutting you, which makes you more likely to nab customers.", "In our article on ", ", Max Robinson, owner of ", ", shared his thoughts on the importance of price monitoring:", "\u201cBut it occurred to me that if you aren\u2019t offering competitive prices, then you\u2019re essentially throwing money down the drain. Even if you have good visibility, users will look elsewhere to buy once they\u2019ve seen your prices.\u201d", "And that\u2019s part of why automation is so important. You don\u2019t want to miss sudden sales or deals from competitors that might make your offerings less desirable.", "In terms of using price monitoring as a consumer, the key is to be able to take advantage of rapid price drops so you can buy during lightning sales. For this tutorial, I used ", ", our open source web scraping\u00a0framework, and ", ", our fully-featured production environment (there\u2019s a ", " account option). Here is the basic outline of my approach:", "I monitored prices from a couple online retailers. To scrape the prices, I built one Scrapy spider for each of these. The spiders work by:", "Here is a sample\u00a0JSON file with product URLs:", "If you want to monitor more retailers than the three I implemented, all you need to do is add their URLs to the JSON file and then create the requisite Scrapy spider for each website.", "If you are new to the world of Scrapy and web scraping, then I suggest that you check out this ", ". When building a spider, you need to pay attention to the layout of each retailer\u2019s product page. For most of these stores, the spider code will be really straightforward, containing only the extraction logic using CSS selectors. In this case, the URLs are read during the spider's startup.", "Here's an example\u00a0spider for Best Buy:", " contains the logic to read the URLs from the JSON file and generate requests. In addition to the spiders, I created ", " to store product data in a Scrapy Cloud collection. You can check out the other spiders that I built in the ", ".", "Now that the spiders have been built, you should start getting product prices that are then stored in a collection. To monitor price fluctuations, the next step is to build a Python script that will pull data from that collection, check if the most recent prices are the lowest in a given time span, and then send an email alert when it finds a good deal.", "Here is my model email notification that is sent out when there's a price drop:", "You can find ", " in the project repository. As you might have noticed, there are customizable options via command line arguments. You can:", "Now that you have the spider(s) and the script, you need to deploy both to ", ", our PaaS for web crawlers.", "I scheduled my spiders to collect prices every 30 minutes and the script to check this data at 30 minute intervals as well. You can configure this through your Scrapy Cloud dashboard, easily changing the periodicity depending on your needs.", "Check out ", " to learn how to deploy Scrapy spiders and ", " on how to run a regular Python script on Scrapy Cloud.", "This price monitor is a good fit for individuals interested in getting the best deals for their wishlist. However, if you\u2019re looking to scale up and create a reliable tool for monitoring competitors, here are some typical challenges that you will face:", "If you\u2019re curious about how to implement or develop an automated price monitoring tool, feel free to reach out with any questions.", "To sum up, there\u2019s no reason why you should be manually searching for prices and monitoring competitors. Using ", ", ", ", a Python script, and just a little bit of programming know-how, you can easily get your holiday shopping done under budget with deals delivered straight to your inbox.", "If you\u2019re looking for a professional-grade competitor and price monitoring service, ", "!"]},
{"tite": "Looking Back at 2016", "date": "January 01, 2017 ", "author": "Cecilia Haynes", "blog_data": ["We started 2016 with an eye on blowing 2015 out of the water. Mission accomplished.", "Together with our users, we crawled more in 2016 than the rest of Scrapinghub\u2019s history combined: a whopping 43.7 billion web pages, resulting in 70.3 billion scraped records! Great work everyone!", "In the what follows, we\u2019ll give you a whirlwind tour of what we\u2019ve been up to in 2016, along with a quick peek at what you can expect in 2017.", "It\u2019s been a\u00a0great\u00a0year for Scrapy Cloud as we wrap up with a\u00a0massive growth in\u00a0our yearly ", ":", "We proudly announced our biggest platform upgrade to date this year with the launch of ", ". Alongside technical improvements like ", " and ", ", this upgrade introduced an improved pricing model that is both less expensive for you while allowing you to better customize Scrapy Cloud based on your resource needs.", "As we move into 2017, we will continue to focus on expanding the technical capabilities of our platform such as:", "Heads up, Scrapy Cloud is in for a massive change this year, so stay tuned!", " is our other flagship product and it basically helps your crawls continue uninterrupted. We were thrilled to launch our ", " this year, which gives you the ability to visualize how you are using the product, to examine what sites and specific URLs you are targeting, and to manage multiple accounts.", "The main goal of ", " is to lower the barrier of entry to web data extraction and to increase the democratization of data (it\u2019s open source!).", "Portia got a lot of love this year with the beta\u00a0release of Portia 2.0. This 2.0 update includes new features like simple extraction of repeated data, loading start urls from a feed, the option to ", ", and the use of CSS selectors to extract specific data.", "Next year we're going to be bringing a host of new features that will make Portia an even more valuable tool for developers and non-developers alike.", "While we had been engaging in data science activities since our earliest days, 2016 saw us formalize a Data Science team proper.\u00a0We\u2019re continuing to push the envelope for machine learning data extraction, so get pumped for some really exciting developments in 2017!", "Scrapinghub has been as committed to open source as ever in 2016. Running Scrapinghub relies on a lot of open source software so we do our best to pay it forward by providing high quality and useful software to the world. Since nearly 40 open source projects maintained by Scrapinghub staff saw new releases this year, we\u2019ll just give you the key highlights.", "Scrapy is the most well-known project that we maintain and 2016 saw our ", " (version 1.1) back in May (running it on Windows is still a challenge, we know, but bear with us). Scrapy is now the 11th most starred Python project on GitHub! 2017 should see it get many new features to keep it the best tool you have (we think) to tackle any web scraping project, so keep sending it some ", " and feature requests!", "Splash, our headless browser with an HTTP interface, hit ", " a few weeks ago with the addition of the long-awaited web scraping helpers: CSS selectors, form filling, interacting with DOM nodes\u2026 This 2.3 release came after a steady series of improvements and a successful Google Summer of Code (GSoC) project this summer by our student ", ".", "Our \u201clittle\u201d library to help with dates in natural language got a bit of attention on GitHub when\u00a0it was picked up as a dependency for ", ". We\u2019re quite proud of this little library :). Keep the bug reports coming if you find any, and if you can, ", "\u00a0even more languages.", "Frontera is the framework we built to allow you to implement distributed crawlers in Python. It provides scaling primitives and ", ". 2016 brought us 11 releases including support for Python 3! A huge thank you to ", ", one of our GSoC students, who helped us to improve test coverage and made sure that all of the parts of Frontera support Python 3.", "As in 2014 and 2015, Scrapinghub participated in ", " under the ", " umbrella. We had four students complete their projects and two of them got their contribution merged into the respective code base (see the \u201cFrontera\u201d and \u201cSplash\u201d sections above). ", " was related to Scrapy performance improvements and is close to being integrated. The last one is ", " to use Scrapy with other programming languages. To our students, ", ", ", ", ", ", and ", ", thank you all very much for your contributions!", "Conferences are always a great opportunity to learn new skills, showcase our projects, and, of course, hang out with our clients, users, and coworkers. As a remote staff, we don't have the opportunity to meet each other in person often, so tech conferences are always a great way to strengthen ties. The traveling Scrapinghubbers thoroughly enjoyed sharing their knowledge and web scraping experiences through presentations, tutorials, and workshops.", "Check out some of the talks that were given this year:", "\n", "Being a fully remote company, we\u2019re thrilled to confirm that we have Scrapinghubbers in almost every continent (we\u2019re *just* missing Antarctica, any Scrapy hackers out there?).", "Aside from the conferences we attended this year, we had a few localized team retreats. The Professional Service managers got together in Bangkok, Thailand; the Sales team had a retreat in Buenos Aires, Argentina, where ", " dominated the show; and the Uruguayan Scrapinghubbers got together for an end-of-year meetup in La Paloma, Uruguay, hosted by ", ".", "Currently our Crawlera team is having a meetup in Poznan, Poland, paving the way for what will become the next version of our flagship ", ".", "And that\u2019s it for 2016! From the whole team at Scrapinghub, I\u2019d like to wish you happy holidays and the best wishes for the start of 2017. We\u2019ve got a lot of exciting events lined up next year, so get ready!"]},
{"tite": "Scraping the Steam Game Store with Scrapy", "date": "July 07, 2017 ", "author": "Andre Perunicic", "blog_data": ["This is a guest post from the folks over at\u00a0", ", one of the awesome\u00a0", " and longtime Scrapy fans.", "The ", " is home to more than ten thousand games and just shy of four million user-submitted reviews. While all kinds of Steam data are available either through official APIs or other bulk-downloadable data dumps, I could not find a way to download the full review dataset. ", " If you want to perform your own analysis of Steam reviews, you therefore have to extract them yourself.", "Doing so can be tricky if scraping is not your primary concern, however. Here's what some of the fields we are interested in look like on the page.", "Even for a well-designed and well-documented project like ", " (my favorite Python scraper) there exists a definite gap between ", " and a larger project dealing with realistic pitfalls. My goal in this guide is to help scraping beginners bridge that gap. ", "What follows is a step-by-step guide explaining how to build up the code that's in this repository, but you should be able able to jump directly into a section you're interested in.", "If you are only interested in using the completed scraper, then you can head directly to the ", ".", "Before we jump into the nitty-gritty of scraper construction, here's a quick setup guide for the project. Start with setting up and initiating a virtualenv:", "I decided to go with Python 3.6 but you should be able to follow along with earlier versions of Python with only minor modifications along the way.", "Install the required Python packages:", "and start a new Scrapy project in the current directory with", "Next, configure rate limiting so that your scrapers are well-behaved and don't get banned by generic DDoS protection by adding", "to ", ". You can optionally set ", " to match your browser's configuration. This way the requests coming from your IP have consistent user agent strings.", "Finally, turn on caching. This is always one of the first things I do during development because it enables rapid iteration and spider testing without blasting the servers with repeated requests.", "We will improve this caching setup a bit later.", "We first write a crawler whose purpose is to discover game pages and extract useful metadata from them. Our job is made somewhat easier due to the existence of a complete product listing which can be found by heading to Steam's search page, and ", ".", "This listing is more than 30,000 pages long, so our crawler needs to be able to navigate between them in addition to following any product links. Scrapy has an existing ", " class for exactly this kind of job. The idea is that we can control the spider's behavior by specifying a few simple ", " for which links to parse, and which to follow in order to find more links.", "Every product has a storefront URL ", " determined by its unique Steam ID. Hence, all we have to do is look for URLs matching this pattern. Using Scrapy's ", " class this can be accomplished with", "where the callback parameter indicates which function parses the response, and ", " is the HTML element containing the product links.", "Next, we have to make a rule that navigates between pages applies the rules recursively, since this will keep advancing the page and finding products using the previous rule. In Scrapy this is accomplished by omitting the ", " parameter which by default uses the ", " method of ", ".", "You can now place a skeleton crawler into ", "!", " ", "(", ", response):", " ", "Next, we turn to actually extracting data from crawled product pages, i.e., implementing the ", " method above. Before writing code, explore a few product pages such as ", " to get a better sense of the available data.", "Poking around the HTML a bit, we can see that Steam developers have chosen to make ample use of narrowly-scoped CSS classes and plenty of ", " tags. This makes it particularly easy to target specific bits of data for extraction. Let's start by scraping the game's name and list of \"specs\" such as whether the game is single- or multi-player, whether it has controller support, etc.", "The simplest approach is to use CSS and XPath selectors on the ", " object followed by a call to ", " or ", " to access text or attributes. One of the nice things about Scrapy is the included Scrapy Shell functionality, allowing you to drop into an interactive iPython shell with a response loaded using your project's settings. Let's drop into this console to see how these selectors work.", "We can get the data by examining the HTML and trying out some selectors:", "The corresponding ", " method might look something like:", "As we add more data to the parser, we encounter HTML that needs to be cleaned before we get something useful. For example, one way to get the number of submitted reviews is to extract all review counters (there are multiple ones on the page sometimes) and get the max. The HTML is of the form", "which we use to write", "This is a pretty mild example, but such mixing of data selection and cleaning can lead to messy code that is annoying to revisit. That is fine in small projects, but I chose to separate selection of interesting data from the page and its subsequent cleaning and normalization with the help of ", " and ", " abstractions.", "The concept is simple: An ", " is a specification of the fields your parser produces. A trivial example would be something like:", "To make this useful, we make a corresponding ", " that is in charge of collecting and cleaning data on the page and passing it to the ", " for storage.", "An ", " collects data corresponding to a given field into an array and processes each extracted element as it's being added with an \"input processor\" method. The array of extracted items is then passed through an \"output processor\" and saved into the corresponding field. For instance, an output processor might concatenate all the entries into a single string or filter incoming items using some criterion.", "Let's look at how this is handled in ", ". Expand the above ", " in ", " with some nontrivial output processors", "and add a corresponding ", "These data processors can be defined within an ", ", where they sit more naturally perhaps, but writing them into an ", "'s field declarations saves us some unnecessary typing.", " To actually extract the data, we integrate these two into the parser with", " ", "(", ", response):", " loader ", " ProductItemLoader(item", "ProductItem(), response", "response)", "loader", "add_css(", ", ", ") ", " loader", "add_css(", ", ", ")", " loader", "add_css(", ", ", ",", " re", ") ", " loader", "load_item()", "Let's step through this piece by piece. Each number here corresponds to the ", " annotation in the code above.", " - A basic field that saves its data using the ", " defined at ", ". In this case, we just take the first extracted item.", " - Here we connect the ", " field to an actual selector with ", ".", " - A field with a customized output processor. ", " is one of a few processors included with Scrapy in ", ", and it applies its arguments to each item in the array of extracted data.", " and ", " - Arguments passed to ", " are just callables, so can be defined however you wish. Here I defined a simple string to integer converter with error handling built-in", "and a text-stripping utility in which you can customize the characters being stripped (which is why it's a class)", " - Notice that ", " can also extract regular expressions, again into a list!", "This may seem like overkill, and in this example it is, but when you have a few dozen selectors, each of which needs to be cleaned and processed in a similar way, using ", "s avoids repetitive code and associated mistakes. In addition, ", "s are easy to integrate with custom ", ", which are simple extensions for saving data streams. In this project we will be outputting line-by-line JSON (", ") streams into files or Amazon S3 buckets, both of which are already implemented in Scrapy, so there's no need to make a custom pipeline. An item pipeline could for instance save incoming data directly into an SQL database via a Python ORM like ", " or ", ".", "You can see a more comprehensive product item pipeline in the ", " file of ", ". Before doing a final crawl of the data it's generally a good idea to test things out with a small depth limit and prototype with caching enabled. Make sure that ", " is enabled in the settings, and do a test run with", "Here's a small excerpt of what the output of this command looks like:", "Exploring this output and cross checking with the Steam store reveals a few potential issues we haven't addressed yet.", "First, there is a 302 redirect that forwards us to a mature content checkpoint that needs to be addressed before Steam will allow us to see the corresponding product listing.", "Second, URLs include a mysterious ", " query string parameter that doesn't have a meaningful effect on page content.", "Although the parameter doesn't seem to be varying too much within a short time span, it could lead to duplicated entries.", "Not the end of the world, but it would be nice to take care of this before proceeding with the crawl.", "In order to avoid scraping the same URL multiple times Scrapy uses a ", ". It works by standardizing the request and comparing it to an in-memory cache of standardized requests to see if it's already been processed.", "Since URLs which differ only by the value of the ", " parameter point to the same resource we want to ignore it when determining which URL to follow.", "So, how is this done?", "The solution is representative of the way I like to deal with custom behavior in Scrapy: read its source code, then overload a class or two with the minimal amount of code necessary to get the job done. Scrapy's source code is pretty readable, so it's easy to learn how a core component functions as long as you are familiar with the ", ".", "For our purposes we look through ", " in ", " and conclude that all we have to do is overload its ", " method", "and point the change out to Scrapy in our project's ", "In the repository, I also update the default caching policy by overloading the _redirect method of RedirectMiddleware from scrapy.downloadermiddlewares.redirect, but you should be fine without doing so.", "Next, we figure out how to deal with mature content redirects.", "Steam actually has two types of checkpoint redirects, both for the purposes of verifying the user's age before allowing access to a product page with some kind of mature content. There is another redirect, appending the product's name to the end of the URL, but it's immaterial for our purposes.", "The first is a simple age input form, asking the user to explicitly input their age. For example, trying to access ", " redirects the user to ", ".", "Submitting the form with a birthday sufficiently far back allows the user to access the desired resource, so all we have to do is instruct Scrapy to submit the form when this happens.", "Checking out the age form's HTML reveals all the input fields whose values are submitted through the form, so we simply replicate them every time we detect the right pattern in our response URL:", "The other type of redirect is a mature content checkpoint that requires the user to press a \"Continue\" button before showing the actual product page. Here's one example: ", ".", "Note that this checkpoint's URL is different than in the previous case, letting us easily distinguish between them from a spider.", "By looking at the HTML, you can see that the mechanism by which access is granted to the product page is also different than last time.", "Hitting \"Continue\" triggers a ", " JavaScript function that sets a ", " cookie and updates the address, inducing a page reload with the new cookie present.", "The routine is hard-coded on the page and the parts we care about resemble the following.", "This suggests that including a ", " cookie with a request is sufficient to pass the checkpoint, and easily verified with", "So, all we have to do is include that cookie with requests to mature content restricted pages.", "Luckily, Scrapy has a redirect middleware which can intercept redirect requests and modify them on the fly. As usual, we observe the source and notice that the only method we need to change is ", " (and only slightly).", "In ", ", add the following", "We could have alternatively added a ", " cookie to all requests by modifying the ", ", or just passed it into the initial request. Note also that overloading redirects like this is the first step in handling captchas and more complex gateways, as explained in this ", " by Intoli's own Evan Sangaline.", "This basically covers the crawler and all gotchas encountered... all that's left to do is run it. The run command is similar to the one given above, except that we want to remove the depth limit, disable caching, and perhaps keep a job file in order to enable resuming in case of an interruption (the job takes a few hours):", "When the crawl completes, we will hopefully have metadata for all games on Steam in the output file ", " . Example output from the completed crawler that is available in the accompanying repository looks like this:", "Note that the output contains a manually created ", " field. We will use it in the next section to generate a list of starting URLs for the review scraper.", "Since product pages display only a few select reviews we need to scrape them from the Steam Community portal which contains the whole dataset.", "Addresses on Steam Community are determined by Steam product IDs, so we can easily generate a list of URLs to process from the output file generated by the product crawl:", "Due to the size of the dataset you'll probably want to split up the whole list into several text files, and have the review spider accept the file through the command line:", " ", "(", "):", " ", " ", "(", "url_file, ", ") ", " f:", " ", " url ", " f:", " ", " scrapy", "Request(url, callback", "parse)", " ", "(", ", response):", " ", "Reviews on each page are loaded dynamically as the user scrolls down using an \"infinite scroll\" design. We need to figure out how this is implemented in order to scrape data from the page, so pull up Chrome's console and monitor XHR traffic while scrolling.", "The reviews are returned as pre-rendered HTML ready to be injected into the page by JavaScript. At the bottom of each HTML response is a form named ", " whose purpose is clearly to get the next page of reviews. You can therefore repeatedly submit the form and scrape the response until reviews run out:", "Here ", " returns a loaded item populated as before, and ", " parses the form and returns a ", ".", "And that's basically it! All that's left is to run the crawl. Since the job is so large, you should probably split up the URLs between a few text files and run each on a separate box with a command like the following:", "The completed crawler, which you can find in the accompanying repo, produces entries like this:", "I hope you enjoyed this relatively detailed guide to getting started with Scrapy. Even if you are not directly interested in the Steam review dataset, we've covered more than just how to make selectors and developed practical solutions to a number of common scenarios such as redirect and infinite scroll scraping.", "Leave a comment on this ", "."]},
{"tite": "How to Increase Sales with Online Reputation Management", "date": "December 15, 2016 ", "author": "Cecilia Haynes", "blog_data": ["One negative review can cost your business up to 22% of its prospects. This was one of the sobering findings in ", " last year. With over half of shoppers rating reviews as important in their buying decision, no company large or small can afford to ignore stats like these - let alone the reviews themselves. In what follows I'll let you in on ", " you stay on top.", "Online reputation management is carefully maintaining and curating your brand\u2019s image by monitoring social media, reviews, and articles about your company. When it comes to online reputation management, you can\u2019t have too much information. This is a critical part of your business strategy that impacts pretty much every level of your organization from customer service to marketing to sales. ", " found that, \u201c84% of people trust online reviews as much as a personal recommendation.\u201d The relationship between brands and customers has become a two-way street because of the multitude of channels for interaction. Hence the rise of ", " and guerilla marketing tactics.", "A key part of online reputation management is highlighting positive reviews to send the message that you are a responsive company that rewards loyal and happy customers. Online reputation management is likewise critical to putting out any potential customer fires. The attrition rate of consumers ", " when they stumble across four or more negative articles. You need to be able to act fast to address criticisms and to mitigate escalating issues. Ideally you should not delete negative feedback, but instead show the steps that you are taking to rectify the situation. Besides sparing you an occasional taste of the ", ", this shows that you are responsible, transparent, and ", ".", "While you could manually monitor social media and review aggregators, in addition to Googling your company for unexpected articles, it\u2019s much more effective to automate the process. There are a lot of different companies and services that specialize in this service including:", "If you want complete control over your data and the type of information that you\u2019d like to monitor, ", " is the most comprehensive and ", ".", "There is an inconceivably vast amount of content on the web which was built for human consumption. However, its unstructured nature presents an obstacle for software. So the general idea behind web scraping is to turn this unstructured web content into a structured format for easy analysis.", " smooths the tedious manual aspect of research and allows you to focus on finding actionable insights and implementing them. And this is especially critical when it comes to online reputation management. Respondents to ", " study showed that when customers contact companies through social media for customer support issues, 32% expect a response within 30 minutes and 42% expect a response within 60 minutes. Using web scraping, you could easily have constantly updating data feeds that alert you to comments, help queries, and complaints about your brand on any website, allowing you to take instant action.", "You also need to be sure that nothing falls through the cracks. You can easily monitor thousands, if not millions of websites for changes and updates that will impact your company.", "Now, a key part of online reputation management is monitoring reviews for positive and negative feedback. Once the extracted web data is in, you can use machine learning to do sentiment analysis. This form of textual analysis can categorize messages as positive or negative, and the more data you use to train the program, the more effective it becomes. This is a great method for being able to quickly respond to negative reviews while keeping track of positive reviews to reward customers and highlight loyalty.", "Here are two entrepreneurs providing real world examples of how they use online reputation management and review monitoring to increase their business.", "Kent Lewis", " President and Founder of Anvil Media, Inc.", " ", "As a career agency professional who has owned my own agency for the past 16 years, I have a few thoughts regarding monitoring reviews and assessing sentiment analysis to move businesses forward:", "Monitoring reviews (including sentiment) is essential to your business. Ignoring (negative) reviews can cause undue and unnecessary harm. Since 90% of customers read online reviews before visiting a business, negative reviews can directly affect sales. Conversely, a one-star increase on Yelp, leads to a 5-9% increase in business revenue.", "Online reviews can be monitored manually (bookmarking and visiting sites like Google My Business, Yelp and others daily or as-needed). However, there are a host of tools available that automate the process. Utilize a mix of free (socialmention.com) and paid tools (Revinate.com) to regularly monitor reviews, in order to address negative reviews and celebrate positive reviews.", "While the primary objective for monitoring reviews is identifying and mitigating negative reviews, there are a host of other benefits to capturing and analyzing the data. Harvesting and analyzing the data will provide insights that will improve your products and services. For starters, you can measure and trend sentiment for the brand overall. With additional insights, you can track and monitor reviews for specific products, services or locations. Social media and review sites are the largest (free) focus group in the world. Additionally, you can look at competitors and create benchmarks to track trends over time. Lastly, you can identify superfans that can be nurtured into brand ambassadors.", "The sources of data vary by company and industry. Most businesses can be reviewed on Google My Business, Yelp, BBB and Glassdoor (for employees). Each industry has specific sites that also must be monitored, including Expedia and Travelocity for travel & hospitality.", "To get maximum value from your monitoring efforts, always look at competitor reviews. Customers are telling you what business you should be in based on their feedback and suggestions for improvement... learn from the entire industry, not just your current or past customers.", "Max Robinson", "WeSwap Euros", "We use tools like Sprout Social which helps us to track mentions on social media for our clients, as this where the majority of the discussion happens about their business. The main reason that our clients want to track these mentions is that people tend to speak more openly and honestly about their experiences with a business on social media than anywhere else online. This also gives our clients the chance to join in conversations with their customers in a casual manner, whereas interactions on review sites can be far more formal.", "We report on the number of mentions, and whether our client is being discussed in a positive or negative manner, as well as what the discussion is specifically related to. We look at 3 main social media platforms - Facebook, Twitter and Reddit. We also monitor mentions of competitors across all of these platforms, as per the request of our clients.", "Do not neglect your competitors when monitoring reviews and social media. Keeping track of the online reputation of competitors allows you to:", "And that\u2019s just the ", ". Competitive intelligence and having an accurate overview of your industry only serves to help you sell your products more effectively. And to bring it back to online reputation management, having a negative perception of your brand is like shooting yourself in the foot. You're already at a severe disadvantage, especially when compared to positively reviewed competitors.", "In an ", ", president of Big Blue Robot, he shared that one company he worked with was losing an estimated $2 million and more in sales due to a poor online reputation. Don't let this be you.", "If you are proactive and have a positive reputation or have managed to repair your reputation, then enthusiastic reviews and word of mouth will increase and improve your lead generation prospects. Your sales team should also be fully aware of your online reputation so they can soothe potential concerns or draw attention to success stories.", "They say that a good reputation is more valuable than money. Guard yours closely with web data and ensure that you are taking every precaution necessary to retain customers and win over new leads.", " or ", " to learn more."]},
{"tite": "Deploy your Scrapy Spiders from GitHub", "date": "April 19, 2017 ", "author": "Valdir Stumm Jr", "blog_data": ["Up until now, your deployment process using Scrapy Cloud has probably been something like this: code and test your spiders locally, commit and push your changes to a GitHub repository, and ", " deploy them to Scrapy Cloud using ", ". However, having the development and the deployment processes in isolated steps might bring you some issues, such as unversioned and outdated code running in production.", "The good news is that, from now on, you can have your code automatically deployed to Scrapy Cloud whenever you push changes to a GitHub repository. All you have to do is connect your Scrapy Cloud project with a repository branch and ", "!", "Scrapy Cloud\u2019s new GitHub integration will help you ensure that your code repository and your deployment environments are always in sync, getting rid of the error-prone manual deployment process and also speeding up the development cycle.", "Check out how to setup automatic deploys in your projects:", "\u00a0", "\u00a0", "If you are not that into videos, have a look at ", ".", "You could use this feature to set up a multi-stage deploy workflow integrated with your repository. Let's say you have a repo called ", ", with three main branches -- ", ", ", " and ", " -- and you need one deployment environment for each one.", "You create one Scrapy Cloud project for each branch:", "And\u00a0connect each of these projects with a specific branch from your ", " repository, as shown below for the development one:", "Then, every time you push changes to one of these branches, the code is automatically deployed to the proper environment.", "If you have any feedback regarding this feature or the whole platform, ", ".", "Start deploying your Scrapy spiders from Github now."]},
{"tite": "Do Androids Dream of Electric Sheep?", "date": "June 19, 2017 ", "author": "Mikhail Korobov", "blog_data": ["It got very easy to do Machine Learning: you install a ML library like ", " or ", ", choose an estimator, feed it some training data, and get a model which can be used for predictions.", "Ok, but what's next? How would you know if it works well? Cross-validation! Good! How would you know that you haven't messed up the cross validation? Are there data leaks? If the quality is not good enough, how to improve it? Are there data preprocessing errors or other software bugs? ML systems are notably good at hiding bugs - they can adapt, so often in case of bug there is a small quality drop, but the system as a whole still works. Should we put the model to production? Can the model be trusted to do reasonable things? Are there pitfalls? How to convince others the system works in a reasonable ways?", "There is no silver bullet; I don't know a true answer for these questions. But understanding of how the model \"thinks\" - what its decisions are based on - should be a big part of the answer.", "AI-powered robots haven't conquered us (yet), so let's start with a 19th century Machine Learning method: Linear Regression.", "As a concrete example, let's say we want to estimate pizza price using Linear Regression. We think that pizza radius, a number of salami slices and a number of tomato slices could affect the price (i.e. we've defined 3 features: radius, salami count, tomato count). So we walk around our XIX century town, visit every pizzeria, order a coffee and take notes of pizzas being sold: price, radius, salami, tomato. After a few gallons of coffee we can derive a formula, based on the notes:", "Coefficients 1.5, 0.4 and 0.1 are selected such as that price is not too off for pizzas we've seen. What we did is a Linear Regression: result is computed as", "$latex y = w_0 x_0 + w_1 x_1 + ... + w_n x_n &s=1$", "- a weighted sum of inputs. $latex w_0, w_1, ..., w_n&s=1 $ are regression parameters (weights, coefficients) which we adjust based on training data; $latex x_0, x_1, ..., x_n&s=1$ are input variables (e.g. pizza radius or a number of salami pieces). Formula can be also written in a vector form: $latex y = x^T w &s=1$", "Most people agree that $latex price = 1.5 \\times radius + 0.4 \\times salami + 0.1 \\times tomato&s=1$ is pretty understandable. Looking at coefficients of a Linear Regression can be enough for humans to see what's going on. For example, we can see that in some sense salami is more important than tomatoes - salami count is multiplied by 0.4, while tomato count is multiplied only by 0.1; this can be an useful information. We can also see how much adding a piece of salami or increasing radius by 1cm affects the price.", "There are caveats though. If scales of features are not the same then comparing coefficients can be misleading - maybe there are 25 tomato slices on average, and only 2 salami slices on average (weird 19th century pizzas!), and so tomatoes contribute much more to the price than salami, despite their coefficient being lower. It is also obvious that radius and salami coefficients can't be compared directly. Another caveat is that if features are not independent (e.g. there is always an extra tomato per salami slice), interpreting coefficents gets trickier.", "One more observation is that to explain the behavior we didn't care how to train the model (how we came up with radius/salami/tomato coefficients), we only needed to know the final formula (algorithm) used at prediction time. It means that we can look at Ridge or Lasso or Elastic Net regression the same way, as they are the same at prediction time.", "That said, understanding of the training process can be important for understanding behavior of the system. For example, if two features are correlated, Ridge regression tend to keep both features, but set lower coefficients for them, while Lasso may eliminate one of the features (set its weight to zero) and use a high coefficient for the remaining feature. It means that e.g. in Ridge regression you're more likely to miss an important feature if you look at top coefficients, and in Lasso a feature can get a zero weight even if it is almost as important as the top feature.", "So, there are two lessons. First, looking at coefficients is still helpful, at least as a sanity check. Second, it is good to understand what you're looking at, because there are caveats.", "It is no longer 19th century: we don't have to walk around beautiful Italian towns, drink coffee and eat pizzas to collect a dataset, we can now go to the Internet. Likewise, for Linear Regression we can use libraries like numpy or scikit-learn instead of visiting a library, armed with quill and paper.", "Let's apply Linear Regression to an example task: predict house pricing based on attributes like town crime rate, pollution, house location, etc. We'll use \"Boston Housing\" ", "; it is available in scikit-learn:", "Internally LinearRegression produces a formula similar to our pizza's formula. To check its coefficients we can look at ", " attribute:", "Linear regression is supposed to be readable, but result above is not; what is unclear is which coefficient corresponds to which feature. So we need to combine these coefficients with feature names; this is easy:", "The result is still scary, but at least we can check if a feature contributes positively or negatively to a price. For example, CRIM (crime level) is a negative factor, while CHAS (if a river is nearby) is a positive factor. It is not possible to compare coefficients directly because scales of features are different; we may normalize data to make scales comparable using e.g. preprocessing ", " from scikit-learn - try it yourselves.", "To make inspecting coefficients easier we created ", " Python library. It can do much more than that, but it started from a snippet similar to a snippet above, which we were copy-pasting across projects.", "\n", "\n", "It shows the same coefficients, but there is also a \"<BIAS>\" feature. We forgot about it when writing the ``get_formula`` snippet: LinearRegression by default creates a feature which is 1 for all examples (it is called \"bias\" or \"intercept\"); its weight is in ``reg.intercept_`` atribute.", "``eli5`` knows where to get coefficients from for a wide range of ML models from several ML frameworks. It provides utilities for common tasks (e.g. you can check only top features or filter them by name), can output to IPython, html, JSON or plain text. ELI5 can also remind you about caveats of the interpretation method - for example, we can get this for our Linear Regression:", "\n", "\n", "So, the lesson here is that machine learning libraries like scikit-learn expose coefficients of trained ML models; it is possible to inspect them, and eli5 library makes it easier.", "Let's say we know nothing about Machine Learning, and want to classifiy documents into several classes - for example, as documents about computer graphics or documents about medicine. If you give this task to someone smart without any ML experience, he/she may propose to solve it this way:", "We can write it this way: $latex y = computer + graphics + photoshop - kidney - treatment - pill$; if $latex y > 0 &s=1$ then text is about computer graphics; if it is less than zero then we have a medical document.", "A smart person may also notice that some of keywords can be more important than others - if there is 'photoshop' in text then the text is very likely to be about CG (computer graphics), but a word 'pen' can be only a small indicator. So to improve the quality one can assign each word a weight, e.g.:", "$latex y = 1.0 \\times computer + 1.5 \\times graphics + 2.0 \\times photoshop - 5.0 \\times kidney - 0.5 \\times treatment - 0.5 \\times pill &s=1$", "Many smart people are lazy, so they likely won't be fond of idea of adjusting all these coefficients by hand. A better idea could be to take documents about CG and documents about medicine, then write a program to find best coefficients automatically.", "It already starts to look suspiciously similar to pizza's Linear Regression formula, isn't it? The difference is that we are not interested in ``y`` value per se, we want to know if it is greater than zero or not.", "Such \"y\" function is often called a \"decision function\": we compute \"y\", check if it is greater or less than zero, and make a yes/no decision. And in fact, this is a very common approach: for example, at prediction time linear Support Vector Machine (SVM) works exactly as our \"y\" function. Congratulations - now you know how linear SVMs work at prediction time! If you look at coefficients of a linear SVM classifier applied for text classification using \"bag-of-words\" features (similar to what we've done), then you'll be looking at the same weights as in our example. There is a weight (coefficient) per word, to do prediction linear SVM computes weighted sum of tokens present in a document, just like our \"y\" function, and then the result is compared to 0.", "We may also notice that a larger \"y\" (positive or negative) means that we're cetrain a document is about CG or about medicine (it has more relevant keywords), while ``y`` close to zero means we either don't have enough information, or keywords cancel each other.", "Let's say we calculated \"y\" and got \"2.5\" value. What does $latex y=-2.5&s=1$ mean? To make \"y\" value easier to interpret it'd be nice for it to be in range from 0 to 1 - this way we can think about it as of a probability. For example, when keywords sum is a very large negative number, \"y\" could be close to 0 (a probaility of a document bein a CG document is close to 0), when there is no information \"y\" could be 0.5, and when sum is a large positive number \"y\" could be close to 1.0.", "To implement this idea one can use a function which transforms original, unbounded scores, to (0, 1) range: $latex y = f(1.0 \\times computer + ... - 5.0 \\times kidney - ...)&s=1$", "So we need a function which takes a value in arbitrary range, and returns a number from 0 to 1. There are many options, but if we take \"Logistic function\" as such function", "$latex f = \\frac{1}{1+e^{-x}}&s=3$", "then we get a Machine Learning model called Logistic Regression. Congratulations - you now know how Logistic Regression works at prediction time!", "Note that at prediction time Logistic Regression and Linear SVM do exactly the same if you only need yes/no labels and don't need probabilities. But they still differ in how weights are selected during the training, i.e. for the same training data you'll get different weights (and so different predictions). Logistic Regression chooses best weights for good probability approximation, while Linear SVM chooses weights such as that decisions are separated by a larger margin; it is common to get a tiny bit higher yes/no accuracy from a linear SVM, but linear SVMs as-is won't give you a probability score.", "Now as you know how Logistic Regression and linear SVMs work and what their coefficients mean, it is time to apply them to a text classification task and check how they are making their predictions. These simple linear models are surprisingly strong baselines for text classification, and they are easy to inspect and debug; if you have a text classification problem it is a good idea to try text-based features and a linear model first, even if you want to go fancy later.", "Scikit-Learn docs have a great ", " on text processing using bag-of-words features and simple ML models. The task in the tutorial is almost the same in our example: classify a text message as a message about computer graphics, medicine, atheism or Christianity. This tutorial uses 4 possible classes, not two. We only discussed how to classify a text document into two classes (CG vs medicine), but don't worry.", "A common way to do multi-class classification (and the way which is used by default in most of scikit-learn) is to train a separate 2-class classifier per each class. So under the hood there will be 4 classifers: CG / not CG, medicine / not medicine, atheism / not atheism, Christianity / not Christianity. Then, at prediction time, all four classifiers are employed; to get a final answer highest-scoring prediction among all classifiers is used.", "It means that instead of inspecting a single classifier we'll be inspecting 4 different classifiers which work together to get us an answer.", "First, let's load the data, as in the tutorial:", "The final model showed in the tutorial is a linear SVM trained on TF*IDF bag-of-words features using SGD training algorithm. We already know how a linear SVM works at prediction time, and we don't care about training algorithm.", "TF*IDF bag-of-words features are very similar to \"bag-of-words\" features we used before - there is still a coefficient per word. The difference is that instead of counting words or simply checking if a word is in a document, a more complex approach is used: words counts are now normalized according to document length, and the result is downscaled for words that occur in many documents (very common words like \"he\" or \"to\" are likely to be irrelevant).", "The quality of this simple pipeline is quite good (0.913 accuracy). But let's check how this classifier works internally, what coefficients it learned:", "\n", "\n", "Here we have much more parameters than in previous examples - a parameter per word per class; there are 4 classes and 20K+ words, so looking at all parameters isn't feasible. Instead of displaying everything eli5 shows only parameters with largest absolute values - these parameters are usually more important (of course, there are caveats).", "We can see that a lot of these words make sense - \"atheism\" is a relevant word for atheism-related messages, \"doctor\" is a good indicator that a text is a medical text, etc. But, at the same time, some of the words are surprising: why do \"keith\" and \"mathew\" indicate a text about atheism, and \"pitt\" indicates a medical text? It doesn't sound right, something is going on here.", "Let's find this mysterious Mathew in the training data:", "From: mathew", " Subject: Re: ( I am almost sure that Zyklon-B is immediate and painless method of", " > death. If not, insert soem other form. )", " >", " > And, ethnic and minority groups have been killed, mutilated and", " > exterminated through out history, so I guess it was not unusual.", " >", " > So, you would agree that the holocost would be allowed under the US", " > Constitution? [ in so far, the punishment. I doubt they recieved what would", " > be considered a \"fair\" trial by US standards.", "Don't be so sure. Look what happened to Japanese citizens in the US during", " World War II. If you're prepared to say \"Let's round these people up and", " stick them in a concentration camp without trial\", it's only a short step to", " gassing them without trial. After all, it seems that the Nazis originally", " only intended to imprison the Jews; the Final Solution was dreamt up partly", " because they couldn't afford to run the camps because of the devastation", " caused by Goering's Total War. Those who weren't gassed generally died of", " malnutrition or disease.", "mathew", "Aha, we have messages as training examples, and some guy named Mathew wrote some of them. His name is in the message header (From: mathew...), and in the message footer. So instead of focusing on message content, our ML system found an easier way to classify messages: just remember person names and email addresses of notable message authors. It may depend on a task, but most likely this is not what we wanted model to learn. Likely we wanted to classify message content, not message authors.", "It also means that likely our accuracy scores are too optimistic. There are messages mentioning Mathew both in training and testing part, so the model can use message author name to get score points. A model which thinks \"Oh, this is my old good friend Mathew! He only talks about atheism, I don't care much about what he's saying\" can still get some accuracy points, even if it does nothing useful for us.", "A lesson learned: by inspecting model parameters sometimes it is possible to check if the model is solving the same problem as we think.", "It doesn't make sense to try more advanced models or tune parameters of the current model at this point: it looks like there is a problem in task specification, and evaluation setup is also not correct for the task we're solving (assuming we're interested in message texts).", "So it could give us at least two ideas: 1) probably we could get a better train/test split for the data if messages by the same author (or mentioning the same author, e.g. via replying) only appear either in train or in test part, but not in both; 2) to train an useful classifier on this data it could make sense to remove message headers, footers, quoting, email addresses, to make model focus on message content - such model could be more useful on unseen data.", "But does the model really only care about Mathew in the example? Until now, we were checking model coefficients; it allows us to get some general feeling of how the model works. But this method has a downside: it is not obvious why a decision was made on a concrete example.", "A related downside is that coefficients depend on feature scales; if features use different scales we can't compare coefficients directly. While indicator bag-of-word features (1 if a word is in a document and 0 otherwise) use the same scale, with TF*IDF features input values are different for different words. It means that for TF*IDF a coefficient with top weight is not necessarily the most important, as in the input data word weight could be low because of IDF multiplier, and a high coefficient just compensates this.", "We only looked at coefficients for words, but we haven't checked which words are in the document, and what are the values coefficients are multiplied by. Previously we were looking at something like $latex y = 2.0 \\times atheism + 1.9 \\times keith + 1.4 \\times mathew + ...$ (for all possible words), but for a concrete example values of \"mathew\" and \"from\" are known - it could be raw word counts in the document, or 0/1 indicator values, or TF*IDF weighted counts, as in our example, and a list of words is much smaller - for most of the words value is zero.", "ELI5 provides a helper to do that computation; even better, it knows how to work with scikit-learn text processing utilities, so instead of showing a table with contribution values it can show these word contributions by highlight them in text:", "\n", "\n", "Green highlighting means positive contribution, red means negative.", "It seems the classifier still uses words from message text, but names like \"Mathew\", email addresses, etc. look more important for a classifier. So yeah, even without author name classifier likely makes a correct decision for this example, but it focuses mostly on wrong parts of the message.", "Let's try one of the ideas - to make the classifier more useful remove message headers, footers and emails from the training data. We would have to write some code for it, but for this particular dataset something similar is already implemented in scikit-learn:", "After re-training of the original pipeline, accuracy becomes much worse", " (0.796 instead of 0.913). There are two main reasons for that:", "Let's check weights of the updated model:", "\n", "\n", "Preprocessing helped - all (or most) of author names are gone, and feature list makes more sense now.", "Some of the features still look strange though - why is \"of\" the most negative word for computer graphics documents? It doesn't make sense. \"Of\" is just a common word which appears in many documents. Probably, all other things equal, a document is less likely to be a computer graphics document, and the model learned to use a common, \"background\" word \"of\" to encode this information.", "A classical approach for improving text classification quality is to remove \"stop words\" from text - generic words like \"of\", \"it\", \"was\", etc., which shouldn't be specific to a particular topic. The idea is to make it easier for model to learn something useful. In our example we'd like the model to focus more on the topic-specific words and use a special \"bias\" feature instead of relying on these \"background\" words.", "There are stop words lists for many languages; scikit-learn has a list of such words for English built-in:", "There is a TfidfVectorizer argument to use this stop words list; let's try it:", "Nice, the accuracy is improved from 0.796 to 0.819. If we check model weights using \"eli5.show_weights\" we'll see that \"of\" word is no longer in a table. Let's also check it on a concrete example:", "\n", "\n", "Mmm, it looks like many \"background\" words are no longer highlighted, but some of them still are. For example, \"don\" in \"don't\" is green, and \"weren\" in \"weren't\" is also green. It looks suspicious, and indeed - we've spotted an issue with scikit-learn 0.18.1 and earlier: stop words list doesn't play well with the default scikit-learn tokenizer. Tokenizer splits contractions (words like \"don't\") into two parts, but stop words list doesn't include first parts of these contractions.", "Let's add such tokens to the stop words list:", "Accuracy improved a tiny bit - 0.820 instead of 0.819.", "A lesson learned: by looking at model weights and prediction explanations it is possible to spot preprocessing bugs. This particular bug was there in scikit-learn for many years, but it was only ", " recently, while it was easy for us to find this bug just by looking at \"eli5.explain_prediction\" result. If you're a reader from the future then maybe this issue is already fixed; examples use scikit-learn 0.18.1.", "We may also notice that last parts of contractions (\"t\" in \"don't\") are not highlighted, unlike first parts (\"don\"). But \"t\" is not in the stop words list, just like \"don\". What's going on? The reason is that default scikit-learn tokenizer removes all single-letter tokens. This piece of information is not mentioned in scikit-learn docs explicitly, but the gotcha becomes visible if we inspect the prediction result. By looking at such explanations you may get a better understanding of how a library works.", "Another lesson is that even with bugs the pipeline worked overall; there was no indication something is wrong, but after fixing the issue we've got a small quality improvement. Systems based on Machine Learning are notably hard to debug; one of the reasons is that they often can adapt to such software bugs - usually it just costs us a small quality drop. Any additional debugging and testing instrument is helpful: unit tests, checking of the invariants which should hold, gradient checking, etc.; looking at model weights and inspecting model predictions is one of these instruments.", "So far we've only used individual words as features. There are other ways to extract features from text. One common way is to use \"n-grams\" - all subsequences of a given length. For example, in a sentence \"The quick brown fox\" word 2-grams (word bigrams) would be \"The quick\", \"quick brown\" and \"brown fox\". So instead of having a parameter per individual word we could have a parameter per such bigram. Or, more commonly, we may have parameters both for individual words and n-grams. It allows to \"catch\" short phrases - often only a phrase has a meaning, not individual words it consists of.", "It is also possible to use \"char n-grams\" - instead of splitting text into words one can use a \"sliding window\" of a given length. \"The quick brown fox\" can be converted to a char 5-gram as \"The q\", \"the qu\", \"he qui\", etc. This approach can be used when one want to make classifier more robust to word variations, typos, and to make a better use of related words.", "scikit-learn provides a way to extract these n-gram features; let's check how it works, and what the model learns. The code looks almost the same as before; the only change is added \"ngram_range\" and \"analyzer='char'\" TfidfVectorizer arguments:", "Score became worse (0.792), so probably word-based approach works better. This is what coefficients look like:", "\n", "\n", "You can see word chunks, but overall parameters are less readable and inspectable. Prediction:", "\n", "\n", "N-grams are overlapping; individual characters are highlighted according to weights of all ngrams they belong to. It is now more clear which parts of text are important; it seems char n-grams make it all a bit more noisy.", "By the way, haven't we removed stop words already? Why are hey still highlighted? We're passing stop_words argument to TfidfVectorizer as before, but it seems this argument does nothing now. And it indeed does nothing - scikit-learn ignores stop words when using char n-grams; this is documented, but still easy to miss.", "So maybe char n-grams are not that much worse than words for this data - accuracy of a word-based model without stop words removal is similar (0.796 instead of 0.792). It could be the case that after removing stop words and tuning optimal SGDClassifier parameters (which are likely different for char-based features) we can get a similar or better quality. We still don't know if this is true, but at least after the inspection we've got some starting points.", "In this article we followed scikit-learn text processing tutorial and built upon it, but in addition to using common sense and checking validation scores we looked inside the classifier using ", " library. As a result, we:", "scikit-learn docs are tutorials are top-notch; they can be easily the best among all ML software library docs and tutorials, and our findings don't change that. Such problems are common in a real world: small processing bugs, misunderstandings; there were similar data issues in every single real-world project I've worked on. Of course, you can't detect and fix all problems by looking inside models and their predictions, but with eli5 at least you have better chances for spotting such problems.", "We've been using these techniques for many projects: model inspection is a part of data science work in our team, being it classification tasks, Named Entity Recognition or ", " based on Reinforcement Learning. Explanations are not only useful for developers, they are helpful for users of your system as well - users get better understanding of how a system works, and can either trust it more, or become aware of its limitations - see this ", " from our firends ", " for a practical example.", "eli5 library is not limited to linear classifiers and text data; it supports several ML frameworks (scikit-learn, xgboost, LightGBM, etc.) and implements several model explanation methods, both model-specific and ", ". Library is improving; we're trying to get most proven explanation methods available in eli5. But even with all its features it barely scratches the surface; there is a lot of research going on, and it is exciting to see how this field develops."]},
{"tite": "Shubber GetTogether 2018", "date": "November 22, 2018 ", "author": "Ian Kerins", "blog_data": ["It\u2019s hard to believe our annual Shubber GetTogether is already over.", "2018 has been a great year at ", ", so there was no better way to cap it off than with a company retreat in Lisbon, Portugal.", "This year 120 Shubbers travelled from all over the world to meet up, make new friends and celebrate what we\u2019ve done together over the past year.\u00a0", "What made this GetTogether extra special was the fact there were so many new faces this year. Over the last year 40 new Shubbers have joined the Scrapinghub family to help us better serve our clients. So for many Shubbers, last week was the first time they got to meet their team members in person.", "As a 100% remote company with such a large and growing team of Shubbers scattered all over the world, it is very important for us to make sure everyone knows where the company is heading.", "Throughout the first two days we held a town hall where we had great talks by the newly formed senior leadership team recapping everything that has happened in the last year and outlining our future plans for the next year.", "We got to see the product roadmap for 2019, showcasing the exciting new products in development and the new features we will be adding to Scrapy Cloud, Crawlera and Splash.", "Throughout the week numerous workshops were organised to discuss everything from remote working, our company values, open source, etc. Giving everyone the opportunity to share their thoughts and ideas on how to make Scrapinghub a better place to work.", "At the 2017 GetTogether, we started to define our company values with a series of workshops.\u00a0", "With that I'd like to announce that Scrapinghub's company values are:", "In a upcoming blog post we'll talk about these company values in detail.", "This year us Shubbers got a special treat. Three of our enterprise customers joined us and participated in a customer panel to discuss their experience of working with Scrapinghub, how they use web data in their businesses and how Scrapinghub can help support their businesses even further.\u00a0", "This was one of the highlights of the town hall, as it gave everyone (especially Shubbers that aren't customer facing) a chance to see how integral Scrapinghub is to the success of these customers and the impact future product improvements will have on their businesses.\u00a0", "While we worked hard over the week, the retreat was filled with laughter and memories in the making.", "Most of all, it was a time to bond with our fellow Shubbers and share in the passion that binds us all together.", "If there is one thing Shubbers know how to do better than web scraping that is having a good time!", "There was plenty of fun and games throughout the week that have bound us together as a company.", "We explored Lisbon for an afternoon and there was plenty of singing and dancing on our many nights out.", "\u00a0", "If you\u2019d like to be part of the Scrapinghub team and go on the next GetTogether then be sure to check out our ", "."]},
{"tite": "Incremental Crawls with Scrapy and DeltaFetch", "date": "July 20, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we\u2019ve run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on ", " or Facebook with any suggestions for future topics.", "is designed to be extensible and loosely coupled with its components. You can easily extend Scrapy\u2019s functionality with your own middleware or pipeline.", "This makes it easy for the Scrapy community to easily develop new plugins to improve upon existing functionality, without making changes to Scrapy itself.", "In this post we\u2019ll show how you can leverage\u00a0the DeltaFetch plugin\u00a0to run\u00a0incremental crawls.", "Some crawlers we develop are designed to crawl and fetch the data we need only once. On the other hand, many crawlers have to run periodically in order to keep our datasets up-to-date.", "In many of these periodic crawlers, we\u2019re only interested in new pages included since the last crawl. For example, we have a crawler that scrapes articles from a bunch of online media outlets. The spiders are executed once a day and they first retrieve article URLs from pre-defined index pages. Then they extract the title, author, date and content from each article. This approach often leads to many duplicate results and an increasing number of requests each time we run the crawler.", "Fortunately, we are not the first ones to have this issue. The community already has a solution: the ", ". You can use this plugin for incremental (delta) crawls. DeltaFetch's main purpose is to avoid requesting pages that have been already scraped before, even if it happened in a previous execution. It will only make requests to pages where no items were extracted before, to URLs from the spiders' ", " attribute or requests generated in the spiders' ", " method.", "DeltaFetch\u00a0works by intercepting every Item and Request objects generated in spider callbacks. For Items, it computes the related request identifier (a.k.a. ", ") and stores it into a local database. For Requests, Deltafetch computes the request fingerprint and drops the request if it already exists in the database.", "Now let's see how to set up Deltafetch for your Scrapy spiders.", "First, install DeltaFetch using pip:", "Then, you have to enable it in your project's settings.py file:", " has a spider that crawls ", ". It navigates through all the listing pages and visits every book details page to fetch some data like book title, description and category. The crawler is executed once a day in order to capture new books that are included in the catalogue. There's no need to revisit book pages that have already been scraped, because the data collected by the spider typically doesn't change.", "To see Deltafetch in action, ", ", which has DeltaFetch already enabled in settings.py, and then run:", "Wait until it finishes and then take a look at the stats that Scrapy logged at the end:", "Among other things, you'll see that the spider did 1051 requests to scrape 1000 items and that DeltaFetch stored 1000 request fingerprints. This means that only 51 page requests haven't generated items and so they will be revisited next time.", "Now, run the spider again and you'll see a lot of log messages like this:", "And in the stats you'll see that 1000 requests were skipped because items have been scraped from those pages in a previous crawl. Now the spider hasn't extracted any items and it did only 51 requests, all of them to listing pages from where no items have been scraped before:", "By default, DeltaFetch uses a request fingerprint to tell requests apart. This fingerprint is a hash computed based on the canonical URL, HTTP method and request body.", "Some websites have several URLs for the same data. For example, an e-commerce site could have the following URLs pointing to a single product:", "Request fingerprints aren\u2019t suitable in these situations as the canonical URL will differ despite the item being the same. In this example, we could use the product\u2019s ID as the DeltaFetch key.", "DeltaFetch allows us to define custom keys by passing a meta parameter named ", " when initializing the Request:", "This way, DeltaFetch will ignore requests to duplicate pages even if they have different URLs.", "If you want to re-scrape pages, you can reset the DeltaFetch cache by passing the ", " argument to your spider:", "You can also use DeltaFetch in your spiders running on ", ". You just have to enable the DeltaFetch and DotScrapy Persistence addons in your project's Addons page. The latter is required to allow your crawler to access the .scrapy folder, where DeltaFetch stores its database.", "Deltafetch is quite handy in situations as the ones we\u2019ve just seen. ", " Pages from where no items were directly scraped will still be crawled every time you run your spiders.", "You can check out the project page on github for further information: ", "You can find many interesting Scrapy plugins in the\u00a0", " page on Github and you can also contribute to the community by\u00a0including your own plugin there.", "If you have a question or a topic that you'd like to see in this monthly column, please drop a comment here letting us know\u00a0or\u00a0reach us out via ", " on Twitter."]},
{"tite": "Meet Parsel: the Selector Library behind Scrapy", "date": "July 28, 2016 ", "author": "Elias Dorneles", "blog_data": ["We eat our own spider food since Scrapy is our go-to workhorse on a daily basis. However, there are certain situations where Scrapy can be overkill and that\u2019s when we use Parsel. ", " for extracting data from XML/HTML text using CSS or XPath selectors. It powers the scraping API of the ", ".", "Not to be confused with ", "We ", " during Europython 2015 as a part of porting ", ". As a library, it\u2019s lighter than Scrapy (it relies on ", " and ", ") and also more flexible, allowing you to use it within any Python program.", "Install Parsel using pip:", "And here\u2019s ", ". Say you have this HTML snippet in a variable:", "You then import the Parsel library, load it into a Parsel Selector and extract links with an XPath expression:", ": Parsel works both in Python 3 and Python 2. If you\u2019re using Python 2, remember to pass the HTML in a unicode object.", "One of the nicest features of Parsel is the ability to chain selectors. This allows you to chain CSS and XPath selectors however you wish, such as in this example:", "You can also iterate through the results of the .css() and .xpath() methods since each element will be another selector:", "You can find more examples of this ", ".", "The beauty of Parsel is in its wide applicability. It is useful for a range of situations including:", "And now, you can also run Parsel with the command-line tool for simple extraction tasks in your terminal. This new development is thanks to our very own ", " who created ", ".", "Install ", " with ", " and play around using the examples below (you need to have curl installed).", "The following command will download and extract the ", ":", "You can also get the current top 5 news items from Hacker News using:", "And how about obtaining a list of the latest YouTube videos from a specific channel?", "I hope that you enjoyed this little tour of Parsel and I am looking forward to seeing how these examples have sparked your imagination when finding solutions for your HTML parsing needs.", "The next time you find yourself wanting to extract data from HTML/XML and don\u2019t need Scrapy and its crawling capabilities, you know what to do: just ", " it!", "Feel free to reach out to us on ", " and let us know how you use Parsel in your projects."]},
{"tite": "This Month in Open Source at Scrapinghub August 2016", "date": "August 04, 2016 ", "author": "Paul Tremberth", "blog_data": ["Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera.", "If you\u2019re interested in learning more or even becoming a contributor, reach out to us by emailing opensource@scrapinghub.com or on ", ".", "This past May, ", " was a big milestone for our Python web scraping community. And 2 weeks ago, Scrapy reached ", ", making it the ", "! We are very proud of this and want to thank all our users, stargazers and contributors!", "We\u2019re moving various Scrapy middleware and helpers to their own repository under ", " on GitHub. They are all available on PyPI.", "Many of these were previously found wrapped inside scrapylib (which will not see a new release).", "Here are some of the newly released ones:", "In mid-June we released ", " with quite a few parsing improvements and new features (as well as several bug fixes). For example, this version introduces its own parser, replacing dateutil\u2019s one. However, we may converge back at some point in the future.", "It also handles relative dates in the future e.g. \u201ctomorrow\u201d, \u201cin two weeks\u201d, etc. We also replaced PyYAML with one of its active forks, ", ". We hope you enjoy it!", "Fun fact: we caught the attention of ", ". And although dateparser didn\u2019t quite solve his issue, \u201c[he] like[s] it a lot\u201d so it made our day ;-)", ", extracted from Scrapy helpers. You may find it handy when walking in the jungle of non-ASCII URLs in Python 3!", "And that\u2019s it for This Month in Open Source at Scrapinghub August 2016. Open Source is in our DNA and so we\u2019re always working on new projects and improving pre-existing ones. Keep up with us and ", ". We welcome contributors and we are also hiring, so ", "!"]},
{"tite": "What the Suicide Squad Tells Us About Web Data", "date": "August 11, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Web data is a bit like the Matrix. It\u2019s all around us, but not everyone knows how to use it meaningfully. So here\u2019s a brief overview of the many ways that web data can benefit you as a researcher, marketer, entrepreneur, or even multinational business owner.", "Since web scraping and web data extraction are sometimes viewed a bit like antiheroes, I\u2019m introducing each of the use cases through characters from the ", ". I did my best to pair according to character traits and real world web data uses, so hopefully this isn\u2019t too much of a stretch.", "This should be spoiler free, with nothing revealed that you can\u2019t get from the ", "! Fair warning, you\u2019re going to have Ballroom Blitz stuck in your head all day. And if you haven\u2019t seen Suicide Squad yet, hopefully we get you pumped up for this popcorn movie.", "\u00a0", "\u00a0", "Deadshot\u2019s claim to fame is accurate aim. He can predict bullet trajectories and he never misses a shot. So I paired him with using web data for market research and trend prediction. You can scrape multiple websites for price fluctuation, new products, reviews, and consumer trends. This is an automated process that allows you to quickly and accurately analyze data without needing to manually monitor websites.", "\u00a0", "\u00a0", "Harley Quinn has a sunny personality that remains chipper even when faced with death, destruction, torture, and mayhem. She also always has a witty comeback no matter the situation. These traits go hand-in-hand with how brands should approach social media channels. Extracting web data from social media interactions help you understand consumer opinions. You can monitor ongoing chatter about your company or your competition and respond in the ", ".", "\u00a0", "This is probably the most obvious pairing since Amanda Waller (played by the wonderful Viola Davis) is the one responsible for assembling the Suicide Squad. She carefully researched and compiled intimate details on all the criminals-turned-reluctant-heroes. This is an aspect of web data that benefits all sales, marketing, recruitment, and HR. With a pre-vetted pool, you\u2019ll have access to qualified leads and decision-makers without needing to wade through the worst of the worst.", "\u00a0", "This sewer-dwelling villain thrives in dark and hidden spaces. He\u2019s used to working underground and in places most people don\u2019t even know exist. This makes Killer Croc the perfect backdrop for the type of web data located in the deep/dark web. The ", " is the part of the internet that is not indexed by search engines (Google, Bing, etc.) and is often a haven for criminal activity. Data scraped from this part of the web is commonly used by law enforcement agencies.", "\u00a0", "\u00a0", "This jewelry thief goes around the world stealing from banks and committing acts of burglary - with a boomerang... Captain Boomerang knows all about pricing and the comparative value of products so he can get the largest bang for his buck. Similarly, web data is a great resource for new companies looking to research their industry and how their prices match up to the competition. And if you are an established company, this is a great way for you to keep track of newcomers and potential market disruptors.", "\u00a0", "\u00a0", "In her 6313 years of existence, the Enchantress has had to cope with changing times, customs, and civilizations. The ability to learn quickly and adapt to new situations is definitely an important part of her continued survival. Likewise, machine learning is a form of artificial intelligence that can learn when given new information. Train your machine learning models using datasets for conducting sentiment analysis, making predictions, and even ", ". Whether you are an SaaS company specializing in developing machine learning technology or someone who needs machine learning analysis, you need to ensure you have up-to-date datasets.", "\u00a0", "\u00a0", "Colonel Rick Flag is a \u201cgood guy\u201d whose job is to keep track of the Suicide Squad and kill them if they get out of line. Now obviously your relationship with resellers is not a life-and-death situation, but it can be good to know how your brand is being represented across the internet. Web scraping can help you keep track of reseller customer reviews and any contract violations that might be occurring.", "\u00a0", "\u00a0", "Katana the samurai is the enforcer of the Suicide Squad. She is there as an additional check to keep the criminal members in line. Similarly, web data allows reporters, lawyers, and concerned citizens to keep track of government officials, potential corruption charges, and changing legal matters. You can scrape obscure or poorly presented public records and then use that information to create accessible interfaces for easy reference and research.", "\u00a0", "\u00a0", "I believe the Joker needs no introduction, whether you know this character from Jack Nicholson, Heath Ledger, or the new Jared Leto incarnation. He is unpredictable, has eclectic tastes, and is capable of doing anything. And honestly, this is what web scraping is all about. Whether you want to build a ", " or ", ", web data provides the backbone for all of your creative endeavors.", "I hope you enjoyed this unorthodox tour of the world of web data! If you\u2019re looking for some mindless fun, Suicide Squad ain\u2019t half bad (it ain\u2019t half good either). If you\u2019re looking to explore how web data fits within your business or personal projects, feel free to ", ". And if you\u2019re looking to hate on or defend Suicide Squad, comment below.", "P.S. There is no way this movie is worse than "]},
{"tite": "Introducing Scrapy Cloud with Python 3 Support", "date": "August 17, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["It\u2019s the end of an era. Python 2 is on its way out with only a few security and bug fixes forthcoming from now until its ", ". Given this withdrawal of support and the fact that Python 3 has snazzier features, we are thrilled to announce that ", " now officially supports Python 3.", "If you are new to Scrapinghub, ", " is our production platform that allows you to deploy, monitor, and scale your web scraping projects. It pairs with ", ", the open source web scraping framework, and ", ", our open source visual web scraper.", "I\u2019m sure you Scrapy users are breathing a huge sigh of relief! While ", " has been around since May, you can now deploy your Scrapy spiders using the fancy new features introduced with Python 3 to Scrapy Cloud. You\u2019ll have the beloved extended tuple unpacking, function annotations, keyword-only arguments and much more at your fingertips.", "Fear not if you are a Python 2 developer and can't port your spiders' codebase to Python 3, because Scrapy Cloud will continue supporting Python 2. In fact, Python 2 remains the default unless you explicitly set your environment to Python 3.", "Docker support was one of the new features that came along with the ", " release in May. It brings more flexibility to your spiders, allowing you to define in which kind of runtime environment (AKA ", ") they will be executed.", "This configuration is done in your local project's ", ". There you have to include a section called ", " having ", " as the stack for your Scrapy Cloud project:", "After doing that, you just have to deploy your project using ", ":", ": make sure you are using shub ", " by upgrading it:", "And you're all done! The next time you run your spiders on Scrapy Cloud, they will run on Scrapy 1.1 + Python 3.", "If you have a multi-target deployment file, you can define a separate stack for each project ID:", "This allows you to deploy your local project to whichever Scrapy Cloud project you want, using a different stack for each one:", "This deploys your crawler to project 99999 and uses Scrapy 1.1 + Python 3 as the execution environment.", "You can find ", ".", "We hope that you\u2019re as excited as we are for this newest upgrade to Python 3. If you have further questions or are interested in learning more about the souped up Scrapy Cloud, take a look at ", ".", "For those new to our platform, Scrapy Cloud has a forever free subscription, so ", "."]},
{"tite": "Embracing the Future of Work: How To Communicate Remotely", "date": "September 22, 2016 ", "author": "Cecilia Haynes", "blog_data": ["What does \u201cthe Future of Work\u201d mean to you? To us, it describes how we approach life at Scrapinghub. We don't work in a traditional office (we're 100% distributed) and we allow folks the freedom to make their own schedules (you know when you work best). By\u00a0finding ways to ", "\u00a0mode, we ended up creating a framework for the Future of Work.", "Maybe you've heard of this term and want to learn more or maybe you're thinking about implementing aspects of it at your own company, regardless we can't stress enough that\u00a0effective communication is a key part.", "According to Jacob Morgan (who literally wrote the book, ", "), this broad term can be broken into ", ": Freedom/Flexibility, Autonomy, and Choice/Customization. Not mentioned in his list is the rise of AI, although we might as well prepare for our inevitable robot overlords.", "The Future of Work both describes how the employment landscape will look in the future as well as how humans will need adapt to technological leaps and changing expectations of employment. Along these lines, we are remote (always have been, always will be) and we are into\u00a0", ". We\u2019re also open source to the core and if you think wrangling regular developers is a challenge, wait until you meet scarily intelligent super brains who know how to maneuver their pet projects into benefiting their company.", "We\u2019re living, breathing, and occasionally chaotic proof that the Future of Work is ", ".", "Our two co-founders, Shane from Ireland and Pablo from Uruguay, established a company based on talent, not geography. And that\u2019s not even a ", ".", "Full disclosure, I\u2019m a millennial who was drawn to Scrapinghub because of its remote stance. I wanted the flexibility to be a digital nomad while not needing to rely on the uncertainty of freelance work. This opportunity was the ", " and I\u2019ve since come to understand how parents can also benefit from remote life (and introverts, and folks who like to work at odd hours).", "Remote work is not for everyone. While obviously there are co-working spaces and coffee shops, you need to be comfortable with establishing your own schedule along with\u00a0having the discipline to work by\u00a0your lonesome.", "On the flip side, you have the flexibility and freedom to sort out your responsibilities\u00a0so that they fit within your life. And this is a pretty important point for companies looking to transition into the remote space. You need to understand how time zones impact the way that\u00a0teams operate and how to trust your team members to get their work done on time.", "When hiring team members from a variety of countries and backgrounds, it\u2019s important to keep local holidays in mind. Adopting an open holiday policy both respects the diversity of your company (we\u2019re based in 48 countries) and also recognizes the importance of having time off.", "Cultural fit is especially important in a remote team. Autonomy and trust is a huge part of how we operate because there is no one looking over your shoulder (quite literally). Finding motivated people who can finish their work and stay on top of their responsibilities without needing oversight is crucial to running a successful remote operation.", "We make sure that our teammates feel comfortable voicing concerns and sharing\u00a0ideas for improving the company\u00a0by promoting\u00a0an\u00a0open Slack policy.\u00a0No matter our\u00a0level or seniority, we remain accessible to all members of Scrapinghub. This policy facilitates collaboration and helps create a sense of community.", "Tone is incredibly difficult to convey through writing. Think of every misunderstanding that you\u2019ve ever had through written communication (text messages, tweets, comments on Facebook, etc.), and then add in work-related stress. This is not a great situation unless you develop straightforward channels of communication.", "This is our system, so feel free to steal the workflow. Honestly, different methods work for different teams. As long as you have one unifying communication stream, go with whatever feels right:", ": Used for day-to-day communication. Slack is mainly how we stay in touch. The\u00a0", " is great, as is the ", ". We're also using ", " as a way for upward feedback and to get a handle on the pulse of our colleagues.", ": This is for team meetings and it can also be used for impromptu watercooler moments. In Growth specifically, we\u00a0plan our sprints and have eclectic meetings about non-work related topics like ", ".", ": We use this platform for our larger gatherings\u00a0so that the majority of the company can join in. We have Town Halls where our founders share company-wide information in order to increase transparency. GoToMeeting is also used for our Friday lightning talks (affectionately known as Shub talks) where team members present on interesting topics ranging from recipes to machine\u00a0learning techniques.", ": Optimal for handling outside communications and for sharing external chats with the rest of our teams.", ": Intercom is how we stay in touch with users and customers.", ": Some teams use this as an\u00a0issue tracker and as a way to assign necessary tasks.", ": Used by other teams for support ticket management and sprint planning.", ": Used by our Growth team\u00a0for sprint planning\u00a0and to keep track of daily activities. I use Trello for my editorial calendar since it\u2019s easy for me to organize categories, assign writers, and set due dates.", ": We\u2019re open source folks and so ", " is a huge part of how we work. Even ", "\u00a0are managed\u00a0through pull requests.", ": The sales team uses this for lead management.", ": Used by the HR team for managing job applications.", ": SHEPs (a play on ", ") are plans created by employees with suggestions for how Scrapinghub can be improved. SHEPs\u00a0include everything from HR perks to business planning. SHEPs can be created and submitted by anyone in the company.", ": We recently adopted Confluence as a knowledge base in addition to Google Drive. We want to reduce the ", "\u00a0and Confluence has been especially beneficial in accomplishing this goal. Team updates, meeting notes, and ongoing projects are easily reviewed and shared within the company using this program.", ": Our weekly newsletter (sent through\u00a0MailChimp) shares information on more personal topics like vacations, new hires, and birthdays. The newsletters help us to keep up-to-date on the quirky and HR-related aspects of Scrapinghub. We highlight exemplary employees, team bios, and conference activities as a way to keep everyone in the loop and to remain connected.", "We\u2019re on the front lines of exploring the Future of Work. Between our technological advances (and we welcome ", ") and our remote organization, we're experimenting with how to best move forward with maximum flexibility while not turning into robots. We stress effective and clear communication because no on wants to play a game of telephone across international waters.", "What are your thoughts on the Future of Work and on remote companies? What applications do you use that we don't? What workflows\u00a0are you implementing that you would like to share? Please let us\u00a0know in the comments or reach out on ", "."]},
{"tite": "How to Crawl the Web Politely with Scrapy", "date": "August 25, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["The first rule of web crawling is you do not harm the website. The second rule of web crawling is you do ", " harm the website. We\u2019re supporters of the democratization of web data, but not at the expense of the website\u2019s owners.", "In this post\u00a0we\u2019re sharing a few tips for our platform and Scrapy users who want polite and considerate web crawlers.", "Whether you call them spiders, crawlers, or robots, let\u2019s work together to create a world of Baymaxs, WALL-Es, and R2-D2s rather than an apocalyptic wasteland of HAL 9000s, T-1000s, and Megatrons.", "A polite crawler respects robots.txt", " A polite crawler never degrades a website\u2019s performance", " A polite crawler identifies its creator with contact information", " A polite crawler is not a pain in the buttocks of system administrators", "Always make sure that your crawler follows the rules defined in the website's robots.txt file. This file is usually available at the root of a website (www.example.com/robots.txt) and it describes what a crawler should or shouldn't crawl according to the ", ". Some websites even use the crawlers\u2019 user agent to specify separate rules for different web crawlers:", "Mission critical to having a polite crawler is making sure your crawler doesn't hit a website too hard. Respect the delay that crawlers should wait between requests by following the robots.txt Crawl-Delay directive.", "When a website gets overloaded with more requests that the web server can handle, they might become unresponsive. Don\u2019t be that guy or girl that causes a headache for the website administrators.", "However, if you have ignored the cardinal rules above (or your crawler has achieved aggressive sentience), there needs to be a way for the website owners to contact you. You can do this by including your company name and an email address or website in the request's User-Agent header. For example, Google's crawler user agent is \"Googlebot\".", "Hey folks using our ", " platform! We trust you will crawl responsibly, but to support website administrators, we provide an ", " where they can report any misbehaviour from crawlers running on our platform. We\u2019ll kindly pass the message along so that you can modify your crawls and avoid ruining a sysadmin\u2019s day. If your crawler\u2019s are turning into Skynet and ", ", we reserve the right to halt their crawling activities and thus avert the robot apocalypse.", " is a bit like Optimus Prime: friendly, fast, and capable of getting the job done no matter what. However, much like Optimus Prime and his fellow Autobots, Scrapy occasionally needs to be ", ". So here\u2019s the nitty gritty for ensuring that Scrapy is as polite as can be.", "Crawlers created using Scrapy 1.1+ already respect robots.txt by default. If your crawlers have been generated using a previous version of Scrapy, you can enable this feature by adding this in the project's settings.py:", "Then, every time your crawler tries to download a page from a disallowed URL, you'll see a message like this:", "It\u2019s important to provide a way for sysadmins to easily contact you if they have any trouble with your crawler. If you don\u2019t, they'll have to dig into their logs and look for the offending IPs.", "Be nice to the friendly sysadmins in your life and identify your crawler via the Scrapy USER_AGENT setting. Share your crawler name, company name and a contact email:", "Scrapy spiders are blazingly fast. They can handle many concurrent requests and they make the most of your bandwidth and computing power. However, with great power comes great responsibility.", "To avoid hitting the web servers too frequently, you need to use the ", " setting in your project (or in your spiders). Scrapy will then introduce a random delay ranging from 0.5 * DOWNLOAD_DELAY to 1.5 * DOWNLOAD_DELAY seconds between consecutive requests to the same domain. If you want to stick to the exact DOWNLOAD_DELAY that you defined, you have to disable ", ".", "By default, DOWNLOAD_DELAY is set to 0. To introduce a 5 second delay between requests from your crawler, add this to your settings.py:", "If you have a multi-spider project crawling multiple sites, you can define a different delay for each spider with the download_delay (yes, it's lowercase) spider attribute:", "Another setting you might want to tweak to make your spider more polite is the number of concurrent requests it will do for each domain. By default, Scrapy will dispatch at most 8 requests simultaneously to any given domain, but you can change this value by updating the ", " setting.", "Heads up, the ", " setting defines the maximum amount of simultaneous requests that Scrapy's downloader will do for all your spiders. Tweaking this setting is more about your own server performance / bandwidth than your target's when you're crawling multiple domains at the same time.", "Websites vary drastically in the number of requests they can handle. Adjusting this manually for every website that you are crawling is about as much fun as watching paint dry. To save your sanity, Scrapy provides an extension called ", ".", "AutoThrottle automatically adjusts the delays between requests according to the current web server load. It first calculates the latency from one request. Then it will adjust the delay between requests for the same domain in a way that no more than ", " requests will be simultaneously active. It also ensures that requests are evenly distributed in a given timespan.", "To enable AutoThrottle, just include this in your project's settings.py:", "Scrapy Cloud users don't have to worry about enabling it, because it's already enabled by default.", "There\u2019s a ", " to help you tweak the throttle mechanism, so have fun playing around!", "Developing a web crawler is an iterative process. However, running a crawler to check if it\u2019s working means hitting the server multiple times for each test. To help you to avoid this impolite activity, Scrapy provides a built-in middleware called ", ". You can enable it by including this in your project's settings.py:", "Once enabled, it caches every request made by your spider along with the related response. So the next time you run your spider, it will not hit the server for requests already done. It's a win-win: your tests will run much faster and the website will save resources.", "Many websites provide HTTP APIs so that third parties can consume their data without having to crawl their web pages. Before building a web scraper, check if the target website already provides an HTTP API that you can use. If it does, go with the API. Again, it's a win-win: you avoid digging into the page\u2019s HTML and your crawler gets more robust because it doesn\u2019t need to depend on the website\u2019s layout.", "Let\u2019s all do our part to keep the peace between sysadmins, website owners, and developers by making sure that our web crawling projects are as noninvasive as possible. Remember, we need to band together to delay the rise of our robot overlords, so let\u2019s keep our crawlers, spiders, and bots polite.", "To all website owners, help a crawler out and ensure your site has an HTTP API. And remember, if someone using our platform is overstepping their bounds, please fill out an ", " and we\u2019ll take care of the issue.", "For those new to our platform, ", " and is the peanut butter to Scrapy\u2019s jelly. For our existing Scrapy and Scrapy Cloud users, hopefully you learned a few tips for how to both speed up your crawls and prevent abuse complaints. Let us know if you have any further suggestions in the comment section below!"]},
{"tite": "How to Deploy Custom Docker Images for Your Web Crawlers", "date": "September 08, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["[UPDATE]: Please see ", "\u00a0for an up-to-date version.", "What if you could have complete control over your environment? Your crawling environment, that is... One of the many benefits of our upgraded production environment, Scrapy Cloud 2.0, is that you can customize your crawler runtime environment via Docker images. It\u2019s like a superpower that allows you to use specific versions of Python, Scrapy and the rest of your stack, deciding if and when to upgrade.", "With this new feature, you can tailor a Docker image to include any dependency your crawler might have. For instance, if you wanted to crawl JavaScript-based pages using Selenium and PhantomJS, you would have to include the PhantomJS executable somewhere in the PATH of your crawler's runtime environment.", "And guess what, we\u2019ll be walking you through how to do just that in this post.", "Download the ", " or clone the ", " to follow along.", "Imagine you ", " to handle website content that is rendered client-side via Javascript. You decide to use selenium and PhantomJS. However, since PhantomJS is not installed by default on Scrapy Cloud, trying to deploy your crawler the usual way would result in this message showing up in the job logs:", "PhantomJS, which is a C++ application, needs to be installed in the runtime environment. You can do this by creating a custom Docker image that downloads and installs the PhantomJS executable.", "First you have to install a ", " that will help you with building and deploying the image:", "Before using shub, you have to include ", " in your project's requirements file, which is a runtime dependency of Scrapy Cloud.", "Once you have done that, run the following command to generate an initial Dockerfile for your custom image:", "It will ask you whether you want to save the Dockerfile, so confirm by answering ", ".", "Now it\u2019s time to include the installation steps for PhantomJS binary in the generated Dockerfile. All you need to do is copy the highlighted code below and put it in the proper place inside your Dockerfile:", "The Docker image you're going to build with shub has to be uploaded to a Docker registry. I used ", ", the default Docker registry, to create a repository under my user account:", "Once this is done, you have to define the ", " setting in your project's scrapinghub.yml (replace stummjr/demo with your own):", "This will tell shub where to push the image once it\u2019s built and also where Scrapy Cloud should pull the image from when deploying.", "Now that you have everything configured as expected, you can build, push and deploy the Docker image to Scrapy Cloud. This step may take a couple minutes, so now might be a good time to go grab a cup of coffee. :)", "If everything went well, you should now be able to run your PhantomJS spider on Scrapy Cloud. If you followed along with the sample project from the GitHub repo, your crawler should have collected 300 quotes scraped from the page that was rendered with PhantomJS.", "You now officially know how to use custom Docker images with Scrapy Cloud to supercharge your crawling projects. For example, you might want\u00a0to do OCR using ", "\u00a0in your crawler. Now you can, it's just a matter of creating a Docker image\u00a0with the\u00a0Tesseract command line tool and ", " installed. You can also install tools from apt repositories and even compile the libraries/tools that you want.", "this feature is still in beta, so be aware that some Scrapy Cloud\u00a0features, such as addons, dependencies and settings, still don't\u00a0work with custom images.", "For further information, check out the ", ".", "Feel free to comment below with any other ideas or tips you\u2019d like to hear more about!"]},
{"tite": "Improved Frontera: Web Crawling at Scale with Python 3 Support", "date": "September 01, 2016 ", "author": "Alexander Sibiryakov", "blog_data": ["Python is our go-to language of choice and Python 2 is losing traction. In order to survive, older programs need to be Python 3 compatible.", "And so we\u2019re pleased to announce that ", " will remain alive and kicking because it now supports Python 3 in full! Joining the ranks of ", " and ", ", you can officially continue to quickly create and scale fully formed crawlers without any issues in your Python 3-ready stack.", "As a key web crawling toolbox that works with Scrapy, along with other web crawling systems, Frontera provides a ", " that is ideal for broad crawls. Frontera manages when and what to crawl next, and checks for crawling goal accomplishment. This is especially useful for building a ", " with multiple web spider\u00a0processes consuming URLs from a frontier.", "Once you\u2019re done cheering with joy, read on to see how you can use this upgrade in your stack.", "This move to Python 3 includes all run modes, workers, message buses, and backends, HBase, ZeroMQ and Kafka clients. The development process is now a lot more reliable since we have tests that cover all major components as well as integration tests running HBase and Kafka.", "Frontera is already available on PyPI. All you need to do is ", ". And then you just run it with Python 3 interpreter and you\u2019re ready to get your crawlers scaled!", "The request object is now propagated throughout the whole pipeline, allowing you to schedule requests with custom methods, headers, cookies and body parameters.", "Now, Frontera guarantees the exclusive assignment of extracted links to strategy workers based on links' hostname.", "So links from a specific host will be always be assigned to the same strategy worker instance which prevents errors and greatly simplifies design.", "In the near to distant future, we want Frontera and Frontera-based crawlers to be the number one software for large scale web crawling. Our next step in this process is to ease the deployment of Frontera in the Docker environment. This includes scaling and management.", "We\u2019re aiming for Frontera to be easily deployable to major cloud providers infrastructures like Google Cloud Platform and AWS, among others. It\u2019s quite likely we will choose Kubernetes as our orchestration platform. Along with this goal, we will develop a good Web UI to manage and monitor Frontera-based crawlers. So stay tuned!", "Have we piqued your interest? Here\u2019s a ", " to get started.", "Well, what are you waiting for? Take full advantage of Frontera with Python 3 support and start scaling your crawls. ", " to see what's possible."]},
{"tite": "Scrapy + MonkeyLearn: Textual Analysis of Web Data", "date": "May 11, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["To kick off the MonkeyLearn Addon Tutorial series, let\u2019s start with something we can all identify with: shopping. Whether you need to buy something for yourself, friends or family, or even the office, you need to evaluate cost, quality, and reviews. And when you\u2019re working on a budget of both money and time, it can be helpful to automate the process with web scraping.", "When scraping shopping and e-commerce sites, you\u2019re most likely going to want product categories. Typically, you\u2019d do this using the breadcrumbs. However, the challenge comes when you want to scrape several websites at once while keeping categories consistent throughout.", "This is where MonkeyLearn comes in. You can use their Retail Classifier to classify products based on their descriptions, taking away the ambiguity of varied product categories.", "This post will walk you through how to use\u00a0MonkeyLearn\u2019s Retail Classifier through the ", " on Scrapy Cloud to scrape and categorise products from an online retailer.", "For those new readers, ", " is our cloud-based platform that lets you easily deploy and run Scrapy and Portia web spiders without needing to deal with servers, libraries and dependencies, scheduling, storage, or monitoring. ", " and now features Docker support and a whole host of other updates.", "In this tutorial, we\u2019re using ", " to crawl and extract data. Scrapy\u2019s decoupled architecture lets you use ready-made integrations for your spiders. The MonkeyLearn addon implements a ", ". The addon takes every item scraped and sends the fields of your choice to MonkeyLearn for analysis. The classifier then stores the resulting category in another field of your choice. This lets you classify items without any extra code.", "If you are a new user, sign up for ", " to continue on with this addon tutorial.", "We\u2019ll begin by trying out the ", " with a sample description:", "Paste this sample in the ", " under the Sandbox > Classify tab. And hit Submit:", "You should get the following results:", "MonkeyLearn's engine analyzed the description and identified that the product belongs in the ", " categories. As a bonus, it specifies how sure it is of its predictions.", "The same example ", " would be:", "You can ", " on MonkeyLearn and replace ", "\u00a0 with your particular API token to play with the retail classifier further.", "Now we are going to deploy a Scrapy project to Scrapy Cloud and use the MonkeyLearn addon to categorize the scraped data. You can clone ", ", or build your own spiders, and\u00a0follow the steps described below.", "For this tutorial, we built a ", " for a ", ". The spider is pretty straightforward so that you can easily ", " and try it by yourself. However, you should be aware of some details when building a spider from scratch to use with the MonkeyLearn addon.", "First, the addon requires your spiders to generate Item objects from a pre-defined Item class. In our case, it's the ", " class:", "Second, you have to declare where the MonkeyLearn addon will store the analysis' results as an additional field in your Item class. For our spider, these results will be stored in the ", " field of each of the items scraped.", " is the command line tool to manage your Scrapy Cloud services and you will use it to deploy your Scrapy projects there. You can install it by:", "Now authenticate yourself on Scrapy Cloud:", "You can get your API key in your ", ".", "First go to Scrapy Cloud's web dashboard and create a project there.", "Now your project is ready to run in Scrapy Cloud.", "To enable the addon, head to the Addons Setup section in your Scrapy Cloud project's settings:", "You can configure the addon with the following settings:", "You can find the id of any classifier in the URL:", "When you\u2019re done filling out all the fields, the addon configuration should look something like this:", "Now that you have the Retail Classifier enabled, run\u00a0the spider by going to your project\u2019s Jobs page. Click \u2018Run Spider\u2019, select the spider and then confirm.", "Give the spider a couple of minutes to gather results. You can then view the job's items and you should see that the category field has been filled by MonkeyLearn:", "You can then download the results as a JSON or XML\u00a0file and then categorize the products by the categories and probabilities returned by the addon.", "Using ", " with ", " on Scrapy Cloud allows you to immediately analyze your data for easier categorization and analysis. So the next time you\u2019ve got a massive list of people to shop for, try using immediate textual analysis with web scraping to simplify the process.", "We\u2019ll continue the series with walkthroughs on using the ", " for language detection, sentiment analysis, keyword extraction, or any custom classification or extraction that you may need personally or professionally. We\u2019ll explore different uses and hopefully help you make the most of this new platform integration.", "If you haven\u2019t already, sign up for ", " (for free) and sign up for the newly upgraded ", " and get to experimenting."]},
{"tite": "Data Extraction with Scrapy and Python 3", "date": "May 25, 2016 ", "author": "Paul Tremberth", "blog_data": ["Fasten your seat belts, ladies and gentlemen: Scrapy 1.1 with Python 3 support is officially out! After a couple months of hard work and ", ", this is the first official Scrapy release to support Python 3.", "We know that many of you have been eagerly looking forward to moving your whole stack to Python 3. Well, wait no more, you can get rid of Python 2 once and for all (for the most part)!", "Without further ado, let's dive into the nuts and bolts of this latest step forward.", "Python 3 support isn\u2019t the only good news coming from this release. There are a few features and general improvements that you might want to be aware of:", "Check out the ", " for a complete list of changes.", "You can Install or upgrade Scrapy in your environment by running:", "You can create a Python 3 virtualenv for Scrapy (e.g. using ", "):", "Scrapy on Python 3 doesn't work in Windows environments yet. Scrapy depends on Twisted and some parts of Twisted haven\u2019t been ported yet. Once ", " is solved, Scrapy users with Windows will be able to run their spiders on Python 3.", "In addition to this, there are a couple of features that are not supported on Python 3:", "Heads up, Scrapy users, Scrapy 1.1 introduces some minor backward incompatible changes that might break your existing spiders:", "We couldn\u2019t have done it without the help of all you folks reporting and fixing issues, requesting and submitting features, commenting on pull requests, improving documentation, etc., the list goes on.", "Scrapy is a community and Scrapy 1.1 is the result of this community effort. You should be proud of yourselves. Kudos to all you Scrapy lovers!", "We'd also like to nominate and thank everyone (in alphabetical order) who contributed directly to the Scrapy 1.1 source code:", "Agustin Castro, Aivars Kalv\u0101ns, Alexander Chekunkov, Alexander Sibiryakov, Ally Weir, Andrew Murray, Andrew Scorpil, Aron Bordin, Artur Gaspar, Berker Peksag, Bryan Crowe, Capi Etheriel, Carlos Pe\u00f1a, cclauss, Chris Nilsson, Christian Pedersen, Daniel Collins, Daniel Gra\u00f1a, David Chen, David Tagatac, Demelziraptor, Dharmesh Pandav, dinesh, djunzu, Elias Dorneles, Gregory Vigo Torres, Hoat Le, hy, Jakob de Maeyer, Jamey Sharp, Julia Medina, Konstantin Lopuhin, Lele, Leonid Amirov, Luar Roji, Lucas Moauro, Marco DallaG, Marius Gedminas, Marven Sanchez, mgachhui, Mikhail Korobov, Mikhail Lyundin, nanolab, nblock, Nicolas Pennequin, Nikola Pavlovi\u0107, \u039d\u03b9\u03ba\u03cc\u03bb\u03b1\u03bf\u03c2-\u0394\u03b9\u03b3\u03b5\u03bd\u03ae\u03c2 \u039a\u03b1\u03c1\u03b1\u03b3\u03b9\u03ac\u03bd\u03bd\u03b7\u03c2, nyov, Olaf Dietsche, orangain, Pablo Hoffman, palego, Panayiotis Lipiridis, Patrick Connolly, Paul Tremberth, Pawel Miech, Pengyu Chen, preetwinder, Rafa\u0142 Gutkowski, Ralph Gutkowski, Raul Gallegos, Rick, Robert Weindl, Rolando Espinoza, seales, smirecki, Valdir Stumm Jr, Victor Mireyev, Yaroslav Halchenko and Zolt\u00e1n Szeredi.", "Without your efforts, none of this would be have been possible!", "Python 3 support has been out in beta release for just a few months. Chances are that there are still some corner cases that have yet to be discovered. If you happen to face any unexpected behaviour, please report your findings in the ", ".", "You can also contribute to the Scrapy community in several ways, such as improving documentation, writing tutorials, fixing bugs and including new features in Scrapy. Check the ", " if you want to engage with this amazing community.", "Happy Scraping!"]},
{"tite": "Introducing the Datasets Catalog", "date": "June 09, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Folks using ", " and ", " are engaged in a variety of fascinating web crawling projects, so we wanted to provide you with a way to share your data extraction prowess with the world.", "With this need in mind, we\u2019re pleased to introduce the latest addition to our Scrapinghub platform: the ", "!", "This new feature allows you to\u00a0immediately share the results of your Scrapinghub projects as\u00a0publicly searchable datasets. Not only is this a great way to collaborate with others, you can also save time by using other people\u2019s datasets in your projects.", "As fans of the open data movement, we hope that this new feature will ease the process of\u00a0disseminating data. Open data has been used to help foster transparency in ", " and corporate systems ", ". ", " and ", "\u00a0have also benefited from the mutual sharing of information. A couple\u00a0of our own engineers have even used open data to power ", " and to help journalists ", ".", "Read on to get some ideas on how to use the ", " in your workflow.", "We are launching the Datasets Catalog\u00a0with the following features:", "You can find this new \u201cDatasets\u201d option in the menu located at\u00a0the top navigation bar.\u00a0On the main ", "\u00a0page,\u00a0you can browse available datasets along with those that you have recently visited.", "Publishing your scraped\u00a0data into complete\u00a0datasets\u00a0takes just one click.\u00a0This ", " on publishing and sharing your extracted data.", "\u00a0", "And there you have it, a way to not only showcase your web crawling and data extraction skills, but to also help others with the information that you provide.", "We invite\u00a0you to contribute your datasets and play your part in helping drive the open source movement forward. Reach out to us on\u00a0", " and let us know what datasets\u00a0you\u00a0would\u00a0like to see featured and if you have\u00a0any recommendations for\u00a0improving the whole\u00a0", "\u00a0experience.", "We're excited to see what you come up with!"]},
{"tite": "How to Debug your Scrapy Spiders", "date": "May 18, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to Scrapy Tips from the Pros! Every month we release a few tricks and hacks to help speed up your web scraping and data extraction activities. As the lead Scrapy maintainers, we have run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on ", " or ", " with suggestions for future topics.", "Your spider isn\u2019t working and you have no idea why. One way to quickly spot potential issues is to add a few print statements to find out what's happening. This is often my first step and sometimes all I need to do to uncover the bugs that are preventing my spider from running properly. If this method works for you, great, but if it\u2019s not enough, then read on to learn about how to deal with the nastier bugs that require a more thorough investigation. In this post, I\u2019ll introduce you to the tools that should be in the toolbelt of every Scrapy user when it comes to debugging spiders.", "Scrapy shell is a full-featured Python shell loaded with the same context that you would get in your spider callback methods. You just have to provide an URL and Scrapy Shell will let you interact with the same objects that your spider handles in its callbacks, including the response object.", "After loading it, you can start playing around with the response in order to build the selectors to extract the data that you need:", "If you're not familiar with Scrapy Shell, give it a try. It's a perfect fit for your development workflow, sitting right after the page inspection in the browser. You can create and test your spider's extraction rules and use them in your spider's code\u00a0once you've built\u00a0the ones you need.", "Learn more about ", ".", "If your\u00a0spider has been\u00a0behaving unexpectedly for certain\u00a0responses, you can quickly see what's happening using\u00a0the ", " method in your spider code. This will open a Scrapy shell session that will let you interact with the current response object.", "For example, imagine that your spider is not extracting the expected amount of items from certain pages and you want to see what's wrong with the response returned by the website:", "Once the execution hits the inspect_response call, Scrapy Shell is opened and you can interact with the response to see what's happening.", "Another approach to debugging spiders is to use a regular Python debugger such as pdb or PuDB. I use ", " because it's quite a powerful yet easy-to-use debugger and all I need to do to activate it is to put this code in the line where I want a breakpoint:", "And when the breakpoint is reached, PuDB opens up a cool text-mode UI in your terminal that will bring\u00a0back fond memories from the old days of using the Turbo Pascal debugger.", "Take\u00a0a look:", "You can install PuDB using pip:", "Check out this video where our very own ", "\u00a0demonstrates a few tips on how to use PuDB: ", "There are certain scraping projects where you need your spiders to run for a long time. However, after a few hours of running, you might sadly see in the logs that one of your spiders had issues scraping specific URLs. You want to debug the spider, but you certainly don\u2019t want to run the whole crawling process again and have to wait until that specific callback is called for that specific URL so that you can start your debugger.", "Don't worry, the ", " from Scrapy CLI is here to save the day! You just need\u00a0to provide the spider name, the callback from the spider that should be used and the URL that you want to parse:", "In this case, Scrapy is going to call the parse_comments method from the blog spider to parse the blog.scrapinghub.com/comments/bla URL. If you don't specify the spider, Scrapy will search for a spider capable of handling this URL in your project based on the spiders' allowed_domains settings.", "It will then show you a summary of your callback's execution:", "You can also attach a debugger inside the method to help you figure out what's happening (see the previous tip).", "Inspecting page contents in browsers might be deceiving since their JavaScript engine could\u00a0render some content that the Scrapy downloader will not do. If you want to quickly check exactly how a page will look when downloaded by Scrapy, you can use these\u00a0commands:", ":", "Writing fail-proof software is nearly impossible. This\u00a0situation is\u00a0worse for web scrapers since\u00a0they deal with web content that is frequently changing (and breaking). It's better to accept that our spiders will eventually fail and to make sure that we have the tools to quickly understand why it's broken and to be able to fix it as soon as possible.", "Python tracebacks are great, but in some cases they don't provide us with enough information about what happened in our code. This is where post-mortem debugging comes into play. Scrapy provides the ", " command line option that fires a pdb session right where your crawler has broken, so you can inspect its context and understand what happened:", "If\u00a0your spider dies due to a fatal exception, the pdb debugger will open and you can thoroughly inspect its cause of death.", "And that\u2019s it for the Scrapy Tips from the Pros May edition. Some of these debugging tips are also available in ", ".", " what you'd like to see in the future since we're here to help you scrape the web more effectively. We'll see you next month!", "\u00a0"]},
{"tite": "Introducing the Crawlera Dashboard", "date": "June 01, 2016 ", "author": "Cecilia Haynes", "blog_data": [" We\u2019ve been rolling out a lot of ", ", ", ", and ", " lately, and we\u2019re continuing this trend by announcing the very first Crawlera Dashboard!", " is a smart downloader that allows you to crawl and scrape websites responsibly.\u00a0It rotates IP addresses and keeps track of which ones have been blocked by websites, ensuring that your crawls continue uninterrupted. Since Crawlera has always been a mainstay of Scrapinghub, we wanted to revamp its presentation to help you crawl the web and extract data more effectively.", "With that in mind, say hello to the dashboard that allows you to visualize how you are using Crawlera, to examine what sites and specific URLs you are targeting, and to manage multiple accounts.", "Read on as we walk you through the various features of the dashboard and show you how best to use it.", "The main benefits of the dashboard include seeing which websites are being crawled the most and understanding the responses you're getting via\u00a0Crawlera overall and for each individual account. You can also create and manage different Crawlera accounts from your dashboard, a feature which was not previously available.", "Having all of your accounts in one place allows you to more accurately manage and monitor your Crawlera use.", "In the past you could only see two global usage graphs: the number of requests you\u2019ve made per day for the last 30 days and per month for the last 12 months.\u00a0Now you can also view the most recent\u00a0requests performed by Crawlera in any of your accounts as well as\u00a0filter the state of each request (succeeded, failed, banned) per website. The graphs are also more comprehensive and representative of your Crawlera activity.", "In the future, we plan to develop support for managing the regions that you use for your Crawlera account(s) ", ". We'll also provide additional resources for our Enterprise clients, such as fine-tuned\u00a0management of a\u00a0dedicated pool of IP addresses.", "Here is the ", " to help you get started. We will also be rolling out a tutorial video, so keep your eyes peeled in the coming weeks.", "To\u00a0our longtime Crawlera users, we hope that you enjoy the enhanced features of the Crawlera Dashboard. To users new to ", ", we're thrilled that you're joining us during the release of this new interface.", "Please let us know what you think of the dashboard and feel free to reach out with further suggestions on ", ". Happy scraping!"]},
{"tite": "This Month in Open Source at Scrapinghub June 2016", "date": "June 15, 2016 ", "author": "Paul Tremberth", "blog_data": ["Welcome to This Month in Open Source at Scrapinghub! In this regular column, we share all the latest updates on our open source projects including Scrapy, Splash, Portia, and Frontera.", "If you\u2019re interested in learning more or even becoming a contributor, reach out to us by email at opensource@scrapinghub.com or on ", "For those who missed the big news, Scrapy 1.1 is live! It\u2019s the first official release that comes with Python 3 support, so you can go ahead and move your stack over.", "The major changes in this release since the ", " include improved HTTPS connections (with proxy support) and handling URLs with non-ASCII characters. Make sure you upgrade ", ".", "We\u2019re very grateful for the feedback we received during the release candidate phase. A huge thanks to all the reporters, reviewers and code/documentation contributors.", "If you find anything that\u2019s not working, please take a few minutes to ", " the issue(s) on ", ".", "Notable limitations still present in this release include:", "Splash 2.1 now lets you:", "If you\u2019re using the Scrapy-Splash plugin (formerly \u201cscrapyjs\u201d), we encourage you to upgrade to the ", ". It includes many goodies that makes integrating with Scrapy much easier. Check the ", " for details, especially the scrapy_splash.SplashRequest utility.", "We\u2019re thrilled to have 5 students this year:", "We\u2019d like to thank the ", " for again having Scrapinghub as a ", " this year!", "Scrapy relies on ", " and ", " for all the XPath and CSS selection awesomeness that we use each and every day at Scrapinghub. We learned that Simon Sapin, author of cssselect package, ", ". So we put ourselves forward and now cssselect is hosted under the ", ". Don\u2019t worry though, Simon is still involved! We\u2019re planning on fixing a few corner cases and maybe working on ", ". We\u2019ll definitely need assistance with this task, so please reach out if you\u2019re interested in helping out!", "We released ", " with support for dates in Danish and Japanese. It now handles dates with accents much better. The library is now working with the latest version of python-dateutil.", "Check the full ", ".", "This ", " is now hosted under Scrapinghub\u2019s organization on GitHub. It\u2019s a little helper library to convert JavaScript code into an XML tree. This means you can use XPath and CSS selectors to extract data (strings, objects, function arguments, etc.) from HTML-embedded JavaScript (this does not interpret it though). You\u2019d be amazed at how much valuable data is \u201chidden\u201d in JavaScript inside web pages.", "It\u2019s on ", " and is now Python 3-compatible.", "Check this ", " for an overview of what you can do with it and make sure to let us know what you think.", "We updated our w3lib library to handle non-ASCII URLs better, as part of adding Python 3 support to Scrapy 1.1. We recommend that you upgrade to the ", ".", "If you\u2019re using Scrapy 1.1, you\u2019re using ", " under the hood. Parsel is Scrapy Selectors as an independent package. There\u2019s a ", " that fixes the hiding of XPath exceptions.", "We\u2019ve made some changes to Slybot, the Portia crawler, that include:", "For Portia itself:", "Most of the recent developments have been taking place in the Portia beta.", "The big changes include:", "Try out the beta using the ", " branch.", " introduces improved crawling strategy, new logging and better test coverage.", " is a library to assist Scrapy spiders to do more optimal crawls. In its basic form, it\u2019s a collection of matchers and a mixin to narrow down the crawl to a specific date range. However, you can extend it to be applicable on any domain (URL paths, location filtering, etc). You can find more details about how it works and how you can create your own matchers in the ", ".", "This concludes the June edition of This Month in Open Source at Scrapinghub. We\u2019re always looking for new contributors, so if you\u2019re interested, feel free to explore our ", "."]},
{"tite": "Scraping Infinite Scrolling Pages", "date": "June 22, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to Scrapy Tips from the Pros! In this monthly column, we share a few tricks and hacks to help speed up your web scraping activities. As the lead Scrapy maintainers, we\u2019ve run into every obstacle you can imagine so don\u2019t worry, you\u2019re in great hands. Feel free to reach out to us on Twitter or Facebook with any suggestions for future topics.", "In the era of single page apps and tons of AJAX requests per page, a lot of websites have replaced \"previous/next\" pagination buttons with a fancy infinite scrolling mechanism. Websites using this technique load new items whenever the user scrolls to the bottom of the page (think Twitter, Facebook, Google Images). Even though ", " maintain that infinite scrolling provides an overwhelming amount of data for users, we\u2019re seeing an increasing number of web pages resorting to presenting this unending list of results.", "When developing our web scrapers, one of the first things we do is look for UI components with links that might lead us to the next page of results. Unfortunately, these links aren\u2019t present on infinite scrolling web pages.", "While this scenario might seem like a classic case for a JavaScript engine such as ", " or ", ", it\u2019s actually a simple fix. Instead of simulating user interaction with such engines, all you have to do is inspect your browser\u2019s AJAX requests when you scroll the target page and then re-create those requests in your Scrapy spider.", "Let's use ", " as an example and build a spider to get all the items listed on it.", "First things first, we need to understand how the infinite scrolling works on this page and we can do so by using the Network panel in the ", ". Open the panel and then scroll down the page to see the requests that the browser is firing:", "Click on a request for a closer look. The browser sends a request to ", " and then receives a JSON object like this in response:", "This is the information\u00a0we need for our spider. All it has to do is generate requests to \"/api/quotes?page=x\" for an increasing ", " until the ", " field becomes false. The best part of this is that we don't even have to scrape the HTML contents to get the data we need. It's all in a beautiful machine-readable JSON.", "Here is our spider. It extracts the target data from the JSON content returned by the server. This approach is easier and more robust than digging into the page\u2019s HTML tree, trusting that layout changes will not break our spiders.", "To further practice this tip, you can experiment with building a spider for our blog since it also uses infinite scrolling to load older posts.", "If you were feeling daunted by the prospect of scraping infinite scrolling websites, hopefully you\u2019re feeling a bit more confident now. The next time that you have to deal with a page based on AJAX calls triggered by user actions, take a look at the requests that your browser is making and then replay them in your spider. The response is usually in a JSON format, making your spider even simpler.", "And that\u2019s it for June! Please let us know what you would like to see in future columns by reaching out on ", ". We also recently released a ", ", so if you\u2019re stumped on what to scrape, take a look for some inspiration."]},
{"tite": "Introducing Portia2Code: Portia Projects into Scrapy Spiders", "date": "June 29, 2016 ", "author": "Ruairi Fahy", "blog_data": ["We\u2019re thrilled to announce the release of our latest tool, ", "!", "With it you can convert your ", " into ", " spiders. This means you can add your own functionality and use Portia\u2019s friendly UI to quickly prototype your spiders, giving you much more control and flexibility.", "A perfect example of where you may find this new feature useful is when you need to interact with the web page. You can convert your Portia project to Scrapy, and then use Splash with a custom script to close pop-ups, scroll for more results, fill in forms, and so on.", "Read on to learn more about using Portia2Code and how it can fit in your stack. But keep in mind that it only supports Portia 2.0 projects.", "First you need to install the ", " library using:", "Then you need to download and extract your Portia project. You can do this through the API:", "Finally, you can convert your project with:", "You can change the functionality as you would with a standard Scrapy spider. Portia2code produces spiders that extend from scrapy.CrawlSpider, the code for which is included in the downloaded project.", "The example below shows you how to make an additional API request when there\u2019s a meta property on the page named \u2018metrics\u2019.", "In this example, the extended spider is separated out from the original spider. This is to demonstrate the changes that you need to make when modifying the spider. In practice you would make changes to the spider in the same class.", "The site contains a meta tag. We join its content attribute with the base URL given by base_api_url to produce the full URL for the metrics.", "The domain of the base_api_url differs from\u00a0the rest of the site. This means we have to add its domain to the allowed_domains array to prevent\u00a0it from being filtered.", "We want to add an extra field to the items extracted, so the first step is to override the parse_item function. The most important part is to loop over parse_item in the superclass in order to extract the items.", "Next we need to check if the meta property \u2018metrics\u2019 is present. If that\u2019s the case, we send another request and store the current item in the request meta. Once we receive a response, we use the add_score method that we defined to add the score property from the JSON response, and then return the final item. If the property is not present, we return the item as is.", "This is a common pattern in Portia-built spiders. You would need to load some pages\u00a0in Splash, which greatly increases the time to crawl a site. This approach means you can download the additional data with a single small request without having to load scripts and other assets on the page.", "When you build a spider in Portia, the output consists largely of JSON definitions that define how the spider should crawl and extract data.", "When you run a spider, the JSON definitions are compiled into a custom Scrapy spider along with trained samples for extraction. The spider uses the ", " library with the trained samples to extract from similar pages.", "Portia uses unique selectors for each annotated element and builds an extraction tree that can use ", " to extract the relevant data.", "Here are the features that\u00a0we are planning to add in the future:", "We predict that Portia2Code will make ", " even more useful to those of you who need to scrape data fast and efficiently. Let us know how you will use the new ", " by ", " at us.", "Happy scraping!"]},
{"tite": "Scrapely: The Brains Behind Portia Spiders", "date": "July 07, 2016 ", "author": "Ruairi Fahy", "blog_data": ["Unlike ", ", the hunting spider that feeds on other spiders, our ", " feeds on data.\u00a0", "\u00a0in the spider world, we modeled our own creation after the intelligence and visual abilities of its arachnid namesake.", "Portia is our visual web scraping tool which is pushing the boundaries of automated data extraction. ", " and we welcome all contributors who are interested in collaborating. ", "since we\u2019re on the beta launch of Portia 2.0, so please jump in!", "You don't need a programming background to use Portia. Its web-based UI means\u00a0you can\u00a0choose the data you want by clicking\u00a0(annotating) elements on a webpage, as shown below. Doing this\u00a0", " uses to\u00a0", "When taking a look at the brains\u00a0of Portia, the first component you need to meet\u00a0is ", ".", " to extract structured data from HTML pages. While other commonly used libraries like ", " (Scrapy's Selector) and ", " use CSS and XPath selectors, Scrapely takes\u00a0annotated HTML samples as input.", "Other libraries work by building a DOM tree based on the HTML source. ", ". ", ". By building a representation of the page this way, Portia is able to handle any type of HTML no matter how badly formatted.", "To extract data, Portia employs machine learning using the ", " extraction method implemented by Scrapely. This is an example of supervised learning. You annotate an HTML page to create a set of pre-trained samples that guide the information extraction.", "Scrapely reads the streams of tokens from the unannotated pages, and looks for regions that are similar to the sample\u2019s annotations. To decide what should be extracted from new pages, it notes the tags that occur before and after the annotated regions, referred to as the prefix and suffix respectively.", "This approach is handy as you don\u2019t need a well-defined HTML page. It instead relies on the order of tags on a page. Another useful feature of this approach is that Scrapely doesn't need to find a 100% match, and instead looks for the best match. Even if the page is updated and tags are changed, Scrapely can still extract the data.", "More importantly, Scrapely will not extract the information if the match isn't similar enough. This approach helps to reduce false positives.", "Now that you better understand the inner workings on Portia, let\u2019s move to the front-facing portion.", "When you click elements on a page, Portia highlights them. You can give each field a name and chose how they\u2019re extracted. Portia also provides a live preview of what data will be extracted as you annotate.", "Portia passes these annotations to Scrapely, which generates a new HTML page that includes the annotations. You can see the annotated page\u2019s HTML in the screenshot below:", "Scrapely then compiles this information into an extraction tree, which looks like this:", "Scrapely uses the extraction tree to extract data from the page.", "The ", " finds the most likely HTML element that contains all the annotations.", "Next, the ", " looks through this element and applies each of the ", " from the tree, one for each field that you defined.", "After trying to match all extractors, Scrapely outputs either an item with the data extracted, or nothing if it couldn't match any elements that are similar enough to the annotations.", "This is how Scrapely works in the background to support the straightforward UI of Portia.", "You currently need to manually annotate pages with Portia. However, we are developing technologies that will do this for you. Our first tool, the ", ", finds list data on a page and automatically annotates the important information contained within.", "Another feature we\u2019ve began working on will let you automatically extract data from commonly found pages such as product, contact, and article pages. It works by using samples created by Portia users (and our team) to build models so that frequently extracted information will be automatically annotated.", "And that\u2019s ", " in a nutshell! Or at least the machine learning brains\u2026 In any case, I hope that you found this an informative peek into the underbelly of our visual web scraping tool. We again invite you to contribute to ", " since it\u2019s completely open source.", " what else you\u2019d like to learn about how Portia works or our advances in automated data extraction."]},
{"tite": "How Web Scraping is Revealing Lobbying and Corruption in Peru", "date": "March 09, 2016 ", "author": "Carlos Pe\u00f1a", "blog_data": ["Paid political influence and corruption continue to be\u00a0major issues in a\u00a0surprising number of\u00a0countries throughout the world. The recent \"", "\" in Brazil, in which officials from the state company Petrobras were accused of taking bribes from construction companies in exchange for contracts financed with taxpayer money, is a reminder of this. It has been suggested that ", " and ", ", a non-governmental organization that monitors corruption, has strongly recommended that governments regulate lobbying.", "I live in Peru and with corruption scandals regularly making headlines, I was curious to see how Peruvian officials fared and to examine the role that lobbyists play in my government.", "The Peruvian Law of Transparency requires government institutions to maintain a website where they publish the list of people that visit their offices. Every visitor must register their ID document number, the reason they\u2019re visiting, and the public servant whom they are visiting along with the time of entrance, time of exit and date of visit. You can find all of this information on ", ".", "While almost all institutions have their visitor list available online, they all suffer from the same model of a broken user interface. One of the major issues with these websites is that you\u2019re not able to search for visitors, you can only browse who visited on a particular day. If you\u2019re looking for a known lobbyist, you would need to visually scan several pages as there can be up to 400 visitors per day in some institutions.", "Obviously this method of search is time-consuming, tedious, boring, and it\u2019s easy to miss the person that you are searching for. This problem is compounded when you want to search for a particular person visiting more than one public institution.", "To help journalists track lobbyists, I started ", ". Manolo is a simple search tool that contains the visitor records of several government institutions in Peru. You can type any name and search for any individual across 14 public institutions in a matter of milliseconds.", "Unfortunately Peruvian institutions do not provide their visitor data in a structured way. There\u2019s no API to fetch this data in a machine readable format, so the only way to get the information is by scraping their websites and then mining the data.", "Manolo consists of a bunch of web scrapers written with Scrapinghub\u2019s popular ", ". The Scrapy spiders crawl the visitor lists once a week, extracting structured data from the HTML content and storing it in a PostgreSQL database. Elasticsearch indexes this data and users can then perform searches via a Django-based web UI, available on the ", ".", "So far, my friend ", " and I have created 14 spiders which have scraped more than 2 million records. You can find the source code for the spiders and the search tool ", ".", "I have always aimed to provide this search tool online for anyone to use free of charge. At the beginning, I was just hoping that I could convince one or two journalists to use Manolo when searching for information.", "You can imagine my suprise when two national newspapers (El Comercio and Diario Exitosa), one TV station (Latina) and two news web portals (Utero.pe and LaMula) became active users of Manolo.", "One of the most impressive uses of Manolo was when ", " that the legal representative\u00a0of a company that signed numerous and extremely profitable construction contracts with the government was a regular visitor to the President's residence building. According to the data scraped by Manolo, this representative\u00a0visited some of the closest allies of the president 33 times. He was already ", " because his company pocketed the money from the contracts and never built anything.", "The discovery of the 33 visits made big headlines in 4 nationwide newspapers. Soon after, journalists from a variety of media outlets started using Manolo to find suspicious visits of known lobbyists to ministries responsible for the construction of public infrastructure. They even found that lobbyists involved in the famous \"", "\" corruption scandal were also regular visitors of Peruvian institutions.", "\"", "\"", "Government corruption through lobbying is shockingly prevalent. Transparency is the solution, but even publicly available information can be convoluted and difficult to access. And that\u2019s where web scraping comes in. Open data is what we should all be striving towards and hopefully others will follow ", ".", "Our use of ", " can easily be applied to any other situation where you are looking to scrape websites for information on government officials (or anyone else, for that matter). Even those without a technical background can scrape websites using ", ", our\u00a0free and open source visual web scraping tool."]},
{"tite": "Scrapy Tips from the Pros: March 2016 Edition", "date": "March 23, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to the March Edition of ", "! Each month we\u2019ll release a few tips and hacks that we\u2019ve developed to help make your Scrapy workflow go more smoothly.", "This month we\u2019ll cover how to use a ", " with the CookiesMiddleware to get around websites that won\u2019t allow you to crawl\u00a0multiple pages at the same time using the same cookie. We\u2019ll also share a handy tip on how to use multiple fallback\u00a0XPath/CSS expressions with ", " to get data from websites more reliably.", "**Students reading this, we are participating in ", " and some of our project ideas involve Scrapy! If you're interested, take a look at ", " and remember to ", "!", "If you are not a student, please share with your student friends. They could get a summer stipend and we might even hire them at the end.**", "Websites that store your UI state on their server's sessions are a pain to navigate, let alone scrape. Have you ever run into websites where one tab affects the other tabs open on the same site? Then you\u2019ve probably run into this issue.", "While this is frustrating for humans, it\u2019s even worse for web crawlers. It can severely hinder a web crawling session. Unfortunately, this is a common pattern for ASP.Net and J2EE-based websites. And that's where ", " come in.\u00a0While the cookiejar is not a frequent need, you\u2019ll be so glad that you have it for those unexpected cases.", "When your spider crawls a website, Scrapy automatically handles the cookie for you, storing and sending it in subsequent requests to the same site. But, as you may know, Scrapy requests are asynchronous. This means that you probably have multiple requests being handled concurrently to the same website while sharing the same cookie. To avoid having requests affect each other when crawling these types of websites, you must set different cookies for different requests.", "You can do this by using a\u00a0", " to store separate\u00a0cookies for different pages in the same website. The cookiejar is just a key-value collection of cookies\u00a0that Scrapy keeps during the crawling session.\u00a0You just have to define a unique identifier for each of the cookies that you want to store\u00a0and then use that identifier when you want to use that specific\u00a0cookie.", "For example, say you want to crawl multiple categories on a website, but this website stores the data related to the category that you are crawling/browsing in the server session. To crawl the categories concurrently, you would need to create a cookie for each category by passing the category name as the identifier to the cookiejar meta parameter:", "Three different cookies will be managed in\u00a0this case\u00a0(\u2018photo\u2019, \u2018videogames\u2019 and \u2018tablets\u2019). You can create a\u00a0new cookie whenever you pass a nonexistent key as the cookiejar meta value (like when a category name hasn\u2019t been visited yet). When the key we pass already exists, ", " uses the respective cookie for that request.", "So, if you want to reuse the cookie that has been used to crawl the 'videogames' page, for example, you just need to pass 'videogames' as the unique key to the cookiejar. Instead of creating a new cookie, it will use the existing one:", " are useful when you need to accomplish more than simply populating a dictionary or an Item object with the data collected by your spider. For example, you might need to add some post-processing logic to the data that you just collected. You might be interested in something as\u00a0simple as capitalizing every word in\u00a0a title to more complex operations. With an ItemLoader, you can decouple this post-processing logic from the spider in order to have a more maintainable design.", "This tip shows you how to add extra functionality to an Item Loader. Let\u2019s say that you are crawling Amazon.com and extracting the price for each product. You can use an Item Loader to populate a ProductItem object with the product data:", "This method works pretty well, unless the scraped product is a deal. This is because Amazon represents deal prices in a slightly different format than regular prices. While the price of a regular product is\u00a0represented like this:", "The price of a deal is shown slightly\u00a0differently:", "A good way to handle\u00a0situations like this is\u00a0to add a fallback rule for the price field in the Item loader. This is a\u00a0rule that is\u00a0applied only if the previous rules for that field have failed. To accomplish this with the Item Loader,\u00a0you can\u00a0add a ", " method:", "As you can see, the ", " method will use the CSS rule if there are no previously collected values for that field. Now, we can change our spider to use AmazonItemLoader and then add the fallback CSS rule to our loader:", "This tip can save you time and make your spiders much more robust. If one CSS rule fails to get the data, there will be other rules that can\u00a0be applied which will\u00a0extract the data you need.", "If Item Loaders are new to you, ", ".", "And there you have it! Please share any and all problems that you\u2019ve run into while web scraping and extracting data. We\u2019re always on the lookout for new tips and hacks to share in our Scrapy Tips from the Pros monthly column. Hit us up on ", " or ", "and let us know if we\u2019ve helped your workflow.", "And if you haven\u2019t yet, give ", ", our open source visual web scraping tool, a try. We know you're attached to ", ", but it never hurts to experiment with your stack ;)", "Please ", " by Friday, March 25!"]},
{"tite": "Join Scrapinghub for Google Summer of Code 2016", "date": "March 14, 2016 ", "author": "Cecilia Haynes", "blog_data": ["It\u2019s that time of year again! ", " are upon us and we welcome any and all students who are interested in open source and web scraping. For those just hearing about this program, ", " provides ", " (", " for a successfully completed project) to students who are interested in writing code for open source projects. This is a full-time commitment and a great way to get involved in the ", ".", "This is our third year participating in this prestigious program and we're excited to announce projects around ", ", ", ", ", ", and ", ". We feature projects that range from \u201cEasy\u201d to \u201cAdvanced\u201d and we're happy to have students with different levels of technical skills.", "Student applications are accepted from ", " and students accepted to our projects will be announced on April 22, 2016. We're very excited to mentor Python enthusiasts!", " so that we can make this process as easy and straightforward as possible.", "Here are our available open source project ideas.\u00a0Browse around:", " is our Python-based web scraping framework.", ":", ":", ":", " is our visual web scraper. This tool allows you to get the data you need from websites without needing to write a single line of code.", ":", " is a headless browser that executes JavaScript for people crawling websites.", " is a web crawling framework consisting of crawl frontier and distribution/scaling primitives. It allows you to build a large scale online web crawler. Frontera takes care of the logic and policies to follow during a crawl. It stores and prioritises links extracted by the crawler to decide which pages to visit next and is capable of doing this in a distributed manner.", ":", ":", "Scrapinghub is a sub-organization under the umbrella of the ", " so please take a moment to read through their guidelines and expectations.", "This is a great opportunity to not only hone your coding skills on some popular open source projects, but you might even get a job out of it. We\u2019ve actually hired two of our previous participants. You never know what might come of participating in ", "!", "\u00a0"]},
{"tite": "Web Scraping to Create Open Data", "date": "March 30, 2016 ", "author": "Llu\u00eds Esquerda", "blog_data": ["I began as most would; I searched the internet and found a library named ", " that was somehow able to retrieve station and bike information. This was my first time using Python and, after some investigation, I learned what the code was doing: accessing the official website, parsing the JavaScript that generated their buggy map and giving back a nice chunk of Python objects that represented bike share stations.", "This I learned\u00a0was called web scraping. It was like I had figured out\u00a0a magic trick that would allow me to always be able to access the data I needed without having\u00a0to rely on faulty websites.", "Shortly after, I launched OpenBicing, an Android app for the local bike sharing system in Barcelona, together with\u00a0a\u00a0backend that used\u00a0", ". I also shared\u00a0a public API that provided this information so that nobody else had to do the dirty work ever again.", "Since other cities were having the same problem, we expanded the scope of the project worldwide and renamed it\u00a0", ". That was 6 years ago.", "To date, ", " is the most comprehensive and widely used ", " for bike sharing information, with support for over 400 cities worldwide. Our API processes around 10 requests per second and we scrape each of the 418 feeds about\u00a0every three minutes. Making our ", " available for anyone to contribute has been crucial in maintaining and adding coverage for all of the supported systems.", "We are usually regarded as \"", "\" even though less than 10% of our feeds come from properly licensed, documented and machine-readable feeds. The remaining 90% is composed of 188 feeds that are machine-readable, but not licensed nor documented and 230 that are entirely maintained\u00a0by scraping HTML pages.", " (North American BikeShare Association) recently published ", " (General Bikeshare Feed Specification). This is clearly a step in the right direction, but I can\u2019t help but look at the almost 60% of services we currently support through scraping and wonder how long it will take the\u00a0remaining\u00a0organizations to release their information, if ever. This is even more the case considering these numbers aren\u2019t even taking into account ", " coverage.", "Over the last few years there has been a progression by transportation companies and city councils toward providing their information as \"open data\". ", " encourages EU member states to release information regarding public services.", "Yet, in most cases, there\u2019s little action in enforcing Public Private Partnerships (PPP) to release their public information under a non-restrictive license or even to transfer ownership of the data to city councils to be included in their open data portals.", "When I\u00a0started this project, I\u00a0sought\u00a0to make a difference in Barcelona. Now you can find tons of\u00a0bike sharing apps that use\u00a0our API on\u00a0all major platforms. It doesn't matter that these\u00a0are not our own apps. They are solving the same problem we were trying to fix, so their success is our success.", "Besides popular apps like Moovit or CityMapper, there are many ", " out there, some of which are published under free software licenses. Ideally, a city council could create a customization of any of these apps for their own use.", "Most official applications for bike sharing systems have terrible ratings. The core business of transportation companies is running a service, so they have no real motivation to create an\u00a0engaging UI\u00a0or innovate further. In some cases, the city council does not even own the rights to the data, being completely at the mercy of the company providing the transportation service.", "When providing public services, city councils and companies often get lost in what they should offer as an aid\u00a0to the service. They focus on a nice map or a flashy\u00a0application, rather than providing the data behind these service aids. Maps, apps, and websites have a limited focus and usually serve a single purpose. On the other hand, data is malleable and the purest form of representation. While you can\u2019t create something new from looking and playing with a static map (except, of course, if you scrape it), data can be used to create countless different iterations. It can even provide a bridge that will allow anyone to participate, improve and build on top of these public services.", "At this point, you might wonder why I care so much about ", ". To me it\u2019s not about bike sharing anymore. ", " is just too good of an open data metaphor, a simulation in which public information is freely accessible to everyone. It shows the benefits of open data and the deficiencies that arise from the lack thereof.", "We shouldn't have to create open data by scraping websites. This information should be already available, easily accessed and provided in a machine-readable format from the original providers, be they city councils or transportation companies. However, until there's another option, we\u2019ll always have scraping."]},
{"tite": "This Month in Open Source at Scrapinghub March 2016", "date": "March 16, 2016 ", "author": "Paul Tremberth", "blog_data": ["Welcome to This Month in Open Source at Scrapinghub! In this monthly column, we share all the latest updates on our open source projects including ", ", ", ", ", ", and ", ".", "If you\u2019re interested in learning more or even becoming a contributor, reach out to us by email at opensource [@] scrapinghub.com or on ", ".", "The big news for Scrapy lately is that Python 3 is now supported for the majority of use cases, the exceptions being FTP and email. We are very proud of the work done by our community of users and contributors, both old and new. It was a long ride, but we\u2019re finally here. You all made it happen!", "Check out the ", " that we packed into this release and please pay attention to the following backward incompatible changes:", "Scrapy 1.1 is not officially released yet (we\u2019re aiming for the end of March), but ", " is available for you to test. It\u2019s the last mile, so we\u2019d really appreciate if you could report any issues that you may have with Scrapy 1.1.0rc3 so that we can do our best to fix them.", "Oh, and for those who want to stay on stabler (and less-shiny) grounds, we released Scrapy 1.0.5 with a few bug fixes.", "! (Actually we\u2019re already at v2.0.3 and 2.1 will be released soon)", "Check out the ", ".", "This is our third year of participating in ", " and we\u2019ve got plenty of ", " for Scrapy, Portia, Splash, and Frontera. This program is open to students who are interested in working on open source projects with professional mentors. We\u2019ve actually hired two of our previous participants, so you might even get a job out of this opportunity!", "Scrapinghub is running under the ", ", so please take the time to read through their guidelines before applying.", "Applications opened on ", ". We\u2019re looking forward to working with you!", "Changes from 0.3.1 (last October):", "Note that 0.3.4 forces python-dateutil before or at 2.4.2. It doesn\u2019t work with python-dateutil 2.5.", "The beta version of Portia 2.0 is out! This major release comes with a completely overhauled UI and plenty of new fancy tricks (including multiple item extraction) to help make automatic data extraction even easier. Stay tuned for the official release and in the meantime, try out Portia 2.0 beta and let us know what you think.", "The other big news in the Portia camp is the ", ". For those affected, we offer a ", " so that you don't need to\u00a0lose any of your work.", "We released ", " in January, however, we feel it deserved more coverage.", "Let us know what you think! (use ", " from ", ")", "The Scrapinghub command line client, Shub, has long lived as merely a fork of scrapyd-client, the command line client for scrapyd. Last January, we freed it in the form of Shub v2.0! This release brings many new features and major improvements in usability.", "If you work with multiple Scrapinghub projects, or even multiple API keys, you were probably irritated about the amount of repetition you needed to put into your ", " file.", "Shub v2.0 now reads from its own configuration file, ", ", where you can configure different projects or keys on a single link. You don\u2019t need to worry about migrating your configuration as Shub will automatically generate new configuration files from your old ones. To avoid storing your API keys in version control, you can run ", " which will take your API key and create a configuration file, ", ", in your home directory. Shub will read this file by default, so you don\u2019t need to specify the API key in future deployments.", "If you\u2019re new to deploying your projects to Scrapinghub, or have just started a new project, running ", " in the project folder will guide you through a wizard and automatically generate your configuration files. No need to copy-and-paste from our web interface anymore!", "Not only have we worked on deploying projects and onboarding new users. Shub provides a much nicer shell experience now, with a dedicated help page for every command (", ") and extensive error messages. If you\u2019re not used to installing Python packages from the command line, our new stand-alone binaries (including for Windows) might be for you.", "A particularly long-awaited new feature is the addition of viewing log entries, or items, live as they are being scraped. Just run ", " and watch your spiders at work. Shub will let you know the JOBID when you schedule a run via ", ". Alternatively,you can simply look it up on the web interface.", "Find the ", ". You can install ", " via ", ", or ", ".", "Don\u2019t forget to tell us what you think!", "Thus concludes the March edition of This Month in Open Source at Scrapinghub. We\u2019re always looking for new contributors so please explore our ", ". And remember, students, there are a variety of projects available for our open source projects, so ", " on Google Summer of Code 2016."]},
{"tite": "Mapping Corruption in the Panama Papers with Open Data", "date": "April 06, 2016 ", "author": "Cecilia Haynes", "blog_data": ["We are at a point in the digital age where corruption is increasingly difficult to hide. ", " are abundant and shocking.", "We rely on whistleblowers for many of these leaks. They have access to confidential information that\u2019s impossible to obtain elsewhere. However, we also live in a time where data is more open and accessible than at any other point in history. With the rise of Open Data, people can no longer ", ". Nothing is ever truly deleted from the internet.", "It might surprise you how many insights into corruption and graft are hiding in plain sight through openly available information. The only barriers are clunky websites, inexperience in data extraction, and unfamiliarity with data analysis tools.", "We now collectively have the resources to produce our own ", ". Not just as one offs, but as regular accountability checks to those in situations of power. This is especially the case if we combine our information to create further links.", "One example of this democratization of information is a recent project in Peru called ", " and its intersection with the Panama Papers. Manolo used the webscraping of open data to collect information on Peruvian government officials and lobbyists.", "Manolo is a web application that uses ", " to extract records (2.2 million so far) of the visitors frequenting several Peruvian state institutions. It then repackages the data into an easily searchable interface, unlike the government websites.", "Peruvian journalists frequently use Manolo. It has even helped them ", " by tracking the visits of construction company representatives who are currently under investigation to specific government officials.", "Developed by Carlos Pe\u00f1a, a Scrapinghub engineer, Manolo is a prime example of what a private citizen can accomplish. By opening access to the Peruvian media, this project has opened up a much needed conversation about transparency and accountability in Peru.", "With leaks like the Panama Papers as a starting point, web scraping can be used to build datasets to discover wrongdoing and to call out corrupt officials.", "For example, you could cross-reference names and facts from the Panama Papers with the data that you retrieve via web scraping. This would give you more context and could lead to you discovering more findings.", "We actually tested this out ourselves with Manolo. One of the names found in the Panama Papers is ", ", currently a Peruvian congressman. We found his name in Manolo\u2019s database since he visited the Ministry of Mining last year.", "According to the Peruvian news publication ", ", Acu\u00f1a wanted to use Mossack Fonseca to reactivate an offshore company that he could use to secure construction contracts with the Peruvian state. As a congressman, this is illegal. In Peru, there are efforts to investigate Virgilio Acu\u00f1a and his brother, who recently ran for president, for money laundering.", "Another name reported in the Panama Papers by ", " was Jaime Carbajal Per\u00e9z, a close associate of former Peruvian president Alan Garc\u00eda.", "Ojo P\u00fablico states that in 2008, Carbajal, along with colleague Percy Uriarte and others, bought the offshore company Winscombe Management Corp. from Mossack Fonseca. Carbajal and Uriarte own a business that sells books to state-run schools. Further plot twist is that the third owner of the bookstore, Jos\u00e9 Antonio Chang, was the Minister of Education from 2006 to 2011 and the Prime Minister from 2010 to 2011.", "A quick search of the Manolo database reveals that ", " visited the Peruvian Ministry of Education 45 times between 2013 and 2015. ", ", another Peruvian news outlet, reported that the company led by Carbajal illegally sold books to the Peruvian government in 2010. He used a front company for these transactions since he was forbidden by law to engage in contracts with the state due to his close association with the former president.", "Data from previous leaks such as ", ", ", " and ", "\u00a0have been released publicly. In many cases, the data has been indexed and catalogued, making it easy for you to ", ". And now we\u2019re just waiting on the full data dump from the ", ".", "You can use information retrieval techniques to dig deeper into the leaks.You could find matching records, sift through the data by tagging specific words, parts of speech, or phrases, and identify different entities like institutions or people.", "If the information is not readily available in a convenient database, then you can always explore open data yourself:", "We actually offer platform integrations with Machine Learning programs like ", " and ", ". We\u2019re looking into integrating more data tools later this year, so keep an eye out!", "Data and corruption are everywhere and they can both seem difficult to access.That\u2019s where web scraping comes in. We are a point where citizens have the tools necessary to hold elected officials, businesses, and folks in power accountable for illegal and corrupt actions. By increasing transparency and creating further links between information leaks and readily available data, we can remove the loopholes where firms like Mossack Fonseca exist.", "Frameworks like ", " are great for those who know how to code, but there shouldn\u2019t be a barrier to acquiring data. Journalists who wish to take advantage of these vast sources of information can use visual web scrapers like ", " to get the data for current and future investigations."]},
{"tite": "Machine Learning with Web Scraping: New MonkeyLearn Addon", "date": "April 14, 2016 ", "author": "Cecilia Haynes", "blog_data": ["We deal in data. Vast amounts of it. But while we\u2019ve been traditionally involved in providing you with the data that you need, we are now taking it a step further by helping you analyze it as well.", "To this end, we\u2019d like to officially announce the ", "\u00a0for\u00a0", ". This feature will bring machine learning technology to the data that you extract through ", ". We also offer a ", " so you can use it on your own platform.", " is a classifier service that lets you analyze text. It provides machine learning capabilities like categorizing products or sentiment analysis to figure out if a customer review is positive or negative.", "You can use MonkeyLearn as an addon for Scrapy Cloud. It only takes a minute to enable and once this is done, your items will flow through a pipeline directly into MonkeyLearn's service. You specify which field you want to analyze, which MonkeyLearn classifier to apply to it, and which field it should output the result to.", "Say you were involved in the production of ", " and you\u2019re interested in how people reacted to your high budget movie. You could use Scrapy to track mentions of this film across the web and then use MonkeyLearn to\u00a0perform sentiment analysis on the samples that you collect. But don't get too excited because you might not like the results of your search...", "There are so many ways that you can use this addon with our platform, so we\u2019ll be featuring a series of tutorials that will help you make the most out of this partnership.", "MonkeyLearn provides ", " that are already ready to go or you can ", " by training a custom machine learning model.", "For example, using traditional sentiment analysis on comments filled with trolls would return a 100% negative rating. To develop a \u201cTroll Finder\u201d you would need to create a custom model with a higher tolerance for the extreme negativity. You could create categories like\u00a0", "\u00a0for further categorization. Check out ", " to help you through this task.", "Before you get started with the MonkeyLearn addon on ", ", you first need to sign up for the ", ". They offer a free account, so you don\u2019t need to worry about the cash monies. Once you\u2019ve signed up, you\u2019ll be taken to your dashboard:", "Click the \u201cExplore\u201d option in the top menu to check out the whole range of ready-made\u00a0classifiers that you can apply to the scraped data. There are a ton of different options to choose from including sentiment analysis for product reviews, language detectors, and extractors for useful data such as phone numbers and addresses.", "Choose the classifier that you\u2019re interested in and make a note of its ID. You can find the ID in the URL:", "And now that you\u2019re all set on the MonkeyLearn side, it\u2019s time to head back over to Scrapy Cloud.", "You can access the MonkeyLearn addon through your dashboard. Navigate to Addons", "Setup:", "Enable the addon and click Configure:", "Head down to Settings:", "To configure the addon, you need to set your MonkeyLearn API key, specify the classifier you want to use and the field in which you want the result to be stored. You\u2019ll need the classifier ID you chose earlier from the MonkeyLearn platform.", "MonkeyLearn reads the content from the ", " you've specified, performs the classification task on the data, and returns the result of the classification/analysis in\u00a0the field that you defined as ", ".", "For example, in order to detect the category of a movie based on the title, you would need to add the ID from the module you want to use in the first text box. In the second text box\u00a0you would list your authorization token and\u00a0the item field you want to analyze (", ", in our case) in the third text box. In the fourth text box\u00a0you would list the name of the field that is going to store the results from MonkeyLearn.", "And you\u2019re all done! Locked and loaded and ready to go with MonkeyLearn.", "The MonkeyLearn addon is a part of Scrapy Cloud, so you can use it with your ", " spiders. Scrapy is also open source, so you can easily run it on your own system.", "The addon means you don\u2019t need to worry about learning MonkeyLearn\u2019s API and how to route requests manually. If you need to use MonkeyLearn outside of Scrapy Cloud, you can use the ", " for the same purpose.", "We\u2019re really excited about this integration because it is a huge step in closing the gap between data acquisition and analysis.", "MonkeyLearn offers a range of text analysis services including:", "We\u2019ll delve into what you can do with each of these tools in future tutorials. For now, feel free to experiment and explore this integration in your web scraping projects.", "Data and textual analysis is more efficient by combining MonkeyLearn's machine learning capabilities with our data extraction platform. Whether you are using this for personal projects (tracking and monitoring advance reviews for Captain America: Civil War [", "]) or for professional tasks, we're excited to see what you come up with.", "Keep your eyes peeled, the first tutorial will walk you through using the Retail Classifier with the MonkeyLearn addon. ", " for Scrapy Cloud and for Monkeylearn and give this addon a whirl."]},
{"tite": "Scraping Websites Based on ViewStates with Scrapy", "date": "April 20, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to the April Edition of ", ". Each month we\u2019ll release a few tricks and hacks that we\u2019ve developed to help make your Scrapy workflow go more smoothly.", "This month we only have one tip for you, but it\u2019s a doozy! So if you ever find yourself scraping an ASP.Net page where you need to submit data through a form, step back and read this post.", "Websites built using ASP.Net technologies are typically a nightmare for web scraping developers, mostly due to the way they handle forms.", "These types of websites usually send state data in requests and responses in order to keep track of the client's UI state. Think about those websites where you register by going through many pages while filling your data in HTML forms. An ASP.Net website would typically store the data that you filled out in the previous pages in a hidden field called \"__VIEWSTATE\" which contains a huge string like the one shown below:", "This is a Base64 encoded string representing the client UI state and contains the values from the form. This setup is particularly common for web applications where user actions in forms trigger POST requests back to the server to fetch data for other fields.", "The __VIEWSTATE field is passed around with each POST request that the browser makes to the server. The server then decodes and loads the client's UI state from this data, performs some processing, computes the value for the new view state based on the new values and renders the resulting page with the new view state as a hidden field.", "If the __VIEWSTATE is not sent back to the server, you are probably going to see a blank form as a result because the server completely lost the client\u2019s UI state. So, in order to crawl pages resulting from forms like this, you have to make sure that your crawler is sending this state data with its requests, otherwise the page will not load what it's expected to load.", "Here\u2019s a concrete example so that you can see firsthand how to handle these types of situations.", "The scraping guinea pig today is ", ". This website lists quotes from famous people and its search page allows you to filter quotes by author and tag:", "A change in the ", " field fires up a POST request to the server to fill the ", " select box with the tags that are related to the selected author. Clicking ", " brings up any quotes that fit the tag from the selected author:", "In order to scrape these quotes, our spider has to simulate the user interaction of selecting an author, a tag and submitting the form. Take a closer look at each step of this flow by using the ", " that you can access through your browser's Developer Tools. First, visit ", " and then load the tool by pressing F12 or Ctrl+Shift+I (if you are using Chrome) and clicking on the Network tab.", "Select an author from the list and you will see that a request to \"/filter.aspx\" has been made. Clicking on the resource name (filter.aspx) leads you to the request details where you can see that your browser sent the author you've selected along with the __VIEWSTATE data that was in the original response from the server.", "Choose a tag and click Search. You will see that your browser sent the values selected in the form along with a __VIEWSTATE value different from the previous one. This is because the server included some new information in the view state when you selected the author.", "Here are the steps that your spider should follow:", "Here's the spider I developed to scrape the quotes from the website, following the steps just described:", " is done by Scrapy, which reads start_urls and generates a GET request to /search.aspx.", "The parse() method is in charge of ", ". It iterates over the ", " found in the first select box and creates a ", " to /filter.aspx for each ", ", simulating if the user had clicked over every element on the list. It is important to note that the parse() method is reading the __VIEWSTATE field from the form that it receives and passing it back to the server, so that the server can keep track of where we are in the page flow.", " is handled by the parse_tags() method. It's pretty similar to the parse() method as it extracts the ", " listed and creates POST requests passing each ", ", the ", " selected in the previous step and the __VIEWSTATE received from the server.", "Finally, in ", " the parse_results() method parses the list of quotes presented by the page and generates items from them.", "You may have noticed that before sending a POST request to the server, our spider extracts the pre-filled values that came in the form it received from the server and includes these values in the request it's going to create.", "We don't need to manually code this since ", " provides the ", " method. This method\u00a0reads the response object and creates a ", "\u00a0that\u00a0automatically includes all the pre-filled values from the form, along with\u00a0the hidden ones. This is how our spider's parse_tags() method looks:", "So, whenever you are dealing with forms containing some hidden fields and pre-filled values, use the ", " method because your code will look much cleaner.", "And that\u2019s it for this month! You can read more about ", ". We hope you found this tip helpful and we\u2019re excited to see what you can do with it. We\u2019re always on the lookout for new hacks to cover, so if you have any obstacles that you\u2019ve faced while scraping the web, please let us know.", "Feel free to reach out on ", " or ", " with what you\u2019d like to see in the future.", "\u00a0"]},
{"tite": "A (not so) Short Story on Getting Decent Internet Access", "date": "April 27, 2016 ", "author": "Agustin", "blog_data": ["This is a tale of trial, tribulation, and triumph. It is the story of how I overcame obstacles including an inconveniently placed grove of", "alyptus trees, armed with little more than a broom and a pair of borrowed binoculars, to establish a stable internet connection.", "I am a remote worker and accessibility to stable internet ranks right up there with food and shelter. With the rise of digital nomads and distributed companies, I am sure many of you can identify with the frustration of nonexistent or slow internet. Read on to learn more than you ever thought you would about how to MacGyver your very own homemade ", " (FTTH) connection.", "This adventure took place in September 2015. My wife and I decided to move from Montevideo, Uruguay to our hometown Durazno. We bought a cozy house on the outskirts of the city, about 3 km away from the downtown.", "The first thing we checked was availability to our main ISP, Antel. They had 2 options:", "I walked around the area in an attempt\u00a0to find a ", " (NAP).\u00a0The NAP is a part of the ISPs infrastructure and the presence of one indicates that the zone is covered with FTTH. I\u00a0know how\u00a0NAPs look, specifically the ones used by Antel, so it was a fast way to determine if I would have access to\u00a0FTTH coverage. To my surprise I was unable to find one and confirmed via ", " that there were none available.", "Copper lines were also a bust because my house was too far from the closest node. Besides, the speed was close to a 56K dial-up modem and there were many disconnection issues. My neighbour had the same service and cancelled it. He said it was useless and had trouble despite only using it for email and day-today web access.", "I then bought a dongle and tested Antel\u2019s ", " service with both my laptop and smartphone. The results were appalling. The upload speed was 128-256Kbps and was by far the biggest let down.", "3G coverage was also terrible and I experienced many disconnections. This put mobile technology off the table.", "In case you care to see details, ", " are the speed tests I performed onsite.", "I looked deeper into Antel's FTTH deployed zone and saw that it was only 1.5km away in a straight line. I figured it was worth a shot to try creating a wireless Point-to-Point link.", "Line of Sight: I needed to put the 2 radios in a place where they could see each other. Since I was going to use an unlicensed frequency (2.4Ghz or 5Ghz), this was a difficult requirement.", "The second necessity was determining the ", ". Basically you need to have the two radios high enough that you can ", ".", "The most destructive force for radio electrical signals is water (radio signals just don\u2019t get through easily or at all) and\u00a0trees are full of water. I realized that the Eucalyptus grove (labeled Major issue in the image below) was right in the path of the signal path. Diving into Google Earth and Google Maps gave me some insight on dodging eucalyptus forest and other troublesome surface heights. I then stood on the roof of my house with some borrowed binoculars to help determine a placement for the remote endpoint.", "Through the binoculars, I spotted a white balloon that looked like some kind of tank in the FTTH zone. I drove up and realized that it was a water tank situated near a large warehouse for trucks. It made\u00a0sense to put the radio on top of this tank\u00a0to benefit from its\u00a0height (labeled \"FTTH\" in the image above) for the Fresnel Zone clearance.", "I rang up the owner and arranged a meeting with him to explain the situation. I found it difficult to explain what I needed and he had no idea what I was talking about, so he took some convincing.", "The owner\u2019s main concern was that I may interfere with his internet access. I reassured him and explained that I would be using a service different to the one he was using. He had nothing to worry about as I only needed the height of his tower.", "He agreed and let me perform some tests to check the viability of my plan.", "Google Earth revealed:", "The major concern was that at halfway point, the surface height of the land was 103m and I needed at least ", " of ", " = 107.4m", "There was nothing I could do to boost the height of the tank, so I had to hope for the best and test out the connection.", "I borrowed 2 Ubiquiti ", "s (5Ghz band) to test the link. I knew that if I could establish a link with these smaller capacity devices, then I was on the right track.", "At the FTTH point (the water tank) I placed a Nano.", "I put the other one onto a broomstick. I then wandered around my property (including the roof of the house and the warehouse) with this makeshift aligning tool, waving it around and trying to find the best connection spot.", "I was shocked when I finally managed to get a one-way 90Mbps while holding the broom and thought, this might actually work!", "I used iperf to test the connection. It runs in a server/client model and I ran the server at the \u201cFTTH\u201d point and then the client at the \u201cNo INET\u201d point.", "Here are the iperf results:", "I decided to commit to the Point-to-Point link as the solution to my internet woes. The next step was figuring out the right pair of antennas to get the most out of this link.", "I picked the ", ", one of the newest products on the Airmax line. I chose it because it was a MIMO 2x2 radio (compared to the LiteBeam) and had a nice antenna gain. It also featured a newer CPU, DDR2 memory, and it was also pretty light and had very high wind survivability.", "List of Materials:", "Total Cost: ", "I enrolled in a basic fiber plan at the \"FTTH\" point (30/4 Mbps) for $25 a month.", "I\u2019ve yet to experience any lags or disconnection issues since I set up the Point-to-Point FTTH link. I\u2019ve even run Netflix and Spotify at the same time to test the connection, all while downloading several things on the computer. If a house is built right in the path of the signal, I\u2019ll need to increase the height if I want the link to continue operating. But, so far so good!", "Just goes to show what a little perseverance can do when paired with a broomstick and borrowed binoculars.", "For those who are especially interested in the subject, here are the nitty gritty details on signal strength along with other information:", "After performing a full duplex \"Network Speed Test\" with NanoBeam's own speed test tool:", "Same test, but with one-way only:", "ISP speed test:"]},
{"tite": "Introducing Scrapy Cloud 2.0", "date": "May 04, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Scrapy Cloud has been with Scrapinghub since the beginning, but we decided some spring cleaning was in order. To that end, we\u2019re proud to announce ", "!", "\u00a0", "This overhaul will help you improve and scale your web scraping projects. Among other perks, our upgraded cloud-based platform includes a brand new and much more flexible architecture based on containers.", "While much of this upgrade is behind the scenes, what\u2019s most important to you is pricing changes, the introduction of Docker support, and a more efficient allocation of resources.", "Without further ado, let\u2019s dive right into how Scrapy Cloud 2.0 will positively (hopefully) affect you!", "I know this is a weird one to start with, but we wanted to show you how the awesome base of Scrapy Cloud 1.0 will still be a part of the 2.0 release. You will still be able to:", "And the major feature that is NOT changing is that you can still ", " with all the perks that you've been enjoying including unlimited team members, unlimited projects, and no credit card required.", "So for those who are worried, rest assured, this is just an improvement on the awesome base that made Scrapy Cloud a staple of Scrapinghub.", "First things first, the price tag. A major benefit of Scrapy Cloud 2.0 is our new pricing model. We\u2019re switching to customizable subscriptions\u00a0so that you have greater flexibility in picking the number of ", " (computing resources) that you need to run your job. There are no more standardized plans, you just build a subscription that is customized to you. Plus, this model will save you money since you can tailor monthly units to your scraping needs.", "Each ", "\u00a0provides 1GB of RAM, 2.5GB of disk space and computing power that is 3x better if compared with similar plans from Scrapy Cloud 1.0. One unit costs $9 per month and you can purchase as many units as you like, allocate them for your organization's spiders and upgrade whenever you want.", "When you sign up for our platform, you are immediately enrolled in a free subscription that contains 1 unit. Each job running on this free account will be able to run for up to 24 hours and has a data retention of 7 days.", "And much as we love talking with our customers, Scrapy Cloud 2.0 makes upgrading your account easier and without needing to go through support. All you have to do is go to the Billing Portal for your organization (accessible via\u00a0the billing page of your organization profile):", "And then change the amount of Container Units to fit your needs, as shown in the screenshot below:", "Once you purchase container units, your data retention period will be increased to 120 days and there will be no more limits to the running time.", "Scrapy Cloud 2.0 features a new resource management model which provides you with ", ". For example, using Scrapy Cloud 1.0 would cost $150 for a worker with 3.45GB of RAM and 7 computing units. With Scrapy Cloud 2.0, you get 16GB of RAM and 12 computing units for a lower price ($144).", "Plus, you can allocate your units (resources) however you need. This includes giving a lot of resources to big, complex jobs and making sure smaller jobs only use the resources they need.", "You can assign units to groups and then move your projects from one group to another whenever you want to adjust\u00a0resources:", "With this new release, your spiders run in containers isolated from the other jobs running on our platform. This means that your performance will not be affected by other resource consuming jobs. Disk quotas are also applied to ensure that a job does not consume all the disk space from a server, which would affect other jobs running on the same server. Long story short, no more DoS!", "With Scrapy Cloud 2.0, your spiders run inside Docker containers. This new resource management engine ensures that you get the most out of the resources available for your subscription.", "Scrapy Cloud 2.0 is fully backwards compatible with the previous version. We did our best to keep your current workflow untouched, so you can still use the ", " to deploy your project and dependencies without the hassle of building Docker images. Just ", " ", " by running: ", ".", "And\u00a0here's some\u00a0good news for power users: you can\u00a0deploy your tailor-made\u00a0Docker images to Scrapy Cloud 2.0.\u00a0Dependencies are no longer a big deal since you can now compile libraries, install anything you want in your Docker image and then deploy it to our platform. This feature is currently only available for paying customers, but it will soon be released for free users as well.", "Check out this ", "\u00a0if you are a paying user and want to deploy custom Docker images to Scrapy Cloud.", "Scrapy Cloud 2.0 introduces ", ", a set of predefined Docker images that users can select when deploying a project. The inclusion of\u00a0", " brings together the best of two worlds: you can deploy your project without having to build your own image\u00a0and\u00a0you can also choose the kind of environment where you want to run your project.", "In Scrapy Cloud terms, a ", " is a pre-built Docker image that you can select when you are deploying a project.\u00a0This\u00a0will\u00a0be used as the environment for this project. In\u00a0this first release, there are two Stacks available:", "This is a scrapinghub.yml file example, using the Scrapy 1.1 Stack and passing additional requirements to be installed on the image\u00a0via requirements.txt:", "Just make sure that you are using ", " to be able to deploy your project based on the scrapinghub.yml file above.", "In the near future, we are going to provide additional Stacks. For example, if you want\u00a0to run a\u00a0crawler that you built using a different crawling framework or programming language, we are looking to\u00a0have a built-in Stack for that.", "There are some things that you should be aware of before migrating your projects to Scrapy Cloud 2.0:", "Check out this ", "\u00a0if you are a paying user and want to deploy custom Docker images to Scrapy Cloud.", "Scrapy Cloud 2.0 is officially live. From today onward, the old model will not be available to new users and all free organizations will be migrated automatically.", "All Scrapy Cloud projects will be upgraded to Scrapy Cloud 2.0 within the year. As a loyal customer, you have the opportunity to upgrade immediately to\u00a0experience dramatically improved performance. Just ", " and click the \"Migrate Now\" button at\u00a0the top of the page:", "And that\u2019s Scrapy Cloud 2.0 in a nutshell! We\u2019re going to be tinkering a bit more in the coming months so keep your eyes peeled for an even more major upgrade."]},
{"tite": "Improving Access to Peruvian Congress Bills with Scrapy", "date": "July 13, 2016 ", "author": "Carlos Pe\u00f1a", "blog_data": ["Many governments worldwide have laws enforcing them to publish their expenses, contracts, decisions, and so forth, on the web. This is so the general public can monitor what their representatives are doing on their behalf.", "For the sake of transparency, Peruvian Congress provides a website where people can check the list of bills that are being processed, voted and eventually become law. For each bill, there\u2019s a page with its authorship, title, submission date and a brief summary. These pages are frequently updated when bills are moved between commissions, approved and then published as laws.", "By having all of this information online, lawyers and the general public can potentially inspect bills that could be the result of lobbying. In Peruvian history, there have been many laws passed that were to benefit only one specific company or individual.", "My lawyer friends from the Peruvian NGOs ", " and ", " asked me about the possibilities to build a web application. Their goal was to organize all the data from the Congress bills, allowing people to easily search and discover bills by keywords, authors and categories.", "The first step in building this was to grab all bill data and metadata from the Congress website. Since they don\u2019t provide an API, we had to use web scraping. For that, ", " is a champ.", "I wrote ", " to crawl the Congress site and download as much data as possible. The spiders wake up every 8 hours and crawl the Congress pages looking for new bills. They parse the data they scrape and save it into a local PostgreSQL database.", "Once we had achieved the critical step of getting all the data, it was relatively easy to build a search tool to navigate the 5400+ bills and counting. I used Django to create a simple interface for users, and so\u00a0", " was born.", "All kinds of possibilities are open once we have the data. For example, we could now generate ", " on the status of the bills. We found that of the 5402\u00a0proposed bills, only 740\u00a0became laws, meaning most of the bills were rejected or forgotten on the pile and never processed.", "Quick searches also revealed that many bills are not that useful. A bunch of them are only proposals to turn some specific days into \"national days\".", "There are proposals for national day of peace, \"peace consolidation\", \"peace and reconciliation\", Peruvian Coffee, Peruvian Cuisine, and also national days for several Peruvian produce.", "There were\u00a0even more than one bill proposing the celebration of the same thing, on the ", ".\u00a0Organizing the bills into a database and building our search tool allowed people\u00a0to discover these redundant and unnecessary bills.", "After we aggregated the data into statistics, my lawyer friends found that the majority of bills are approved after only one round of voting. In the Peruvian legislation, dismissal of the second round of voting for any bill should be carried out only under exceptional circumstances.", "However, the numbers show that the use of one round of voting has become the norm, as 88% of the bills approved were only done so in one round. The second round of voting has been created to compensate for the fact that the Peruvian Congress has only one chamber were all the decisions are made. It\u2019s also expected that members of Congress should use the time between first and second voting for further debate and consultation with advisers and outside experts.", "The nice thing about having such information in a well-structured machine-readable format, is that we can create cool data visualizations, such as ", " that shows all the events that happened for a given bill:", "Another cool thing is that this data allows us to monitor Congress\u2019 activities. Our ", " allows users to subscribe to a RSS feed in order to get the latest\u00a0bills, hot off the Congress press. My lawyer friends use it to issue \"Legal Alerts\" in the social media when some of the bills intend to do more wrong than good.", "People can build very useful tools with data available on the web. Unfortunately, government data often has poor accessibility and usability, making the transparency laws less useful than they should be. The work of volunteers is key in order to build tools that turn the otherwise clunky content into useful data for journalists, lawyers and regular citizens as well. Thanks to open source software such as Scrapy and Django, we can quickly grab the data and create useful tools like this.", "See? You can help a lot of people by doing what you love! :-)"]},
{"tite": "How to Run Python Scripts in Scrapy Cloud", "date": "September 28, 2016 ", "author": "Elias Dorneles", "blog_data": ["You can deploy, run, and maintain control over your ", " spiders in ", ", our production environment. Keeping control means you need to be able to know what\u2019s going on with your spiders and to find out early if they are in trouble.", "\u00a0", "\u00a0", "This is one of the reasons why being able to ", " is a nice feature. You can customize to your heart\u2019s content and automate any crawling-related tasks that you may need (like monitoring your spiders\u2019 execution). Plus, there\u2019s no need to scatter your workflow since you can run any Python code in the same place that you run your spiders and store your data.", "While just the tip of the iceberg, I\u2019ll demonstrate how to use custom Python scripts to notify you about jobs with errors. If this tutorial sparks some creative applications, let me know in the comments below.", "We\u2019ll start off with a regular Scrapy project that includes a Python script for building a summary of jobs with errors that have finished in the last 24 hours. The results will be emailed to you using AWS Simple Email Service.", "You can check out the ", ".", " To use this script you will have to ", " so that the email function works.", "In addition to the traditional Scrapy project structure, it also contains a ", "\u00a0script in the ", " folder. This is the script responsible for building and sending the report.", "The deploy will be done via ", ", just like your regular projects.", "But first, you have to make sure that your project's ", " file lists the script that you\u2019ll want to run (see the line highlighted in the snippet below):", " If there's no ", " file in your project root yet, you can run ", " and the deploy process will generate it for you.", "Once you have included the ", " parameter in the ", " file, you can deploy your spider to Scrapy Cloud with this command:", "Running a Python script is very much like running a Scrapy spider in Scrapy Cloud. All you need to do is set the job type as \"Scripts\" and then select the script you want to execute.", "The ", " script expects three arguments: your Scrapinghub API key, an email address to send the report to, and the project ID (the numeric value in the project URL):", "Since this script is meant to be executed once a day, you need to schedule it under Periodic Jobs, as shown below:", "Select the script to run, configure when you want it to run and specify any arguments that may be necessary.", "After scheduling the periodic job (I\u2019ve set it up to run once a day at 7 AM UTC), you will see a screen like this:", " You can run the script immediately by clicking the play button as well.", "And you\u2019re done! The script will run every day at 7 AM UTC and send a report of jobs with errors (if any) right into your email inbox. This is how the report email looks:", "\u00a0", "Heads up, here\u2019s what else you should know about Python scripts in Scrapy Cloud:", "While this specific example demonstrated how to automate the reporting of jobs with errors, keep in mind that you can use any Python script with Scrapy Cloud. This is helpful for customizing your crawls, monitoring jobs, and also handling post-processing tasks.", ".", " is forever free, so no need to worry about a bait-and-switch. Try it out and let me know what Python scripts you\u2019re using in the comments below."]},
{"tite": "Christmas Eve vs New Year's Eve: Last Minute Price Inflation?", "date": "January 06, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Welcome to Part 3 of our ongoing series! We\u2019re tracing the prices of popular products across multiple retailers throughout the winter season. Starting from Black Friday and continuing through to January, we are keeping a close eye on price movements and the actual effects of discounts during specific days or weekends.", "The New Year is upon us and I hope that this holiday season was full of laughter, love, family, and, of course, gifts. Much as we may not want to admit it, buying presents is a large part of how we celebrate our holidays in December. And while we may all be mourning the hit our bank accounts took, hopefully you did not fall prey to any unexpected price hikes.", "The reason that I am comparing the Eves rather than Christmas Day or New Year\u2019s Day is because I was curious to see if stores would ", " of ", " or if the discounts would hold. Christmas Day is also incredibly dead and I wanted to keep the continuity of Christmas Eve with New Year\u2019s Eve.", "Let\u2019s look at the damage and see which retailers might have pulled a bait and switch.", "I laid out our methodology, parameters, and intervals ", ", but I am going to continue to update the Tech and Trivia section so that you can follow along with the obstacles and solutions that have cropped up in the course of this project.", "While conducting this price experiment, there were times when the contents that we wanted to scrape were hidden in a piece of JavaScript code. This included situations where the variations and prices of a particular product were stored in a JavaScript array that was mixed within the HTML. For example:", "We used Scrapy selectors to extract the JS code from inside the <script> element:", "Once we got the code, we used ", ", a library that converts JS code into XML data. Then we built a selector for the XML contents:", "We were then able to parse the contents using our usual selectors:", "So, if you ever have to deal with JavaScript code embedded in HTML, give ", " a try.", "The volatility of prices never fails to amaze me. In the span of a single week, a product can rise or drop $100. Even when prices hold consistently, where you shop determines if you are saving or spending at any given time.", "I\u2019m still congratulating myself on ", " on Black Friday. Prices haven\u2019t dipped back that low in over a month.", "This was one of the most consistent categories. There weren\u2019t any major price fluctuations except for the Samsung Galaxy Tab S2 at Best Buy (really hoping you didn\u2019t buy that on New Year\u2019s Eve). Amazon prices didn\u2019t shift by more than $10 and most other prices stayed the same. One point to note is that it seems that Walmart has stopped carrying the Samsung Galaxy Tab S2 after Black Friday. Time will tell if it\u2019s because they sold too many or too few.", "The two largest price discrepancies for Wearables are the Apple Watch and the Jawbone UP3. New Year\u2019s Eve (and current as of January 6, 2016) is the best price that Target has offered for the Apple Watch. Even during ", ", prices were $399 with a $100 gift card thrown in. So hopefully you were not one of those shoppers looking for a flashy last minute gift on Christmas Eve because you might be kicking yourself right now.", "Christmas Eve was the clear winner for the Home Theater category. Prices were either lower than or remained the same as New Year\u2019s Eve. And a hearty congratulations to those Christmas procrastinators that scored a great price on the Apple TV at Target.", "Once again, Christmas Eve came out on top. The ", " has finally come to an end, but this was an expected development. Walmart either sold out or ceased to offer the PS4 Bundle, which is why it is not included on this graph.", "Phones was one of the least impacted categories with only the iPhone 6S at Best Buy showing a significant price difference.", "I\u2019m going to go ahead and call it for New Year\u2019s Eve. On Christmas Eve, the Star Wars BB-8 Droid R/C at Amazon had the highest price seen since we started tracking in late November. The LeapFrog Epic Tablet at Best Buy and the Mega Bloks Minions Supervillain Jet at Walmart conveniently kept higher prices during Christmas Eve before discounts during New Year\u2019s Eve.", "Our last holiday prices post comes at the end of January so remember to follow us on ", ", ", ", and ", " or subscribe to our ", ". This is the grand finale where we\u2019ll help you make more informed shopping decisions next year!"]},
{"tite": "Vizlegal: Rise of Machine-Readable Laws and Court Judgments", "date": "January 13, 2016 ", "author": "vizlegal", "blog_data": ["The Code of Ur-Nammu: It sounds like something dreamt up by ", " over a boozy lunch. But actually it\u2019s the name of one of the earliest examples of humans writing down laws. The stone tablets of the code were found in modern-day Iraq and date back to more than 4,000 years ago.", "\"Ur Nammu code Istanbul\" by Istanbul Archaeology Museum", "In the following millennia, humans continued to write down laws while also keeping a record of the judgments of courts. From stone to wax tablets to paper, civilizations experimented with the best medium to share these laws and judgments. The Romans codified some of their laws - The Twelve Tables - as did the Chinese via the Tang Code.", "The leap from stone to paper was a huge milestone. This allowed for laws and judgments to be easily copied and disseminated, later helped along by the wonders of the printing press.", "The Paragon Press", "As we progressed into the information age, we moved into a far more sophisticated world. Most modern countries now use the internet to disseminate the laws of their parliaments and the rulings of their courts, mostly using one wondrous modern technology: the PDF.", "I can hear the techies among you sniggering. But seriously, in 4,000 years we\u2019ve managed to go from writing laws on stone to writing them on PDFs. Some countries are using HTML to publish laws and judgments. Very, very few have built APIs.", "This is a problem we\u2019re working on at ", ". We believe that all legal information should be available via APIs. We also believe the Git model should be used for legislation and for the versioning of court judgments. We prefer to live in a world of machine-readable law, where sections of Acts or portions of judgments are accessible not just via search, but via an API call. APIs provide a simple method for applications to access legal information and at present, if an application wants to access a law, there is essentially no way to do it.", "Some of you might be wondering: why would we need versioning of court judgments? Aren\u2019t court judgments simply written, published and disseminated and that\u2019s the end of the story?", "No.", "As The New York Times reported, even the US Supreme Court ", " and in some cases, years afterwards. Unfortunately they\u2019ve been pretty secretive about it too. In October they announced a ", " of providing slightly more details of their editing, albeit in a technically ad hoc fashion. This doesn\u2019t go far enough - it\u2019s 2016 not 1996. Adding some text at the end of a document to say the document has been edited is something Wikipedia solved 15 years ago.", "We should be able to see each stage of the alterations in judgments, with the most up-to-date version being clearly displayed. So too with laws. Additionally If I\u2019m reading a law, I should be able to jump to which judges interpreted it and how (and vice versa). Most constitutions are in place to ensure that justice is administered in public. We believe that laws and judgments must also be as accessible as possible and fully transparent to ensure trust in the democratic system.", "And how does Scrapinghub help us with this? Well, as mentioned earlier, most countries in the common law English-speaking world are actively publishing laws, judgments and administrative decisions online like it\u2019s 1999 - HTML and PDFs - with no APIs, no document versioning and little or no connection between laws and the judgments that interpret them. This is bad for the legal industry, bad for legislatures, bad for government, bad for industry, and bad for society. Law is one of the core components of nation states. Minute changes in the wording of laws and obscure interpretations actively affect the lives of billions of people each and every day.", "Many government-built websites that publish court judgments are available. Unfortunately, they are usually ", " with little or no thought given to the users who must access them every day. They often have non-existent or sloppily implemented user interfaces. This usually goes with not providing an open data source - it\u2019s basically \u201cpublish some ", " and move on\u201d.", "Scrapinghub provides the most comprehensive services for gathering this raw unstructured information into our API, which serves our web application and which in turn serves our customers. Scrapinghub grabs this information and converts it into data. We then use this data to build our own platform and analytics.", "Our goal is to take the legal field from an information-rich industry to a data-rich one. Scrapinghub\u2019s essential package of tools buttress our mission - and it\u00a0is\u00a0a \u201cmission\u201d because the information sources are so vast and varied that it would end up consuming enormous time and resources if we did it by ourselves.", "We believe in making laws and judgments machine-readable because we can\u2019t imagine a future of humanity that hasn\u2019t done this (and we also believe in a future of law-abiding robots)."]},
{"tite": "Scrapy Tips from the Pros: Part 1", "date": "January 19, 2016 ", "author": "Valdir Stumm Jr", "blog_data": [" is at the heart of ", ". We use this framework extensively and have accumulated a wide range of shortcuts to get around common problems. We\u2019re launching a series to share these Scrapy tips with you so that you can get the most out of your daily workflow. Each post will feature two to three tips, so stay tuned.", "I am sure each and every developer of web crawlers has had a reason to curse web developers who use messy layouts for their websites. Websites with no semantic markups and especially those based on HTML tables are the absolute worst. These types of websites make scraping much harder because there are little-to-no clues about what each element means. Sometimes you even have to trust that the order of the elements on each page will remain the same to grab the data you need.", "Which is why we are so grateful for ", ", a collaborative effort to bring semantic markup to the web. This project provides web developers with schemas to represent a range of different objects in their websites, including Person, Product, and Review, using any metadata format like ", ", ", ", ", ", etc. It makes the job of search engines easier because they can extract useful information from websites without having to dig into the HTML structure of all the websites they crawl.", "For example, ", " is a schema used by online retailers to represent user ratings for their products. Here\u2019s the markup that describes user ratings for a product in an online store using the ", ":", "This way, search engines can show the ratings for a product alongside the URL in the search results, with no need to write specific spiders for each website:", "You can also benefit from the semantic markup that some websites use. We recommend you to use ", ", a library to extract ", " from HTML documents. It parses the whole HTML and returns the microdata items in a python dictionary. Take a look at how we used it to extract microdata showing user ratings:", "Now, let\u2019s build a spider using Extruct to extract prices and ratings from the ", ". This website uses microdata to store information about the products listed. It uses this structure for each of the products:", "With this setup, you don\u2019t need to use XPath or CSS selectors to extract the data that you need. It is just a matter of using Extruct\u2019s MicrodataExtractor in your spider:", "This spider generates items like this:", "So, when the site you\u2019re scraping uses microdata to add semantic information to its contents, use ", ". It is a much more robust solution than relying on a uniform page layout or wasting time investigating the HTML source.", "Have you ever been frustrated by the differences between the web page rendered by your browser and the web page that Scrapy downloads? This is probably because some of the contents from that web page are not in the response that the server sends to you. Instead, they are generated in your browser through JavaScript code.", "You can solve this issue by passing the requests to a JavaScript rendering service like ", ". Splash runs the JavaScript on the page and returns the final page structure for your spider to consume.", "Splash was designed specifically for this purpose and ", ". However, in some cases, all you need is something simple like the value of a variable from a JavaScript snippet, so using such a powerful tool would be overkill. This is where ", " comes in. It is a library that converts JavaScript code into XML data.", "For example, say an online retailer website loads the ratings of a product via JavaScript. Mixed in with the HTML lies a piece of JavaScript code like this:", "To extract the value of ", " using js2xml, we first have to extract the ", " block and then use js2xml to convert the code into XML:", "Now, it is just a matter of building a Scrapy Selector and then using XPath to get the value that we want:", "While you could probably write a regular expression with the speed of thought to solve this problem, a JavaScript parser is more reliable. The example we used here is pretty simple, but in more complex cases regular expressions can be a lot harder to maintain.", "Sometimes the piece of data that you are interested in is not alone inside an HTML tag. Usually you need to get the values of some parameters from URLs listed in the page. For example, you might be interested in getting the values of \u2018username\u2019 parameters from URLs listed in the HTML:", "You might be tempted to use ", ", but calm down, ", " is here to save the day with a more dependable solution:", "If ", " is new to you, take a look at the ", ". We\u2019ll be covering some of the other features of this Python library later in our Scrapy Tips From the Pros series.", "Share how you used our tips in the comments below. If you have any suggestions on scenarios that you\u2019d like covered or tips you\u2019d like to see, let us know!", "Make sure to follow us on ", ", ", ", and ", " and don\u2019t forget to subscribe to our ", ". More tips are coming your way, so buckle up!"]},
{"tite": "Happy Anniversary: Scrapinghub Turns 5", "date": "January 27, 2016 ", "author": "Cecilia Haynes", "blog_data": ["Birthdays are a big deal at Scrapinghub. We always make sure to celebrate each team member on their special day, recognizing achievements and sending well-wishes for the year to come. Well, on December 15, 2015, we celebrated one of the most momentous birthdays of the year: our own. We are officially 5 years old and what an amazing 5 years it has been.", "From humble beginnings of a mere 3 Scrapinghubbers, we are now 124 strong and distributed over 37 countries. With each step forward, we have added ", ", developed more ", ", and ", " to remain the leaders in web scraping services.", "So talk a walk with us down memory lane and join us in celebrating this major milestone:", "\n", "\n", "Now that we\u2019ve ", " at our accomplishments, it\u2019s time to turn our sights forward. 2016 is looking to be a year of major releases and huge leaps in scraping the web even more efficiently.", "It will be easier than ever to get data without needing to write a line of code. With our upcoming ", " release, we\u2019ll be improving the UX and simplifying the process of web scraping. Our goal is to make Portia more intelligent and to start, it will be able to automatically extract data from lists or tables in web pages. For all of you advanced users, we will also be including the ability to export your Portia spider as a Scrapy spider written in Python so that you can fine-tune the code however you like. Portia will also be able to decide what should be crawled and when actions should be run so that you can focus on getting data and not shepherding your spiders.", "The new generation of web scraping platforms is coming! Originally developed as a lab project, we are almost ready to share a beta version of Kumo, AKA Scrapy Cloud 2.0. A key feature of Kumo is better resource usage through advanced cluster management software. We are able to allocate resources more efficiently, allowing us to offer better performance and more flexible pricing plans.", "Kumo also provides a way to build and deploy ", " images. This feature is incredibly helpful for those who want to deploy crawlers that use their favorite Natural Language Processing toolkit, which may require a couple of C or Fortran libraries to be compiled. Kumo allows for a finer control over production environment.", "Our platform developers are working on a brand new Web API for Scrapinghub called OneAPI.\u00a0OneAPI will cover the entire Scrapinghub platform and will provide a single, consistent and well documented interface for all platform resources. We aim to provide the best possible experience for developers who are using our platform by making our new API easy to learn and use.", "We are proud to announce that Python 3 support for ", " is almost ready to go! Scrapy core developers are hard at work and Scrapy 1.1 will be released soon with basic Python 3 support. This will be the year of Python 3 at Scrapinghub because once Scrapy fully supports Python 3, we will start rolling it out across all of our projects.", " is also receiving an overhaul this year. We\u2019re updating the architecture and doing more data analysis to improve how we manage IP addresses and crawl settings. We are providing more feedback to users along with advanced crawling features. Our goal is for Crawlera to become even more reliable and scalable, so that you can continue to crawl uninterrupted.", "is no longer only a crawl frontier framework, it\u2019s also a set of components that allows you to distribute and scale custom web crawlers. Plus, it includes Scrapy support out of the box. Frontera allows\u00a0us to build large scale web crawlers in Python. This will be a great year for Frontera, with a lot features coming including a web management UI, adaptive revisiting, docker containers, and a standalone Frontera-based crawler.", "With the growing number of Scrapinghubbers, we\u2019ve been able to continue to ", " and our resources. We\u2019re building more datasets and products based on data for tasks like lead generation, competitive intelligence, trend prediction, and marketing analytics.", "In non-platform news, keep your eyes peeled for two new monthly columns on our blog. We\u2019re ramping up this year to provide more tips, workflow hacks, and release information so that you are up-to-date on the happenings at Scrapinghub.", "We are Scrapy experts who have done battle in the trenches and faced pretty much any scraping-related problem you could conceivably imagine. Scrapy was developed by our two founders, we maintain it, and our products and services are designed to make the most out of this framework. We know our Scrapy and we will share our hard-won knowledge with you in this ", ". Each post will feature a couple of tricks and new developments to help you overcome any obstacles that you may come across while scraping the web.", "It\u2019s shaping up to be a pretty busy year for us, what with releases and advancements in Portia, Scrapy Cloud, Frontera, etc. So, we\u2019re making it easier for you to keep up with the slew of Scrapinghubber projects coming at you with the release of this column. Each month we\u2019ll round up the most significant updates so that you can always catch up on anything you might have missed.", "We hope that you are as psyched about the coming year as we are. Lots of improvements are in store and we invite you to come along for the ride. Follow us on ", ", ", ", and ", " or subscribe to our ", ". Reach out and let us know what you would like to see from Scrapinghub in 2016."]},
{"tite": "Python 3 is Coming to Scrapy", "date": "February 04, 2016 ", "author": "Richard Dowinton", "blog_data": ["Scrapy with ", " is finally here! Released through ", ", this is the result of several months of hard work on the part of the ", " community and ", " engineers.", "This is a huge milestone for all you Scrapy users (and those who haven\u2019t used Scrapy due to the lack of Python 3). Scrapy veterans and new adopters will soon be able to move their entire stack to Python 3 once the release becomes stable. Keep in mind that since this a release candidate, it is not ready to be used in production.", "In addition to Python 3 support, Scrapy 1.1.0rc1 includes a lot of enhancements and bug fixes:", "This beta release includes some limitations for Python 3 (which will not affect Python 2 users):", "If you want the full details of all the benefits of this new release, ", ".", "And now it\u2019s your turn, Scrapy users! The sooner this is battle-tested, the sooner a stable release comes your way, so please give it a try by installing or upgrading Scrapy via pip:", "We appreciate any feedback or bug reports as they will help a lot in making Python 3 support with Scrapy stable. If you run into any problems, ", ". We\u2019re looking forward to releasing the official Scrapy 1.1, so get to experimenting!", "\u00a0"]},
{"tite": "Web Scraping Finds Stores Guilty of Price Inflation", "date": "February 10, 2016 ", "author": "Cecilia Haynes", "blog_data": ["This is the last post ending a series of articles that have traced the prices of some of the top gifts, gadgets, and gizmos from Black Friday 2015 through to January 2016.", "Check out ", " comparing Black Friday and Cyber Monday,\u00a0", " comparing Black Friday, Cyber Monday, and Green Monday, and ", " comparing Christmas Eve and New Year\u2019s Eve.", "We\u2019re all looking to be savvy shoppers who can buy nice things on a budget. So get ready to block out your calendar and wait in line at the right store.", "It\u2019s been a little over two months since we first started tracking the prices of popular products, starting on Black Friday and covering all the major discount days during the winter of 2015. We\u2019ve finally reached the culmination of this project and are ready to declare the best days to shop for presents and deals this year.", "Plus, we're calling out those guilty of price inflation, so you're going to want to keep reading.", "For this specific post, we compared all prices from Black Friday, Cyber Monday, Green Monday, Christmas Eve, and New Year\u2019s Eve to those taken from January 13, 2016. We chose this date as our baseline because the traditional sales period is over, but it is still in a relevant time frame.", "While we covered the specifics of our methodology in the first post, we would like to share several tips and tricks that we used to get past some of the challenges of scraping so many different websites and products.", "Some websites are really friendly toward web crawlers, providing machine-readable data for the content that you see in your browser. During this project, we got lucky. One of the websites we scraped for this series provided\u00a0", " for the relevant information we needed\u00a0about its\u00a0products:", "This was a perfect opportunity to use\u00a0", ", a Python library we developed to ", ". Take a look at how Extruct works in this Scrapy shell session:", "As you can see, it\u2019s a piece of cake to scrape websites that use semantic markup. With\u00a0Extruct there\u2019s no need to write specific XPath or CSS selectors for the website.\u00a0So, if the website you are trying to scrape has some ", " properties inside the HTML, it\u2019s your lucky day. The site is using Microdata and you can likely extract all the data you need with Extruct.", "If this article doesn\u2019t hammer home the completely arbitrary nature of prices, I\u2019m not sure what will. Between price inflation and false sales, there were actually very few examples of products being fully discounted across the board on any particular day. Instead, getting a good deal was much more contingent on visiting the right store at the right time.", "The Apple TV was one of the most interesting examples of this:", "On Black Friday 2015, only Target and Walmart offered sales for this item (both $51.75). On Cyber Monday, Target shot up the price to $69, about $5 more than Best Buy and Sam\u2019s Club.", "Target continued this overpriced trend until Christmas Eve before discounting to $59. They then returned the price to the inflated $69 for New Year\u2019s Eve and kept this price constant through to January 13, 2016.", "Walmart, on the other hand, brought the price up to $63 on Cyber Monday before holding steady at $64.99, the same price as Best Buy, for the rest of the study. Target was clearly the best bet for a cheaper price as long as you purchased at the right time (Black Friday and Christmas Eve), but they are clearly guilty of inflating prices to capitalize on shopper anxiety.", "Out of all of the products, there were only 3 that were continuously discounted throughout the winter season compared to January 13, 2016. These were:", "When compared to every other day, Black Friday was by far the cheapest day to purchase these 7 products:", "The Jawbone UP 3 warrants closer attention because while Black Friday was the cheapest day to purchase this at Best Buy, the second cheapest time to buy it was January 13, 2016. For the rest of the winter season, Best Buy\u2019s prices for the Jawbone UP 3 were higher than both Amazon and Target (see the Wearables graphs below).", "When compared to every other day, Black Friday was the most expensive day to purchase\u00a0these products:", "January 13, 2016 was the cheapest day for these products:", "The worst culprits for inflated pricing were products in the Toys (4 products) and Tablets (5 products) categories\u00a0(not counting the same product that was inflated in different stores). \u00a0Amazon was the worst place to shop since it had 8 of the\u00a0inflated items in these categories.", "These were the products that were completely unaffected throughout the duration of this project:", "Let's take a closer look at the guilty parties who snuck in inflated prices among the sales.", "Amazon was by far and away the worst offender when it came to\u00a0price inflation. On every single one of the 5 days, there were about 13 to 18 products that were more expensive than the prices listed on January 13, 2016. This was far in excess of the 8 to 11 deals offered.", "\u00a0", "[gallery type=\"slideshow\" columns=\"5\" ids=\"4920,4922,4923,4925,4926\"]", "[gallery type=\"slideshow\" columns=\"5\" ids=\"4930,4931,4932,4934,4935\"]", "[gallery type=\"slideshow\" columns=\"5\" ids=\"4943,4945,4946,4948,4949\"]", "[gallery type=\"slideshow\" columns=\"5\" ids=\"4950,4951,4953,4954,4956\"]", "[gallery type=\"slideshow\" columns=\"5\" ids=\"4958,4959,4961,4962,4964\"]", "[gallery type=\"slideshow\" columns=\"4\" ids=\"4966,4968,4970,4971\"]", "I\u2019m going to go ahead and place\u00a0the winners:", "Now, there is no guarantee that these same patterns will hold this\u00a0year. But based on 2015, I highly recommend that you plan around the Toys situation. Every single toy suffered from price inflation with every single store inflating the price of at least one toy. If you are in a bind, though, then I\u2019d wait for Green Monday and avoid Amazon.", "If a Fitbit goes on sale on Black Friday, take the deal. It\u2019s not likely to get any cheaper. I\u2019m still congratulating myself on getting my Fitbit Charge HR for $30 cheaper on Black Friday!", "Do your homework ahead of time and don\u2019t worry if you hesitate and miss a sale, there\u2019s usually another retailer and another day.", "For more articles like this, follow us on ", ", ", ", and ", ", and don\u2019t forget to subscribe to our ", "."]},
{"tite": "Portia: The Open Source Alternative to Kimono Labs", "date": "February 17, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Attention Kimono users: we've ", " so you can easily convert your projects from Kimono to Portia!", "Imagine your business depended heavily on a third party tool and one day that company decided to shut down its service with only 2 weeks notice. That, unfortunately, is what happened to users of ", " yesterday.", "And it\u2019s one of the many reasons why we love open source so much.", " is an ", " visual scraping tool developed by Scrapinghub to make it easier to get data from the web without needing to write a line of code.", "You can do anything with Portia that you can with Kimono Labs, but without vendor lock-in. This allows you to run your spiders on our platform, but you always have the option to move to your own infrastructure.", "Also, we won\u2019t be leaving you high and dry, Kimono users. We've developed an exporter to easily migrate your Kimono projects to Portia. Check it out:\u00a0", "Get started with ", ". And take a look at our ", " in case you have any questions.", "Take a look at Kimono\u00a0and Portia\u00a0in action:", "Our upcoming Portia 2.0 release includes:", "And soon after, we\u2019ll also be adding new features like:", "Portia is fully integrated into our platform, ", ", but you can also ", " and run it locally or on your own server. The benefits of running Portia on Scrapy Cloud include:", "You can ", " here, although you\u2019ll need to ", "\u00a0(free!) to deploy your spiders to Scrapy Cloud. Plus, since ", ", we welcome any and all developers who are interested in contributing."]},
{"tite": "Migrate your Kimono Projects to Portia", "date": "February 26, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Heads up, Kimono Labs users!", "Today, we are releasing ", " your ", " projects to ", ".", "All you have to do is provide your Kimono credentials and let it convert your Kimono projects into Portia projects.", "You will then be able to run those projects on ", " or on your own Portia instance, since ", ".", "coming soon!", "Portia 2.0 comes with\u00a0a brand new user interface that we designed based on a meticulous study of real world users led by our UX team \u2013 and of course all existing Portia spiders will continue to be supported.", "It also includes\u00a0some long-awaited features:", "We're excited to ensure that your work continues uninterrupted."]},
{"tite": "Scrapy Tips from the Pros: February 2016 Edition", "date": "February 24, 2016 ", "author": "Valdir Stumm Jr", "blog_data": ["Welcome to the February Edition of ", ". Each month we\u2019ll release a few tips and hacks that we\u2019ve developed to help make your Scrapy workflow go more smoothly.", "This month we\u2019ll show you how to crawl websites more effectively following sitemaps and we'll also demonstrate how to add custom settings to individual spiders in a project.", "Web crawlers feed on URLs. The more they have, the longer they live. Finding a good source of URLs for any given website is very important as it gives the crawler a strong starting point.", " are an excellent source of seed URLs. Website developers use them to indicate which URLs are available for crawling in a machine-readable format. Sitemaps are also a good way to discover web pages that would be otherwise unreachable, since some pages may not be linked to from any other page outside of the sitemap.", "Sitemaps are often available at ", " or in a different location specified in the robots.txt file.", "With ", " you don\u2019t need to worry about parsing XML and making requests. It includes a ", " class you can inherit to handle all of this for you.", "Let\u2019s say you want to crawl Apple\u2019s website to price check different products. You would want to visit as many pages as possible so that you can scrape as much data as you can. Fortunately, Apple's website provides a sitemap at ", ", which looks like this:", "Scrapy\u2019s generic ", " class implements all the logic for parsing and dispatching requests necessary to handle sitemaps. It reads and extracts URLs from the sitemap and it will dispatch a single request for each URL it finds. Here is a spider that will scrape Apple's website using the sitemap as its seed:", "As you can see, you only need to subclass SitemapSpider and add the sitemap\u2019s URL to the ", " attribute.", "Now, run the spider and check the results:", "Scrapy dispatches a request for each URL found by SitemapSpider in the sitemap and then it calls the ", " method to handle each response it gets. However, some pages in a website may vary in structure and so you might want to use multiple callbacks for different types of pages.", "For instance, you can define a specific callback to handle the Mac pages, another one for the iTunes pages and the default ", " method for all the other pages:", "To do it, you have to add a ", " attribute to your class, mapping URL patterns to callbacks. For instance, the URLs matching the '/mac/' pattern will have its response handled by the ", " method.", "So, the next time you write a crawler, make sure to use SitemapSpider if you want to have comprehensive crawls of the website.", "For more features, check ", ".", "The settings for Scrapy projects are typically stored in the project\u2019s settings.py file. However, these are global settings that apply to each of the spiders in your project. If you want to set individual settings for each spider, all you need to do is add an attribute called ", " to your spider class.", "This is especially useful when you need to enable or disable pipelines or middlewares for certain spiders or to specify different settings for each one. For example, some spiders in your project might require ", " (a smart proxy service) enabled while others don't. You can achieve this by adding ", " in ", " in the specific spiders.", "Take a look at a simplified version of the spiders from a book catalog project. They need ", " to define where the book covers are going to be stored in the filesystem:", "These spiders scrape metadata from books on a range of websites. Book covers are retrieved using ", ", which can be enabled and configured in the project global settings.py file:", "The ", " setting lets you define where the downloaded images will be stored in your filesystem. So if you want to separate the images downloaded by each spider into different folders, you just need to override the global ", " setting via ", " in each spider:", "Alternatively, you can pass the settings as arguments for the spider using the scrapy -s command line option, but that adds the hassle of having to pass all custom settings from the command line:", "So, if you ever need different settings for some spider in your project, include them in the ", " attribute in the spider. Heads up, this feature is available for Scrapy >= 1.0.0.", "If you are interested in learning how to download images using your spiders, ", " for more information.", "And that\u2019s about it for our February tips. Check back in with us in March, follow us on ", ", ", ", and ", ", and subscribe to our ", " to catch our next ", "."]},
{"tite": "Splash 2.0 Is Here with Qt 5 and Python 3", "date": "February 29, 2016 ", "author": "Richard Dowinton", "blog_data": ["We\u2019re pleased to announce that Splash 2.0 is officially live after many months of hard work.", "For those unfamiliar with Splash, it\u2019s a headless browser we developed specifically for web crawling. Splash executes and renders JavaScript so you can deal with dynamic content. It also supports scripting so you can perform actions on the page.", "Splash is open source and fully integrated with Scrapy and Portia. You can also use its API to integrate with any project that needs to render JavaScript pages. Splash is included with Scrapy Cloud and is supported without any additional dependencies. You can also run a Splash instance on your own infrastructure, so there is no platform lock-in.", "Splash 2.0 now runs on ", " and brings Python 3 support, lots of UI improvements and many other changes and bug fixes.", "We\u2019ve added support for Python 3 and the Docker container now uses it by default instead of Python 2.", "Splash now runs on Qt 5 which brings improved JavaScript and HTML5 features such as CSS filters and canvas.", "We would like to thank ", ", who joined us through Google Summer of Code 2015, for all his hard work in doing the initial port for Qt 5 and Python 3. If you want to contribute in some of our projects, please stay tuned for GSoC 2016.", "We\u2019ve also included modules for ", "You can now find script examples in a new drop-down menu named \u2018Examples\u2019:", "Auto-completion of Splash methods:", " ", "Default script result:", " ", "We fixed a bug which prevented cookies being set via JavaScript as well as one which resulted in cookies being shared across tabs.", "We fixed a bug that prevented users from updating proxy settings after ", " had been called.", "We also made the following backwards-incompatible changes:", "Splash no longer supports the QT-based disk cache. We have discouraged usage since 1.0 and we recommend using something more reliable like ", ".", "We\u2019ve made changes to how ", ", ", " and ", " serialize objects. Circular objects are no longer returned and DOM elements are no longer serialized.", "We removed the Splash-as-a-proxy service because it didn\u2019t work with HTTPS and we wanted to reduce the dependency on advanced Twisted features. There was also ", " due to Python treating headers with values containing new lines as unsafe.", "We made a few backwards-incompatible changes to Splash scripting as well and there\u2019s a small chance you will need to update your scripts. Here are the changes we\u2019ve made:", "Receiving a response object. You should now use the returned object\u2019s info attribute.", " ", " should be replaced with ", " ", " should be replaced with ", " You should also do the same for the response object received by ", " and ", ".", " Default encoding of ", " is now Base64.", "You can find the full release notes ", ".", "Thanks for reading and hope you enjoy using Splash! Take a look at our previous blog post to learn more about using Splash in your Scrapy projects. Also stay tuned because we have an even bigger release coming soon. You won\u2019t want to miss it."]},
{"tite": "Looking Back at 2015", "date": "December 31, 2015 ", "author": "Cecilia Haynes", "blog_data": ["2015\u00a0has been a standout year for Scrapinghub with new developments in expanded products, noteworthy clients, and cherished partnerships. We were curious about whether we could continue our growth from 2014 and are pleased to report that our curiosity has been successfully laid to rest. So without further ado, let\u2019s dive straight into what has made 2015 a hard act to follow.", "First of all, we turned 5 on December 15. In a competitive startup world with a failure rate of 90%, we are solidly in the ", " that made it past the high attrition period. This is a huge milestone and, while it\u2019s debatable when we actually graduated into the established company category, I think it\u2019s safe to say that we\u2019re here to stay. We\u2019ve also managed to retain our core company culture of remote work and distributed team members while toiling through the growing pains of the startup scene.", "One of the major causes of failure cited for startups is a ", " for the product. We attribute a major part of our success to not only having a highly prized ", " that help you scrape the web efficiently, but we take it a step further by offering ", " for every need. Our ability to deliver new and ", " like ", " and ", " has continued to set us apart from the competition.", "This has been a high growth year for Scrapy Cloud. We scraped and stored data from over 22 billion pages (2 times the amount of last year) and we ran 18M crawler jobs (4 times the number of 2014 and 22 times that of 2013). There were over 20 billion pages that passed through Crawlera this year (4 times the amount of last year) with an increasing number of users discovering our platform.", "\u00a0", "In 2015, Frontera grew from a small library that implemented simple crawling strategies like FIFO/LIFO and was only suitable for single process usage to a much bigger web crawling toolbox. It now contains a transport layer (by means of Apache Kafka or ZeroMQ) and a more powerful backend concept. This backend concept allows you to separate crawling policies from low-level storage-specific code. The new HBase backend is also geared for high performance crawling. Finally, we ran a stress test crawl of 45 million documents from about 100K websites in the .es zone and presented our findings in 4 conferences. Over the last few months, we\u2019ve been working on making ", " easier to use as well as adding helpful\u00a0features, so expect a new release with a smaller configuration footprint and more accessible documentation in the near future.", "This has been a bumper year for Portia. We\u2019ve made many changes that have allowed you to build spiders more quickly and much more easily. At the beginning of the year, we addressed user feedback and simplified the annotation process.", "To the delight of many of you Portia users, during the summer we added support when creating spiders for sites that require JavaScript. Along with this release, we added the ability to record actions on a page and execute them during a crawl. Another new feature of Portia is the addition of algorithms that make suggestions for information on the page that you may want to annotate.", "We have been working hard behind the scenes on the future of Portia, so be on the lookout in 2016 for a new user interface (UI) designed to make creating spiders a breeze. With this new UI, you will be able to easily extract many items from a single page with even more advanced annotation suggestions.", "We\u2019ve deployed a number of open source projects this year, all aimed at improving scraping techniques for polite data extraction.", " and has an ", ". From this process emerged ", ", an lxml-based library used for extracting data from HTML/XML documents, along with a bunch of Scrapy plugins. We\u2019ve given these ", " a new home!", "We have continued to improve other projects, including Dateparser (which now runs in PyPy and Python 3) and Splash.", "We really hit the road this year with 15 conferences and 12 talks scattered around the world. We went everywhere from PyCon Philippines to ", "\u00a0to PyCon Russia. We had booths in 4 conferences, including Python Brasil (featuring some pretty sweet swag). A couple of our Scrapinghubbers also reached out to their local communities either by leading an Hour of Code or being a part of BIME Hack Day.", "This past summer was our second year participating in Google Summer of Code. We worked with students under the umbrella of the Python Software Foundation and one is now a dedicated Scrapinghubber. We\u2019re always looking for new ways to cultivate interest in and understanding of Python and our scraping technologies, and this provided a wonderful opportunity for us to act as mentors. We\u2019re psyched to lead a Google Summer of Code program in 2016, so keep an eye out for fun project proposals.", "Our Scrapinghubber community has continued to grow and there are now Scrapinghubbers in 36 countries. We were 85 at the end of last year and we\u2019re at 119 team members in December with 6 more to come just in January alone! We\u2019re thrilled with the talents that each of our new hires has brought to their established teams.", "In fact, our team has grown so much in\u00a0the last\u00a0couple of years that we\u00a0decided to create a dedicated Human Resources team to take on the bulk of the HR logistics. Some of the challenges that the Scrapinghub HR team has faced include the rapid expansion of the company and the multicultural and distributed nature of our team.", "This year we\u2019ve enacted some perks to help our Scrapinghubbers achieve new heights in their careers. Starting this year, our crew has access to a hardware allowance and an online course stipend to help with professional development.", "A huge thank you to our Scrapinghub team, our wonderful clients, and the open source community that works with us to make our products even stronger.", "That wraps up 2015, which has been monumental in so many ways. So cheers to 2016, I can\u2019t even begin to imagine where we\u2019ll go from here!"]},
{"tite": "Distributed Frontera: Web Crawling at Scale", "date": "August 06, 2015 ", "author": "Alexander Sibiryakov", "blog_data": []},
{"tite": "Tips for Creating a Cohesive Company Culture Remotely", "date": "November 16, 2015 ", "author": "Cecilia Haynes", "blog_data": [". Rapidly. This past year, we have gone from 80 to 114 Scrapinghubbers. Companies expanding so rapidly can risk losing what made them them in the first place. We don\u2019t want that to happen, especially since we have the added challenge of being an entirely distributed company with remote coworkers ", " in 36 countries.", "However, we also want to be sure to adapt to the changing dynamic. Sticking to a method that worked well with 40 people means stagnation. It falls apart when you double your numbers. We have had to learn how to communicate through these growth spurts while maintaining our core culture.", "One of the further challenges has been to stay true to our quirky selves while building up. With engineers, programmers, data scientists, writers, marketers, and a whole slew of other positions, we\u2019ve got a lot of different personalities. Extroverts, introverts, niche interests in dystopian novels; you name it, we\u2019ve got it. With applications coming in each day, narrowing down qualified candidates can be challenging because above all else, we stress a cultural fit. We want to ensure that each new hire fits in with their team and with the wider community as a whole.", "Here\u2019s a breakdown of the key steps we take to get there.", "CVs and resumes are a very flat way to learn about a person. We know that sometimes people have trouble showing their full potential through such a limited document. So, as part of our screening process, we assign a trial task to give people a chance to shine. This gives both us and the candidate a little taste of what it would be like to work together in a real world scenario. A successful project then leads into an interview.", "Our interviews reflect the position that is being filled. Whether that includes a sales pitch or a detailed overview of relevant background experiences, rest assured that we\u2019re not going to ask you any odd hypotheticals about being ", ". Instead, we will look for situations where you have demonstrated:", "We cover all of our bases before making an offer. Picking the best candidate right off the bat smooths the rest of the hiring process and ensures that we strengthen the baseline of our company culture.", "Once we find the right cultural fit, onboarding our new teammate is the next priority. This includes maintaining open communication and transparency throughout this process. Immediately working closely with a new hire in a remote setting is critical to easing people into our company environment. It can be an overwhelming enough experience to be onboarded onsite while in the physical presence of your coworkers. That overwhelming sensation can multiply exponentially when you are dealing with remote situations.", "This is why we have a buddy system. We pair up new hires with veterans from their own teams who can be their mentor (much like Yoda).", "This mentor is the point of contact for the new hire and the gateway to the pulse of Scrapinghub. Mentors are crucial to the continued success of our remote lifestyle. Mentors help with all questions big or small and are always available for reassurance and feedback.", "Confidence is one of the foundations of independence and we strongly believe in working with self-assured coworkers who are excited about taking control of their own projects.", "Since Scrapinghub was founded around Scrapy, the Open Source project created by our co-founders, most of our culture was inherited from the Open Source community. This includes an atmosphere of collaboration and problem-solving alongside innovation. While this is a great environment for independent individuals who enjoy working on passion projects, it can be challenging to keep everyone on the same page.", "Factor in time zones, language barriers, and varying levels of technical expertise, and it\u2019s like we\u2019re attempting to herd cats. This is why it is critical for us to keep many lines of communication open.", "Here are some of the methods that we use to ensure that our teams are all on the same page:", "This is an internal newsletter that covers new projects, products, features, new hires, and where we feature remarkable Scrapinghubbers every week. It provides a weekly overview of achievements within the company.", "We love Slack. Slack is easily our go-to communication route between Scrapinghubbers. It\u2019s immediate, clearly shows who\u2019s online (important with our various timezones), and has a ton of different features.", "We have many channels that not only cover clients and projects, but also different topics of interest to Scrapinghubbers. These interests include #radiohub to share music, #beer for beer lovers, and channels for our individual countries, regions, and everything else.", "We don\u2019t have cubicles to show off. Instead, we work from places like:", "This is how the Office Tour Week started. It is a way for us to share our lives with our distributed colleagues. We remain connected in spite of the distance through these exchanges and glimpses into each other\u2019s lives.", "The \u201ctour\u201d takes place on our Google+ Community where people share photos and/or videos from their workplaces and their lives.", "We plan to continue to grow with our tried-and-true roadmap. In case you\u2019re struggling with developing a company culture that is realistic, productive, and cohesive, try out the methods that have worked for us.", "From finding the right candidates to hiring a cultural fit and maintaining open lines of communication with a mentor, it is a process. Please share yours and let us know what tips you\u2019ve picked up along the way!"]},
{"tite": "Aduana: Link Analysis to Crawl the Web at Scale", "date": "September 29, 2015 ", "author": "Pedro L\u00f3pez-Adeva", "blog_data": ["Crawling vast numbers of websites for specific types of information is impractical. Unless, that is, you prioritize what you crawl. Aduana is an experimental tool that we developed to help you do that. It\u2019s a special backend for Frontera, our tool to expedite massive crawls in parallel (", ").", "Aduana is designed for situations where the information you\u2019re after is dispersed all over the web. It provides you with two link analysis algorithms, PageRank and HITS (", "). These algorithms analyze link structure and page content to identify relevant pages. Aduana then prioritize the pages you\u2019ll crawl next based on this information.", "You can use Aduana for a number of things. For instance:", "Concrete example: imagine that you\u2019re working for a travel agency and want to monitor trends on location popularity. Marketing needs this to decide what locations they\u2019ll put forward. One way you could do this is by finding mentions of locations on news websites, forums, and so forth. You\u2019d then be able to observe which locations are trending and which ones are not. You could refine this even further by digging into demographics or the time of year - but let\u2019s keep things simple for now.", "This post will walk you through that scenario. We\u2019ll explore how you can extract locations from web pages using geotagging algorithms, and feed this data into Aduana to prioritize crawling the most relevant web pages. We\u2019ll then discusses how performance bottlenecks led us to build Aduana in the first place. We'll lastly\u00a0give you its backstory and a glimpse at what\u2019s coming next.", "In case you\u2019d like to try this yourself as you read, you\u2019ll find the source code for everything discussed in this post in the Aduana repository\u2019s ", ".", "Extracting location names from arbitrary text is a lot easier said than done. You can\u2019t just take a geographical dictionary (", ") and match its contents against your text. Two main reasons:", "For instance, ", " references 8 locations called Hello, one called Hi, and a whopping 95 places called Hola:", " ", "Our first idea was to use ", " (NER) to reduce the number of false positives and false negatives. A NER tokenizer allows to classify elements in a text into predefined categories such as person names or location names. We figured we\u2019d run our text through the NER and match the relevant output against the gazetteer.", " is one of the few open source libraries available for this task. It uses heuristics that disambiguate locations based on the text\u2019s context.", "We built a similar solution in Python. It\u2019s based on ", ", a library for natural language processing that includes a ", " tagger. A POS tagger is an algorithm that assigns parts of speech - e.g. noun or verb - to each word. This lets us extract geopolitical entities (GPEs) from a text directly. And then match the GPEs against GeoNames to get candidate locations.", "The result is better than naive location name matching but not bulletproof. Consider the following paragraph from ", " to illustrate:", "Green words are locations our NER tokenizer recognized. Red words are those it missed. As you can see, it ignored Bizkaia. It also missed Viejo after incorrectly splitting Puerto Viejo in two separate words.", "Undeterred, we tried a different approach. Rather than extracting GPEs using a NER and feeding them directly into a gazetteer, we\u2019d use the gazetteer to build a neural network and feed the GPEs into that instead.", "We initially experimented with centrality measures using the ", " library. In short, a centrality measure assigns a score to each node in a graph to represent its importance. A typical one would be PageRank.", "But we were getting poor results: nodes values were too similar. We were leaning towards introducing non-linearities to the centrality measures to work around that. When it hit us: a special form of recurrent neural network called a ", " network had them built-in.", "We built our Hopfield network (", ") with each unit representing a location. Unit activation values range between -1 and +1, and represent how sure we are that a location appears in a text. The network\u2019s links connect related locations with positive weights - e.g. a city with its state, a country with its continent, or a country with neighboring countries. Links also connect mutually exclusive locations with negative weights, to address cases where different locations have identical names. This network then lets us pair each location in a text with the unit that has the highest activation.", "You\u2019d normally train such a network\u2019s weights until you minimize an error function. But manually building a training corpus would take too much time and effort for this post\u2019s purpose. As a shortcut, we simply set the weights of the network with fixed values (an arbitrary +1 connection strength for related locations) or using values based on GeoNames (an activation bias based on the location\u2019s population relative to its country).", "The example we used earlier yields the following neural network:", " ", "Colors represent the final unit activations. Warmer colors (red) show activated units; cooler colors (blue) deactivated ones.", "The activation values for our example paragraph are:", " ", "The results are still not perfect but very promising. Puerto gets recognized as Puerto Viejo but is incorrectly assigned to a location in Mexico. This may look strange at first, but Mexico actually has locations named San Sebasti\u00e1n and Bilbao. Algorta is correctly identified but received a low score. It\u2019s too weakly connected to the related locations in the graph. Both issues occur because we didn\u2019t train our network.", "That\u2019s sensible and good enough for our purpose. So let\u2019s move forward and make Aduana use this data.", "You can safely skip this section if you\u2019re not running the code as you read.", "You\u2019ll find ", " in the Aduana repository and ", " in the documentation.", "To run the spider:", "If you get problems with the dependencies - namely with numpy and scipy - install them manually. One at a time, in order:", "Our spider starts with a seed of around 1000 news articles. It extracts links to new pages and location data as it crawls. It then relies on Aduana to tell it what to crawl next.", "Each page\u2019s text goes through our geotagging algorithm. Our spider only keeps locations with an activation score of 0.90 or above. It outputs the rows in a locations.csv file. Each row contains the date and time of the crawl, the GeoName ID, the location\u2019s name, and the number of times the location appears in the page.", "Our spider uses this data to compute a score for each page. The score is a function of the proportion of locations in the text:", " ", "Aduana then uses these scores as inputs for its ", " algorithm. This allows it to identify which pages are most likely to mention locations and rank pages on the fly.", "Our spider is still running as we write this. Aduana ranked about 2.87M web pages. Our spider crawled 129k. And our geotagging algorithm found over 300k locations on these pages. The results so far look good.", "In a future blog post we\u2019ll dig into our findings and see if anything interesting stands out.", "To conclude this section, Aduana helps a lot to guide your crawler when running broad crawls. And depending on what you\u2019re using it for, consider implementing some kind of machine learning algorithm to extract the information you need to compute page scores.", "If you\u2019re familiar with graph databases, you may have been wondering all along why we created Aduana instead of using off-the-shelf solutions like ", " and ", ". The main reason is performance.", "We actually wanted to use a graph database at first. We figured we\u2019d pick one, write a thin layer of code so Frontera can use it as a storage backend, and call it a day. This was the logical choice because the web is a graph and popular graph databases include link analysis algorithms out of the box.", "We chose ", " because it\u2019s open source and works with HDFS. We ran the provided ", " on the ", ". The results were surprisingly poor. We cancelled the job 90 minutes in. It was already consuming 10GB of RAM.", "Wondering why, we ended up reading ", ". It turns out that running PageRank on a 40 million node graph is only twice as fast as running GraphChi on a single computer. That eye opener made us reconsider whether we should distribute the work.", "We tested a few non-distributed libraries with same LiveJournal data. The results:", "To be clear, take these timings with a grain of salt. We only tested each library for a brief period of time and we didn\u2019t fine-tune anything. What counted for us was the order of magnitude. The non-distributed algorithms were much faster than the distributed ones.", "We then came across two more articles that confirmed what we had found: ", " and ", ". The second one is particularly interesting. It discusses computing PageRank on a 128 billion edge graph using a single computer.", "In the end, SNAP wasn\u2019t fast enough for our needs, and FlashGraph and X-Stream were rather complex. We realized it would be simpler to develop our own solution.", "With this in mind, we began implementing a Frontera storage engine on top of a regular key-value store: ", ". Aduana was born.", "We considered using ", ". It might have been faster. LMDB is optimized for reads after all; we do a lot of writing. We picked LMDB regardless because:", "When Aduana needs to recompute PageRank/HITS scores, it stores the vertex data as a flat array in memory, and streams the links between them from the database.", "Using LMDB and writing the algorithms in C was well worth the effort in the end. Aduana now fits a ", " onto a 1TB disk, and its vertex data in 16GB of memory. Which is pretty good.", "Let\u2019s now segue into why we needed any of this to begin with.", "Aduana came about when we looked into identifying and monitoring specialized news and alerts on a massive scale for one of our Professional Services clients.", "To cut a long story short, we wanted to locate relevant pages first rather than on an ad hoc basis. We also wanted to revisit the more interesting ones more often than the others. We ultimately ran a pilot to see what happens.", "We figured our sheer capacity might be enough. After all, our ", "\u2019s users scrape over two billion web pages per month. And Crawlera, our ", ", lets them work around crawler countermeasures when needed.", "But no. The sorry reality is that relevant pages and updates were still coming in too slowly. We needed to prioritize more. That got us into link analysis and ultimately to Aduana itself.", "We think Aduana is\u00a0a very promising tool to expedite broad crawls at scale. Using it, you can prioritize crawling pages with the specific type of information you\u2019re after.", "It\u2019s still experimental. And\u00a0not production-ready yet. Our next steps will be to improve how Aduana decides to revisit web pages. And make it play well with ", ".", "If you\u2019d like to dig deeper or contribute, don\u2019t hesitate to fork Aduana\u2019s ", ". The ", " includes a section on ", " if you\u2019d like to extend it.", "As an aside, ", "; and we're\u00a0", " hire\u00a0if you need\u00a0help ", "."]},
{"tite": "Introducing Javascript support for Portia", "date": "August 19, 2015 ", "author": "Ruairi Fahy", "blog_data": ["Today we released the latest version of Portia bringing with it the ability to crawl pages that require JavaScript. To celebrate this release we are making ", " available as a free trial to all Portia users so you can try it out with your projects.", "Open a project within Portia. If you don't already have a an instance of Portia you can get started by downloading Portia from ", " or by ", " for our hosted instance.", "If you would like to crawl using JavaScript in your project you can do so by:", "\n", "\n", "\n", "\n", "\n", "\n", "By clicking this checkbox you will now be able to annotate pages which require JavaScript to be enabled to be crawled correctly.", "After you enable JavaScript you will be presented with the ability to limit which pages that JavaScript is enabled for. You can choose for JavaScript to only be run on certain pages in the same way that you can limit which pages are followed by Portia.", "\n", "\n", "The reason you may want to limit which pages load JavaScript is that loading JavaScript can increase the amount of time required to run your spider.", "Once you have made these changes you can publish your spider and it will be able to crawl pages that require JavaScript.", "When you are creating a spider you can use the show followed links checkbox to decide if you need to JavaScript enabled on a page or not. By showing followed links you can see which links are followed only if JavaScript is enabled and which links are always followed.", "\u00a0", "\u00a0", "To decide if you need JavaScript enabled for extracting data you can try to create a sample for the page that you wish to extract data from. If JavaScript is not enabled for this page and you can see the data you wish to extract then you don't need to change anything; your spider works! If you don't see the data you want then you can:", "If you already have your own dedicated splash instance you can enable it for your project by adding its URL and your API key to the Portia addon in your project settings. If you would like to request your own Splash instance please visit your organization's dashboard page. If you would like to learn more about splash you can do so ", ".", "This should be all you need to get started with JavaScript in Portia. I", " Happy Scraping!"]},
{"tite": "Parse Natural Language Dates with Dateparser", "date": "November 09, 2015 ", "author": "Richard Dowinton", "blog_data": ["We recently released ", " with support for Belarusian and Indonesian, as well as the Jalali calendar used in Iran and Afghanistan. With this in mind, we\u2019re taking the opportunity to introduce and demonstrate the features of Dateparser.", "Dateparser is an open source library we created to parse dates written using natural language into Python. It translates specific dates like \u20185:47pm 29th of December, 2015\u2019 or \u20189:22am on 15/05/2015\u2019, and even relative times like \u201810 minutes ago\u2019, into Python datetime objects. From there, it\u2019s simple to convert the datetime object into any format you like.", "When scraping dates from the web, you need them in a structured format so you can easily search, sort, and compare them.", "Dates written in natural language aren\u2019t suitable for this. For example, the 24th of December shows up first if you sort the 25th of November and the 24th of December alphanumerically. Dateparser solves this by taking the natural language date and parsing it into a datetime object.", "A bonus perk of Dateparser is that you don\u2019t need to worry about translation. It supports ", " including English, French, Spanish, Russian, and Chinese. Better yet, Dateparser autodetects languages, so you don\u2019t need to write any additional code.", "This makes Dateparser especially useful when you want to scrape data from websites in multiple languages, such as international job boards or real estate listings, without necessarily knowing what language the data you\u2019re scraping is in. Think ", ".", "Dateparser developed while we were working on a broad crawl project that involved scraping many forums and blogs. The websites were written in numerous languages and we needed to parse dates in a consistent format.", "None of the existing solutions met our needs. So we created Dateparser as a simple set of functions that sanitised the input and passed it to the dateutil library, using parserinfo objects to work with other languages.", "This process worked well at first. But as the crawling project matured we ran into problems with short words, strings containing several languages, and a host of other issues. We decided to move away from parserinfo objects and handle language translation on our own. With the help of contributors from the Scrapy community, we significantly improved Dateparser\u2019s language detection feature and made it easier to add languages by using YAML to store the translations.", "Dateparser is simple to use and highly extendable. We have successfully used it to extract dates on over 100 million web pages. It\u2019s well tested and robust.", "You can install Dateparser via pip. Import the library and use the dateparser.parse method to try it out:", "Supporting new languages is simple. If yours is missing and you\u2019d like to contribute, send us a pull request after updating the ", " file. Here is what the definitions for French look like:", "When parsing dates, you don\u2019t need to set the language explicitly. Dateparser will detect it for you:", "See ", " for more examples.", " is the most popular Python library to parse dates. Dateparser actually uses ", " as a base, and builds its features on top of it. However, Dateutil was designed for formatted dates, e.g. \u201822-10-15\u2019, rather than natural language dates such as \u201810pm yesterday\u2019.", " is closer to Dateparser in that it also parses natural language dates. One advantage of Parsedatetime is that it supports future relative dates like \u2018tomorrow\u2019. However, while Parsedatetime also supports non-English languages, you must specify the location manually, whereas Dateparser detects the language for you.", "Parsedatetime also has more boilerplate code, compare:", "to:", "dateparser.parse('today')", "If you are dealing with multiple languages and want a simple API with no unnecessary boilerplate, then Dateparser is likely a good fit for your needs.", "Dateparser is an extensible, easy-to-use, and effective method for parsing international dates from websites. Its unique features arose from the specific problems we needed to address. Namely, parsing dates from websites whose language we did not know in advance.", "The library has been well tested against a large number of sites in over 20 languages and we continue to refine and improve it. Contributors are most welcome, so if you\u2019re interested, please don\u2019t hesitate to ", "!"]},
{"tite": "Scrapy on the Road to Python 3 Support", "date": "August 19, 2015 ", "author": "Giovanni Piazza", "blog_data": ["Scrapy is one of the few popular Python packages (almost 10k github stars) that's not yet compatible with Python 3. The team and community around it are working to make it compatible as soon as possible. Here's an overview of what has been happening so far.", "You're invited to read along and participate in the porting process from ", ".", "If you asked around you likely\u00a0heard an answer\u00a0like \"It's based on Twisted, and Twisted is not fully ported yet, you know?\". Many blame Twisted, but other things are actually holding back the Scrapy Python 3 port.", "When it comes to Twisted, its most important parts are already ported. But if we want Scrapy spiders to download from HTTP urls, we are really going to need a fix or workaround on Twisted's http agent, since it doesn't work on Python 3.", "The Scrapy team started to make moves towards Python 3 support two years ago by porting some of the Scrapy dependencies. For about a year now a subset of Scrapy tests is executed under Python 3 on each commit. Apart from Twisted, one bottleneck that was blocking the progress for a while was that most of Scrapy requires Request or Response objects to work - which was recently resolved as described below.", "During EuroPython 2015, from 20 to 28 of July, several Scrapy core developers gathered together in Bilbao and were able to make progress\u00a0on the porting process\u00a0-\u00a0meeting for the first time after several years of working together as they did.", "There was a Scrapy sprint scheduled to the weekend. Mikhail Korobov, Daniel Gra\u00f1a, and Elias Dorneles teamed up to prepare Scrapy for it by porting Request and Response in advance. This way, it would be easier for other people to join and contribute during the weekend sprint.", "In the end, time was short for them to fully port Request and Response before the weekend sprint. Some of the issues they faced were:", "To unblock further porting, the team decided to use bytes for HTTP headers and momentarily disable some of the tests for non-ascii URL handling, thus eliminating the\u00a0two\u00a0bottlenecks that were holding things back.", "The sprint itself was quiet but productive. Some highlights on what developers did there:", "In the end the plan worked as expected. After porting Request and Response objects and making some hard decisions, the road to contributions is open.", "In the weeks that followed\u00a0the sprint, developers continued to work on the port. They also got important contributions from the community. As you can see ", ", Scrapy already got several pull requests merged (for example from @GregoryVigoTorres and @nyov).", "Before the sprint there were ~250 tests passing in Python 3. The number is now over\u00a0600. These recent advances helped increase our test coverage under Python 3 from 19% to 54%.", "Our next major goal is to port the Twisted HTTP client so spiders can actually download something from remote sites.", "It's still a long way to the Python 3 support, but when it comes to Python 3 porting Scrapy is in a much better shape now. Join us\u00a0and contribute to porting Scrapy following ", ". We have added a badge to Github\u00a0to show\u00a0the progress of Python 3 support. The percentage is calculated using the tests that pass in Python 3 vs the total number of tests available. Currently, 633 tests passing on Python vs 1153\u00a0in total.", "Thanks for reading!"]},
{"tite": "Black Friday, Cyber Monday: Are They Worth It?", "date": "December 03, 2015 ", "author": "Cecilia Haynes", "blog_data": ["This post kicks off a series of articles that will trace the prices of some of the top gifts, gadgets, and gizmos from Black Friday through to January 2016.", "In the past\u00a0two\u00a0weeks, you've\u00a0probably been flooded with offers of discounts, sales, and \u201cHUGE PRICE DROPS\u201d that might all very well turn out to be\u00a0meaningless. Honestly, how much have you really saved through splurging over the weekend?", "Between the competing articles\u00a0detailing ", "\u00a0and still others rounding up the ever growing ", ", it all adds up to a ton of noise. Whenever I open my laptop it\u2019s like websites and ads are screaming at me to buy\u00a0things I don't need or want. This\u00a0abundance of mostly useless information just burns me out. It's like they hope you'll buy things just for the sake of the sale.", " We picked the upgraded versions of the popular gifts from the fall and winter of 2014. If you spot a favorite item, we invite you to keep checking back as we log in further data.", "Without further ado, let the Winter Games Begin!", "Here\u2019s a quick overview of what we took into consideration when setting up the data-driven project.", " We also eliminated the variations of each of the products and chose a baseline representation to follow. So keep that in mind in case something you\u2019ve been eyeing goes out of stock, it may just be that particular version.", "Websites don\u2019t all display products the same way, so we had to make choices on\u00a0sites that don\u2019t differentiate specific colors. For instance, there are some items like the iPhone 6S where color impacts price. In these cases, we chose a middle-of-the-road color to\u00a0compare apples to apples as much as we can.", "Heads up, Amazon does not directly ", ", so we chose reputable third-party sellers.", "Stores often feature lightning deals and abrupt price shifts during the days before and after Black Friday and Cyber Monday. ", " This interval will accurately capture the organic movement of prices.", "As we continue this project, we will adjust the interval to reflect projected and potential price variations.", "For the tech-minded folk out there, we\u2019re using Scrapy, Scrapy Cloud, and Crawlera to get our data. ", " is an open-source Python framework that allows you to easily build web crawlers. ", " is, as the name suggests, our cloud-based platform to deploy, scale, and monitor Scrapy web crawlers.\u00a0", " is our\u00a0smart proxy rotator, designed to\u00a0crawl websites faster\u00a0by spreading\u00a0requests through a pool of IP addresses.", " We initially tried to add a delay between the requests through the DOWNLOAD_DELAY setting, but it didn\u2019t solve the problem. So we resorted to using\u00a0Crawlera to\u00a0conduct a gentle crawl.", "This introduced a side effect\u00a0though: requests were being redirected to\u00a0the international versions of the sites\u00a0because of the use of international IP addresses. Crawlera provides the option to be restricted to US-only IP addresses. We enabled that\u00a0and\u00a0problem solved. ", "We wanted to give\u00a0you a sense of\u00a0price\u00a0differences across multiple product categories. So we\u00a0chose major retailers and stores that offer a\u00a0large variety of products:", "We aim to remove the guesswork from shopping during the holiday season, so hopefully you won\u2019t get too much buyer\u2019s remorse when checking back with us over the next month and a half.", "Here is our list of worthy contenders:", "We broke down results by product category to keep things readable.", "Tablets are always a big hit whether you are buying them as a gift or just as a treat for yourself. These are some of the top tablets for this winter season. Hopefully\u00a0you won't beat yourself up too much if you already missed the lowest deal.", "\u00a0", "Honestly, not too huge a difference except for a couple of minor price variations. Black Friday does seem to have been the better deal on the whole.", "Between fitness trackers and the rise of smartwatches, wearables are going to be pretty much everywhere you look. Here is a taste of some of the top models out there.", "\u00a0", "Once again, it seems that Black Friday reigns minimally supreme, with Sam's Club really raising prices on Cyber Monday.", "Who doesn\u2019t like to Netflix and Chill? Plus, with all of the ", " out this season, now is the time to pick up a handy streaming device for your TV.", "\u00a0", "At this point, Cyber Monday is really not living up to its deal steal reputation with Best Buy, Sam's Club, and Walmart all raising their prices.", "Black Friday and Cyber Monday are both\u00a0traditionally huge for delivering great deals on game consoles and bundles. If you were thinking about taking the leap on a new system, this might be the right time for you to ", ".", "\u00a0", "Clearly the Nintendo 3DS XL and the Wii U were great buys for Black Friday, so congratulations to all the savvy shoppers! Those who were slow off the mark hopefully went on a Best Buy run for Cyber Monday.", "With new releases and (potentially) great savings, now might be the time to \u201caccidentally\u201d lose your old phone and trade in for the upgrade.", "\u00a0", "Seriously, Sam's Club is clearly not a Cyber Monday type of company...", "Toys are one of those categories that just tend to be in danger of being ", " during the holidays. Here\u2019s hoping that you won\u2019t need to do battle this winter season for the perfect gift.", "\u00a0", "Toys had the least amount of price variation of all the categories, so hopefully these sales will continue on through the winter season.", "Remember to stay tuned for the continuation of this series - subscribe to our blog's ", "\u00a0or\u00a0", ". We\u2019ll be monitoring\u00a0products and prices throughout the Christmas shopping season so you don't have\u00a0to.", "In the next post, we\u2019ll probe into whether and\u00a0who\u00a0abuses the Christmas season by increasing their list prices ahead of holiday\u00a0sales."]},
{"tite": "Chats With RINAR Solutions", "date": "December 16, 2015 ", "author": "Cecilia Haynes", "blog_data": ["Meet Tom\u00e1s Rinke. He is the CTO and Co-Founder of ", ", a startup that provides data consulting services to inform decision making. He is an avid Scrapy user and a Scrapinghub development partner. As an off-shoot of RINAR Solutions, he developed ", ", an app that provides information on the legal sector.", "We sat down with Rinke to learn more about his successes, his business model, and how he uses Scrapinghub to power up his startup.", "RINAR Solutions started out of the idea to develop an innovative online shopping experience for grocery and supermarkets. This idea was called Buyzter (buy + faster). To create a viable product, we needed data. After an experience with hiring a freelancer to scrape data from Walmart ($200 bucks for the whole set), we decided we would teach ourselves how to scrape websites in order to cut our costs. During this learning process, we explored different scraping frameworks and eventually came across Scrapy.", "However, Buyzter continued to build up quite slowly and we didn't find the best business model. We kept our options open to find new avenues for growth, and, in the meantime, we kept scraping new sites.", "A client request to scrape a justice website was what launched RINAR. After that project, we started offering scraping services before becoming development partners with Scrapinghub. We started to notice a demand for data on the niche topics related to legal matters. DataJudicial grew out of the need to handle high volume information requests by law firms and business. We do massive queries and creation of documents.", "Our clients value the time they save with our services; they work less. We do in one hour what takes them 15 hours manually. This saves clients in operational costs and allows employees to concentrate on other critical tasks. Our clients appreciate that there are no errors in the process because despite the fact that we have automated 90% of the workflow, we validate the data we deliver, ensuring quality. And since our work involves legal information, we emphasize security within our app and in our disclosure.", "We feature a straightforward platform, provide connections to legal websites, and handle high volume operations in order to save time and reduce operational costs.", "While our competition is still offering services through email, we have developed a platform that scales for a great user experience and is based on material design.", "When we started offering our services, we ran all the jobs locally and orders were handled by email. The only automated part of our operation was the actual spider.", "To use an analogy, it is as if we bought a small machine to make clothes in our garage. We knew we could make great clothes but we needed to scale. Since we wanted to focus on designing our clothes rather than the manufacturing process, we looked for a way to outsource or use a factory to allow others build our clothes for us.", "We were interested in finding a cloud platform that could handle the orders (no more emails and no more misunderstandings) and ", " had what we needed: a platform for effective crawls and an API that allows for easy integration. Plus their founders are the creators of Scrapy, so all the pieces were in place. We invested and DataJudicial was born.", "Easy learning curve. Great active community. Complete documentation. It came up first on the Google search.", "And our technical background is based on SOA, so our knowledge of XPath and XML allowed for easy integration.", "We started exploring Scrapinghub features like the API, the libraries, and all the add-ons in order to scale up our product. Plus the plans offered allowed us to scale from a couple of clients to 1,000 in no time. As a development partner, we are experienced with the platform and find it intuitive to use.", "Only one? It does the job. The whole ecosystem makes sense and Scrapy is a great framework (open source). It is backed by Scrapinghub, a company that combines a platform with comprehensive support.", "Scrapy surprised us with its capabilities of scraping any kind of site using any type of technology. We started telling our clients: \"If you can see it, we can scrape it.\" We faced sites with logins, captchas, needing sequential flows, etc.", "Scrapinghub solved our availability issues. Our spiders now work 24/7. Before, we needed to have a computer online and hope that the internet didn't fail.", "Be pragmatic, test an idea fast and use the tools that are available. An implemented good idea is 1 billion times better than 100 excellent ideas with no implementation.", "We want grow to 1,000% with our clients. We are looking to reach 3 more countries with our product.", "RINAR Solutions developed a generic platform to handle every information need. While we are focused and testing our product with a niche field (lawyers and legal matters), the idea is to expand to different types of businesses and industries (marketing, industrial, sales).", "Right now, we are testing the platform in two cities in Argentina: Cordoba and Buenos Aires. We are thinking of expanding to more states within Argentina before moving into other Spanish speaking countries. The final plan is to go worldwide once our product and processes have been validated. Scrapinghub has been a crucial partner in scaling up our business. We have gone from offering a service through email to offering a product in the cloud. Scrapinghub has everything we need: a rich platform to deploy, run and control the spiders, an API to interact with, and a python library that matches our Django backend development.", "We hope to keep on the path of evolution."]},
{"tite": "Gender Inequality Across Programming Languages", "date": "May 27, 2015 ", "author": "Richard Dowinton", "blog_data": ["Gender inequality is a hot topic in the tech industry. Over the last several years we\u2019ve gathered business profiles\u00a0for our clients, and we realised this data would prove useful in identifying trends in how gender and employment relate to one another.", "The following study is based on\u00a0UK\u00a0profiles to determine\u00a0the gender of a profile using the given name, which covered approximately 80% of the users. We had collected data from 2010 through to 2015, so we were able to identify changes between each year.", "The following languages were analyzed:", "Ruby by a large margin appears to have the highest percentage of women, and C++ the lowest.", "Gender imbalance seems to be less prominent outside the IT industry, but the percentage of women across languages seems to be increasing over time.", "The source for this study was\u00a0provided by our\u00a0Data Services massive business profiles collection. The\u00a0gender of a profile was inferred by its given name.", "The programming language associated with the user was determined by inspecting the descriptions of the person\u2019s prior experience. Two methodologies were used for this.", "The first methodology, which we\u2019ll refer to as 'Method 1', associated a user with a programming language if the language name appeared in the description.", "As this can lead to a user being assigned more than one language, we also used an alternate methodology, 'Method 2', that assigned a language if that language was the only one which appeared in the description.", "The results presented above were the average of these two methodolgoies", "We considered using people's list of skills, but decided against it as it would've prevented us from retrieving results by year.", "We excluded languages such as BASIC due to ambiguity, and analyzed jobs from 2010 to present. We also excluded languages where there weren't enough jobs to keep our confidence interval below 1% at the 95th percentile.", "If you would like additional information about our methodology or have any suggestions for a study please contact ", "."]},
{"tite": "Winter Sales Showdown: Black Friday vs Cyber Monday vs Green Monday", "date": "December 21, 2015 ", "author": "Cecilia Haynes", "blog_data": ["Welcome to Part 2 of the ongoing series tracing the prices of popular products across multiple vendors throughout the winter season. Starting from Black Friday and continuing through to January, we are keeping a close eye on price movements and the actual effects of discounts during specific days or weekends.", "Last Monday, December 14, was ", ", yet another online sales day. Green Monday takes place on the second Monday in December and is supposedly the third largest discount day after Black Friday and Cyber Monday. This post dishes\u00a0all the dirt on whether or not you missed the boat on rock-bottom deals during Black Friday and Cyber Monday or if Green Monday managed to live up to its more well-known discount relatives.", "So without further ado, let\u2019s dive right into the aftermath.", "I laid out our methodology, parameters, and intervals in ", ", but I am going to continue to update the Tech and Trivia section so that you can follow along with the obstacles and solutions that have cropped up in the course of this project.", "Our graphs show a direct comparison of prices across Black Friday, Cyber Monday, and Green Monday and are divided further by product and by retailer. If there is data missing from certain days, but present on others, this signifies that the product was out of stock or that it is newly being offered by the retailer.", "We have been accumulating data since Black Friday and scraping retailer websites every 30 minutes. In order to keep our spiders running so consistently, we used ", ". As an aside we recently released an updated version of shub so you can do this from the command line. ", " for details.", "We deployed our spiders to Scrapy Cloud, which is as simple as typing ", ". Then it was just a matter of scheduling our spiders to run periodically and letting Scrapy Cloud handle the rest. Apart from running our spiders and storing all of the data collected, Scrapy Cloud allowed\u00a0us to monitor the execution of the spiders and to review the data in its web UI.", "The results\u00a0varied widely. It seems that some companies even extended sale prices from Black Friday onward to continue to capitalize on how well certain products sold\u00a0(so if you wanted an ", ", you\u2019ve still got a shot). This resulted in minimal price variations for a few products in each of the categories from the last three weeks.", "I actually picked up a FitBit Charge HR on Black Friday since I was worried about future price fluctuations going into December. Let\u2019s find out if my prudence served me well or if I could have waited to make my big purchase.", "Tablets actually remained pretty steady over the last few weeks. Most of the big steals for Green Monday ended up being from Best Buy for the iPad Air 2, iPad Mini 4, and the Dell Venue 8 7000. Amazon tapped a little into Green Monday by dipping the price of the iPad Pro 128GB.", "Looks like it was a good thing that I snagged a Fitbit Charge HR when I had the chance! Green Monday was not kind to sales for this particular product.", "Green Monday was clearly the day to pick up an Apple Watch. Although, keep\u00a0in mind that our prices don't reflect gift cards and other bundle deals.", "The cheapest Jawbone UP3 by far and away was from Amazon on Cyber Monday.", "Black Friday seemed to be the best day overall for deals on home theater products. The only Green Monday notable is the\u00a0Google Chromecast offered by Best Buy and Walmart.", "Green Monday definitely showed great form in the Video Game Console category with many prices matching Black Friday or even lower. The Nintendo 3DS XL was an especially great purchase in case you missed the initial sales on Black Friday.", "Phones has been the trickiest category to track since we were facing some stores that sell unlocked and carrier-free products while others offered great deals, but the phones are restricted to specific carriers. So if you see absolute rock-bottom prices, don't kick yourself too much, there might be strings attached.", "In this case, it was honestly hit or miss, although Amazon stands out as having done\u00a0well on Green Monday by\u00a0having equal or lower prices\u00a0for pretty much every single product.", "Green Monday was a great day for toys. For every single product, you could find the lowest price, you just had to go to the right retailer. You might want to keep that in mind for next year when you're scrambling for holiday presents.", "Remember to follow us on ", ", ", ", and ", "or subscribe to our ", " if you\u2019re interested in follow-up posts. We\u2019ll cover Christmas and New Year prices\u00a0next so stay tuned!"]},
{"tite": "EuroPython, here we go!", "date": "June 12, 2015 ", "author": "Giovanni Piazza", "blog_data": ["We are\u00a0very excited about\u00a0EuroPython 2015!", "33 Scrapinghubbers from 15 countries will be meeting (most of them, for the first time) in ", ", for\u00a0what is going to be\u00a0our largest\u00a0get-together event so far. We are also thrilled to have gotten our\u00a0", " (5 talks, 1 poster, 1 tutorial, 1 helpdesk) and couldn't feel\u00a0prouder of being\u00a0a Gold Sponsor.", "Here is a summary of the talks, tutorials and poster sessions\u00a0that our staff will be giving\u00a0at EuroPython 2015.", " at 11:45", "Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.", "In this talk some advanced techniques will be shown based on how Scrapy is used at Scrapinghub.", "yet\u00a0to be defined", "Scrapy is an open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way.", "This helpdesk is run by members of Scrapinghub, where Scrapy was built and designed.", " at 14:30", "If you want to get data from the web, and there are no APIs available, then you need to use web scraping! Scrapy is the most effective and popular choice for web scraping and is used in many areas such as data science, journalism, business intelligence, web development, etc.", "This workshop will provide an overview of Scrapy, starting from the fundamentals and working through each new topic with hands-on examples.", "Participants will come away with a good understanding of Scrapy, the principles behind its design, and how to apply the best practices encouraged by Scrapy to any scraping task.", " at 11:00", "Python is a fantastic language for writing web scrapers. There is a large ecosystem of useful projects and a great developer community. However, it can be confusing once you go beyond the simpler scrapers typically covered in tutorials.", "In this talk, we will explore some common real-world scraping tasks. You will learn best practises and get a deeper understanding of what tools and techniques can be used.", "Topics covered will include:", " at 15:15", "In this talk I\u2019m going to introduce Scrapinghub\u2019s new open source framework\u00a0", ". Frontera allows to build real-time distributed web crawlers and website focused ones.", "Offering:", "Along with framework description I\u2019ll demonstrate how to build a distributed crawler using ", ", Kafka and HBase, and hopefully present some statistics of Spanish internet collected with newly built crawler. Happy EuroPythoning!", "yet\u00a0to be defined", "In this poster session I\u2019m going to introduce Scrapinghub\u2019s new open source framework ", ". Frontera allows to build real-time distributed web crawlers and website focused ones.", "Offering:", "Along with framework description I\u2019ll demonstrate how to build a distributed crawler using ", ", Kafka and HBase, and hopefully present some statistics of Spanish internet collected with newly built crawler. Happy EuroPythoning!", " at 15:45", "How to write a test so you would remember what it does in a year from now? How to write selective tests with different inputs? What is test? How to subclass tests cases and yet maintain control on which tests would run? How to extend or to filter inputs used in parent classes? Are you a tiny bit intrigued now? :)", "This is not another talk about how to test, but how to organize your tests so they were maintainable. I will be using nose framework as an example, however main ideas should be applicable to any other framework you choose. Explaining how some parts of code works I would have to briefly touch some advanced python topics, although I will provide need-to-know basics there, so people with any level of python knowledge could enjoy the ride.", " at 11:00", "CityBikes [1] started on 2010 as a FOSS alternative endpoint (and Android client) to gather information for Barcelona\u2019s Bicing bike sharing service. Later evolved as an open API [2] providing bike sharing data of any (mostly) service worldwide.", "Fast forward today and after some C&D letters, there\u2019s support for more than 200 cities, more than 170M historical entries have been gathered for analysis (in approx. a year) and the CityBikes API is the main source for open bike share data worldwide. This talk will tour about how we got there with the help of python and the community [3].", "PS: We have a realtime map, it is awesome [4].", "[1]: http://citybik.es", " [2]: http://api.citybik.es", " [3]: http://github.com/eskerda/pybikes", " [4]: http://upcoming.citybik.es", "In case you haven't registered for EuroPython yet, you can do it\u00a0", ".", "If you have any suggestions for anything specific you would like to hear us talk about or questions you would like us to answer related to our talks, please tell us in the comments."]},
{"tite": "Aduana: Link Analysis with Frontera", "date": "June 08, 2015 ", "author": "Richard Dowinton", "blog_data": ["It's not uncommon to need to crawl a large number of unfamiliar websites when gathering content. Page ranking algorithms are incredibly useful in these scenarios as it can be tricky to determine which pages are relevant to the content you're looking for.", "In this post we'll show you how you can make use of Aduana and Frontera to implement popular page ranking algorithms in your Scrapy projects.", "Aduana is a library to track the state of crawled pages and decide which page should be crawled next. It is written in C but is wrapped in Python to use it as a Frontera backend. We intend it\u00a0to be the backend for a distributed crawled, where several spiders ask for the next pages to be crawled.", "The driving force behind Aduana design has been speed: we want it to operate fast even when serving multiple spiders even when billions of links have already been discovered. Speed must be maintained even when executing relatively complex algorithms like PageRank or HITS scoring.", "Right now, Aduana implements a Best First scheduler, meaning that pages are ordered by a calculated\u00a0score and the page with highest score is selected to be crawled next. Once a page has been crawled it's never crawled again. Scores in Aduana can be computed in several different ways:", "The advantage of using PageRank or HITS is that they are not application specific, however they can wander off-topic when crawling for a large time and they are computationally more expensive.", "Using specific scores give good results but requires fine tuning the algorithm, for example, to make it\u00a0robust\u00a0against content spam pages. On the plus\u00a0side, they are computationally cheap, at least with Aduana.", "Finally, merging specific scores with PageRank or HITS might give the best of both worlds in terms of accuracy, at the same computational cost that running plain PageRank or HITS.", "To understand how PageRank and HITS are merged with specific scores we first need to understand the plain algorithms.", "PageRank is perhaps the most well known page ranking algorithm. The PageRank algorithm makes use of the link structure of the web to calculate a score which can be used to influence search results. PageRank is one of Google\u2019s methods for determining a page\u2019s relevance.", "One possible way of coming up with PageRank is by modelling the surfing behavior of a user on the web graph. Imagine that at a given instant of time $latex t$ the surfer, $latex S$, is on page $latex i$. We denote this event as $latex S_t = i$. Page $latex i$ has $latex n_i$ outgoing and $latex m_i$ incoming links as depicted in the figure.", "While on page $latex i$ the algorithm can do one of the following to continue navigating:", "What happens if a page has no outgoing links? It's reasonable to assume that the user will not get stuck there forever. Instead, we will assume that it will teleport.", "Now that we have a model of the user behavior let's calculate $latex P(S_t = i)$: the probability of being at page $latex i$ at time instant $latex t$. The total number of pages is $latex N$.", "$latex Pleft(S_t = iright) = sum_{j=1}^N Pleft(S_{t} = i mid S_{t-1} = jright)Pleft(S_{t-1} = jright)$", "Now, the probability of going from page $latex j$ to page $latex i$ is different depending on wether or not $latex j$ links to $latex i$ ($latex j rightarrow i$) or not ($latex j nrightarrow i$).", "If $latex j rightarrow i$ then:", "$latex Pleft(S_{t} = i mid S_{t-1} = jright) = c frac{1}{n_j} + (1 - c)frac{1}{N}$", "If $latex j nrightarrow i$ but $latex n_j neq 0$ then the only possibility is that the user chooses to teleport and it lands on page $latex i$.", "$latex Pleft(S_{t} = i mid S_{t-1}right) = (1 - c)frac{1}{N}$", "If $latex j nrightarrow i$ and $latex n_j = 0$ then the only", " possibility is that the user teleports to page $latex i$.", "$latex Pleft(S_{t} = i mid S_{t-1}right) = frac{1}{N}$", "We have assumed uniform probabilities in two cases: when following outgoing links all links are equally probable and also when teleporting to another page we assume all pages are equally probable. In the next section we will change this second assumption about teleporting in order to get personalized PageRank.", "Putting together the above formulas we obtain after some manipulation:", "$latex Pleft(S_t = iright) = c sum_{j rightarrow i} frac{P(S_{t-1} = j)}{n_j} + frac{1}{N}left(1 - csum_{n_j neq 0}P(S_{t-1} = j)right)$", "Finally, for notational convenience let's call $latex pi_i^t = Pleft(S_t = iright)$:", "$latex pi_i^t = c sum_{j rightarrow i} frac{pi_j^{t-1}}{n_j} + frac{1}{N}left(1 - csum_{n_j neq 0}pi_j^{t-1}right)$", "PageRank is defined as the limit of the above sequence:", "$latex text{PageRank}(i) = pi_i = lim_{t to infty}pi_i^t$", "In practice however PageRank is computed by iterating the above formula a finite number of times, either a fixed number or until changes in the PageRank score are low enough.", "If we look at the formula for $latex pi_i^t$ we see that the PageRank of a page has two parts. One part depends on how many pages are linking to the page but the other part is distributed equally to all pages. This means that all pages are going to get at least:", "$latex frac{1}{N}left(1 - csum_{n_j neq 0}pi_j^{t-1}right) geq frac{1 - c}{N}$", "This gives an oportunity to link spammers to artificially increase the PageRank of any page they want by maintaing link farms, which are huge amounts of pages controlled by the spammer.", "As PageRank will give all pages a minimum score, all of these pages will get some PageRank that can be redirected to the page the spammer wants to rise in search results.", "Spammers will try to achieve a greater effect by linking from pages they don't own to either their spam page or link farm. This can be achieved since lots of pages are user-editable like blog comments, forums or wikis.", "Trying to detect web spam is a never-ending war between search engines and spammers. We will now discuss Personalized PageRank as an attempt to filter out web spam. It works precisely by not giving any free score to undeserving pages.", "Personalized PageRank is obtained very similar to PageRank but instead of a uniform teleporting probability each page has its own probability $latex r_i$ of being teleported to, irrespective of the originating page:", "$latex Pleft(j mathbin{text{teleports to}} i mid j: text{teleports}right) = r_i$", "The update equations are therefore:", " $latex pi_i^t = c sum_{j rightarrow i} frac{pi_j^{t-1}}{n_j} + r_ileft(1 - csum_{n_j neq 0}pi_j^{t-1}right)$", "Of course it must be that:", "$latex sum_{i=1}^N r_i = 1$", "As you can see plain PageRank is just a special case where $latex r_i = 1/N$.", "There are several ways the score $latex r_i$ can be calculated. For example, it could be computed using some text classification algorithm on the page content. Alternatively, it could be set to 1 for some set of seeds pages and 0 for the rest of pages, in which case we get ", ".", "Of course, there are ways to defeat this algorithm:", "HITS (hyperlink-induced topic search) is another link analysis algorithm that assigns two scores: hub score and authority score. A page\u2019s hub score is influenced by the authority scores of the pages linking to it, and vice versa. Twitter makes use of HITS to suggest users to follow.", "The idea is to compute for each page a pair of numbers called the ", " and ", " scores. Intutively we say a page is a hub when it points to lot of pages with high authority, and we say that a page has high authority if it is pointed by many hubs.", "The following graph shows one several pages with one clear hub $latex H$ and two clear authorities $latex A_1$ and $latex A_2$.", "Mathematically this is expressed as:", "$latex h_i = sum_{j: i to j} a_j$", " $latex a_i = sum_{j: j to i} h_j$", "Where $latex h_i$ represents the hub score of page $latex i$ and $latex a_i$ represents its authority score.", "As with PageRank these equations are solved iteratively until they converge to the required precision. HITS was conceived as a ranking algorithm for user queries where the set of pages that were not relevant to the query were filtered out before computing HITS scores. For the purposes of our crawler we make a compromise: authority scores are modulated with the topic specific score $latex r_i$ to give the following modified equations:", "$latex h_i = sum_{j: i to j} r_j a_j$", " $latex a_i = sum_{j: j to i} h_j$", "As we can see totally irrelevant pages ($latex r_j = 0$) don't contribute back authority.", "HITS is slightly more expensive to run than PageRank because it has to maintains two sets of scores and also propagates scores twice. However it is a very interesting algorithm for crawling because by propagating scores back to the parent pages it allows to make better predictions of how good a link is likely to be and right now it's the algorithm that seems to work better, although that might be application specific.", "The web is very much a graph, so it's natural to consider storing all of the information in a graph database. As a bonus most graph databases come with tested implementations of PageRank and in some cases HITS. Not wanting to reinvent the wheel we decided to go with ", " as it's open source and works with HDFS. However, we decided to run the provided ", " on the ", ", and the results were rather poor. We cancelled the job around one and a half hours in as it was already consuming 10GB of RAM.", "It was surprising reading in the paper ", " that 50 Spark nodes took half the time to compute PageRank on a graph with 40MM nodes and 1.5B edges than GraphChi on a single computer, so perhaps it didn't make sense to distribute the work. Here are some of the non-distributed libraries we tested and their timings for the LiveJournal data:", "Please take these timings with a grain of salt. We didn't spend any more than half an hour testing each library nor was there any fine tuning. The important thing here is the order of magnitude.", "We were actually concerned we were doing something wrong while benchmarking the algorithms because the non-distributed results far exceeded the results of the distributed algorithms that it was hard to believe. However, we came across these two great blog posts that confirmed what we had found: ", " and ", ". The second one is particularly interesting as it computes PageRank on a 128 billion edge graph on a single computer!", "With all of this in mind we ended up implementing the database on top of a regular key/value store: ", ". When the PageRank/HITS scores need to be recomputed, the vertex data is stored as a flat array in-memory and the links between them are streamed from the database.", "We chose LMDB for the following reasons:", "Overall using LMDB and implementing the PageRank and HITS algorithms in C has been worth the effort. Even a 128 billion edge graph fits inside a 1TB disk, and the vertex data only requires around 16GB of memory. It's true that we don't have the same generality of full-blown databases or distributed computing frameworks, but for crawling the web it has been much more simple to develop and manage a non-distributed system.", "First install Aduana by cloning the ", " and following the installation steps:", "There's a ready to use example in Aduana's repository which you can find ", ".", "The example implements a very simple content scorer which just counts the number of keywords in the page text without making any tokenization at all.", "You can select any of the above algorithms changing the settings inside ", ":", "Possible values for ", " are ", ", ", " and ", ". If any of the two latest are selected then you can decide setting ", " to ", " or ", " depending on whether you want the topic-focused or plain algorithms.", "When you are satisfied with the settings and any modification you are interested in, like for example actually saving items, just run the crawler:", "You can export the crawled graph to a CSV file (actually the separator is space) even while the crawl is running:", "The fields are, in order:", "To get the links between pages:", "This will output a list of edges where the first element is the index of the originating page and the second element the destination page.", "With the exported files you can perform whatever analysis you want with your favorite graph library, however, for convenience there are some additional tools provided.", " will output URLs and their hashes which match a given ", ".", "This will output all the URLs, and their hashes, that match the given extended ", ".", "Finally, you can get the links, both forward and backwards of a given page with:", "Implementing this in your own project is just a matter of including the Frontera middleware in your settings and configuring it to use the PageDB backend provided by Aduana.", "You can find the relevant Frontera settings for the example in ", ".", "If you aren't sure how to enable Frontera in your project, here are the settings from the example:", "#--------------------------------------------------------------------------", " # Seed loaders", " #--------------------------------------------------------------------------", " SPIDER_MIDDLEWARES.update({", " 'frontera.contrib.scrapy.middlewares.seeds.file.FileSeedLoader': 1,", " })", " SEEDS_SOURCE = 'seeds.txt'", "The ", " setting refers to the settings.py file where we store the Frontera specific settings. The ", " file contains a list of URLs to begin crawling with.", "For more information about using Frontera, see our previous post, ", ".", "Further reading on ranking algorithms:", "Hopefully after reading this you have a better understanding of popular page ranking algorithms and how to use them in your Scrapy projects. If you have any questions feel free to comment!"]},
{"tite": "Using git to manage vacations in a large distributed team", "date": "June 08, 2015 ", "author": "Pablo Hoffman", "blog_data": ["Here at Scrapinghub we are a ", ". As part of their standard contract, Scrapinghubbers get 20 vacation days per year and local country holidays off, and yet we spent almost zero time managing this. How do we do it?. The answer is \u201cgit\u201d and here we explain how.", "We have a github repository where each employee has a (", ") file describing their personal vacations and each country has another one describing their public holidays. Employees are linked to the countries they live in. All this information in structured format is compiled to create an ", " and expose it as a (company accessible) calendar with all staff holidays (including country holidays, medical leave, conferences, etc - it\u2019s all in the YAML file, with that level of detail). Anyone can check this calendar to see who is off this, next week or anytime.", "On top of this we have built some graphs to monitor your accrued vacation days (1.66 days per worked month) and warn you if you haven\u2019t taken holidays in a long time.", "Now here comes the best part: when Scrapinghubbers want to take vacations all they need to do is fork the time off repository and send a pull request to their managers, who merges them to approve the vacation. Because we are all technical people, we wanted to make the time off request process as developer-friendly as possible. Carefully wording a polite email to your manager is a thing of the past!. Just send a pull request!", " ", "For country holidays we apply something similar. In a large distributed team there are many countries involved (in our case, more than 30) and the list of country holidays is better maintained by the people who live in those countries. They know better which holidays apply than a Human Resources person collecting them from Google. So Scrapinghubbers are required to submit (and help maintain) their country holidays pro-actively and responsibly. This is done also by sending pull requests (with changes to the country file) that gets signed off (ie. merged) by the HR manager.", "This model is heavily based around trust and having a technically fluent team (working with pull requests may put non-technical people off or require training) but we are happy to have both. If your company is in a position to try this model, we strongly suggest you give it a try. Even if you can\u2019t apply this company wide, you could try it in specific teams. Having all in structured data means you can deliver this information in any format requested by your company\u2019s HR team. We have been using this method for half a year now and never looked back. We couldn\u2019t think of a better way to manage vacations and country holidays from such a large distributed team that wouldn\u2019t involve large Human Resources overheads. And we hate unnecessary overheads!", "We plan to continue writing about how we manage our distributed team, so stay tuned and ", ". If you would like to work in a strong technical team and can\u2019t wait to request your vacations with git, we are ", "."]},
{"tite": "Link Analysis Algorithms Explained", "date": "June 19, 2015 ", "author": "Richard Dowinton", "blog_data": ["When scraping content from the web, you often crawl websites which you have no prior knowledge of. Link analysis algorithms are incredibly useful in these scenarios to guide the crawler to relevant pages.", "This post aims to provide a lightweight introduction to page ranking algorithms so you have a better understanding of how to implement and use them in your spiders. There will\u00a0be a\u00a0follow up post soon detailing how to use these algorithms in your crawl.", " is perhaps the most well known page ranking algorithm. The PageRank algorithm uses the link structure of the web to calculate a score. This score provides insight into how relevant the page is and in our case can be used to guide the crawler. Many search engines use this score to influence search results.", "One possible way of coming up with PageRank is by modelling the surfing behavior of a user on the web graph. Imagine that at a given instant of time $latex t&bg=ffffff$ the surfer, $latex S&bg=ffffff$, is on page $latex i&bg=ffffff$. We denote this event as $latex S_t = i&bg=ffffff$. Page $latex i&bg=ffffff$ has $latex n_i&bg=ffffff$ outgoing and $latex m_i&bg=ffffff$ incoming links as depicted in the figure.", "While on page $latex i&bg=ffffff$ the algorithm can do one of the following to continue navigating:", "So what happens if a page has no outgoing links? It's reasonable to assume the user won't stick around, so it's assumed that the user will 'teleport', meaning they will visit another page through different means such as entering the address manually.", "Now that we have a model of the user behavior let's calculate $latex P(S_t = i)&bg=ffffff$: the probability of being at page $latex i&bg=ffffff$ at time instant $latex t&bg=ffffff$. The total number of pages is $latex N&bg=ffffff$.", "$latex P\\left(S_t = i\\right) = \\sum_{j=1}^N P\\left(S_{t} = i \\mid S_{t-1} = j\\right)P\\left(S_{t-1} = j\\right)&bg=ffffff$", "Now, the probability of going from page $latex j&bg=ffffff$ to page $latex i&bg=ffffff&bg=ffffff$ is different depending on wether or not $latex j&bg=ffffff$ links to $latex i&bg=ffffff$ ($latex j \\rightarrow i&bg=ffffff$) or not ($latex j \\nrightarrow i&bg=ffffff$).", "If $latex j \\rightarrow i&bg=ffffff$ then:", "$latex P\\left(S_{t} = i \\mid S_{t-1} = j\\right) = c \\frac{1}{n_j} + (1 - c)\\frac{1}{N}&bg=ffffff$", "If $latex j \\nrightarrow i&bg=ffffff$ but $latex n_j \\neq 0&bg=ffffff$ then the only possibility is that the user chooses to teleport and it lands on page $latex i&bg=ffffff$.", "$latex P\\left(S_{t} = i \\mid S_{t-1}\\right) = (1 - c)\\frac{1}{N}&bg=ffffff$", "If $latex j \\nrightarrow i&bg=ffffff$ and $latex n_j = 0&bg=ffffff$ then the only possibility is that the user teleports to page $latex i&bg=ffffff$.", "$latex P\\left(S_{t} = i \\mid S_{t-1}\\right) = \\frac{1}{N}&bg=ffffff$", "We have assumed uniform probabilities in two cases:", "In the next section we'll remove this second assumption about teleporting in order to calculate personalized PageRank.", "Using the formulas above, with some manipulation we obtain:", "$latex P\\left(S_t = i\\right) = c \\sum_{j \\rightarrow i} \\frac{P(S_{t-1} = j)}{n_j} + \\frac{1}{N}\\left(1 - c\\sum_{n_j \\neq 0}P(S_{t-1} = j)\\right)&bg=ffffff$", "Finally, for convenience let's call $latex \\pi_i^t = P\\left(S_t = i\\right)&bg=ffffff$:", "$latex \\pi_i^t = c \\sum_{j \\rightarrow i} \\frac{\\pi_j^{t-1}}{n_j} + \\frac{1}{N}\\left(1 - c\\sum_{n_j \\neq 0}\\pi_j^{t-1}\\right)&bg=ffffff$", "PageRank is defined as the limit of the above sequence:", "$latex \\text{PageRank}(i) = \\pi_i = \\lim_{t \\to \\infty}\\pi_i^t&bg=ffffff$", "In practice, however, PageRank is computed by iterating the above formula a finite number of times: either a fixed number or until changes in the PageRank score are low enough.", "If we look at the formula for $latex \\pi_i^t&bg=ffffff$ we see that the PageRank of a page has two parts. One part depends on how many pages are linking to the page but the other part is distributed equally to all pages. This means that all pages are going to get at least:", "$latex \\frac{1}{N}\\left(1 - c\\sum_{n_j \\neq 0}\\pi_j^{t-1}\\right) \\geq \\frac{1 - c}{N}&bg=ffffff$", "This gives an oportunity to link spammers to artificially increase the PageRank of any page they want by maintaing link farms, which are huge amounts of pages controlled by the spammer.", "As PageRank will give all pages a minimum score, all of these pages will have some PageRank that can be redirected to the page the spammer wants to rise in search results.", "Spammers will try to build backlinks to their pages by linking to their sites on pages they don't own. This is most common on blog comments and forums where content is accepted from users.", "Trying to detect web spam is a never-ending war between search engines and spammers. To help filter out spam we can use Personalized PageRank which works by not assigning a free score to undeserving pages.", " is obtained very similar to PageRank but instead of a uniform teleporting probability, each page has its own probability $latex r_i&bg=ffffff$ of being teleported to irrespective of the originating page:", "$latex P\\left(j \\mathbin{\\text{teleports to}} i \\mid j\\: \\text{teleports}\\right) = r_i&bg=ffffff$", "The update equations are therefore:", " $latex \\pi_i^t = c \\sum_{j \\rightarrow i} \\frac{\\pi_j^{t-1}}{n_j} + r_i\\left(1 - c\\sum_{n_j \\neq 0}\\pi_j^{t-1}\\right)&bg=ffffff$", "Of course it must be that:", "$latex \\sum_{i=1}^N r_i = 1&bg=ffffff$", "As you can see plain PageRank is just a special case where $latex r_i = 1/N&bg=ffffff$.", "There are several ways the score $latex r_i&bg=ffffff$ can be calculated. For example, it could be computed using some text classification algorithm on the page content. Alternatively, it could be set to 1 for some set of seeds pages and 0 for the rest of pages, in which case we get ", ".", "Of course, there are ways to defeat this algorithm:", " is another link analysis algorithm that assigns two scores: hub score and authority score. A page\u2019s hub score is influenced by the authority scores of the pages linking to it, and vice versa. Twitter makes use of HITS to suggest users to follow.", "The idea is to compute for each page a pair of numbers called the ", " and ", " scores. A page is considered a hub when it points to lot of pages with high authority, and page has high authority if it's pointed to by many hubs.", "The following graph shows one several pages with one clear hub $latex H&bg=ffffff$ and two clear authorities $latex A_1&bg=ffffff$ and $latex A_2&bg=ffffff$.", "Mathematically this is expressed as:", "$latex h_i = \\sum_{j: i \\to j} a_j&bg=ffffff$", " $latex a_i = \\sum_{j: j \\to i} h_j&bg=ffffff$", "Where $latex h_i$ represents the hub score of page $latex i$ and $latex a_i&bg=ffffff$ represents its authority score.", "Similar to PageRank, these equations are solved iteratively until they converge to the required precision. HITS was conceived as a ranking algorithm for user queries where the set of pages that were not relevant to the query were filtered out before computing HITS scores.", "For the purposes of our crawler we make a compromise: authority scores are modulated with the topic specific score $latex r_i&bg=ffffff$ to give the following modified equations:", "$latex h_i = \\sum_{j: i \\to j} r_j a_j&bg=ffffff$", " $latex a_i = \\sum_{j: j \\to i} h_j&bg=ffffff$", "As we can see totally irrelevant pages ($latex r_j = 0&bg=ffffff$) don't contribute back authority.", "HITS is slightly more expensive to run than PageRank because it has to maintains two sets of scores and also propagates scores twice. However, it's particularly useful for crawling as it propagates scores back to the parent pages, providing a more accurate prediction of the strength of a link.", "Further reading:", "We hope this post has served as a good introduction to page ranking algorithms and their usefulness in web scraping projects. Stay tuned for our next post where we'll show you how to use these algorithms in your Scrapy projects!"]},
{"tite": "PyCon Philippines 2015", "date": "July 15, 2015 ", "author": "Richard Dowinton", "blog_data": ["Earlier this month we attended PyCon Philippines as a gold sponsor, presenting on the 2nd day. This was particularly exciting as it was the first time the whole Philippines team was together in one place and it was nice meeting each other in person!", "Checkout the S", "The talk started with how people would scrape manually in the past, the pain of dealing with handling timeouts, retries, HTTP errors and so forth. We presented Scrapy as a solution to these issues and explained how to address them, as well as giving a brief history of how Scrapy came to be.", "We proceeded with a live demo showing how to scrape the Republic Acts of the Philippines from the Philippine government website as well as scraping ", " to retrieve cinema screening schedules in Metro Manila. Some of the audience joined in during the demo and we helped answer their questions.", "We also talked about some of the projects we have done for customers at Scrapinghub:", "We then showed a number of side projects including:", "Finally we discussed some of the legalities of scraping, of which there were a lot of questions. We also had many questions on how we deal with complaints and sites blocking us, and how to deal with sites that make heavy use of JavaScript and AJAX.", "Afterwards we had dinner in a nearby mall with the other speakers and it was great meeting like minded people from the Python community."]},
{"tite": "Google Summer of Code 2015", "date": "June 25, 2015 ", "author": "Pablo Hoffman", "blog_data": ["We are very excited to be participating again this year on Google Summer of Code. After a successful experience last year where ", " (now a proud Scrapinghubber!) worked on ", ", we are back again this year with 3 ideas approved:", "We would like to thank the Python Software Foundation for taking us again this year and wish the best luck to our students and all Summer of Code participants."]},
{"tite": "StartupChats Remote Working Q&A", "date": "July 17, 2015 ", "author": "Richard Dowinton", "blog_data": ["Earlier this week, Scrapinghub was invited along with several other fully-distributed companies to participate in a remote working Q&A hosted by Startups Canada.", "The various companies invited, as well as their guests attending, shared their insights into a number of questions related to building a remote company and being a remote worker.", "We\u2019re sharing our answers and our favourite answers from other companies.", "[tweet https://twitter.com/ScrapingHub/status/621351235048701952]", "[tweet https://twitter.com/redman/status/621350633673740288]", "[tweet https://twitter.com/businessfriend/status/621350241615220736]", "[tweet https://twitter.com/redman/status/621351303084683265]", "[tweet https://twitter.com/alextbrown/status/621352421067853824]", "[tweet https://twitter.com/ScrapingHub/status/621352498062753792]", "[tweet https://twitter.com/ScrapingHub/status/621352771388751872]", "[tweet https://twitter.com/ScrapingHub/status/621354056800342016]", "[tweet https://twitter.com/ScrapingHub/status/621354320852709380]", "[tweet https://twitter.com/myworkabode/status/621354089020952576]", "[tweet https://twitter.com/QuickBooksCA/status/621354180834406400]", "[tweet https://twitter.com/redman/status/621353717925838848]", "[tweet https://twitter.com/ScrapingHub/status/621355168022446080]", "[tweet https://twitter.com/businessfriend/status/621355950201438208]", "[tweet https://twitter.com/redman/status/621356439379013632]", "[tweet https://twitter.com/ScrapingHub/status/621356647072448512]", "[tweet https://twitter.com/ScrapingHub/status/621357383671898112]", "[tweet https://twitter.com/ScrapingHub/status/621358569695244289]", "[tweet https://twitter.com/ScrapingHub/status/621359197528068096]", "[tweet https://twitter.com/MindofSari/status/621358218850246657]", "[tweet https://twitter.com/merhl/status/621359348627836928]", "[tweet https://twitter.com/ScrapingHub/status/621360195940188160]", "[tweet https://twitter.com/ScrapingHub/status/621360861521645568]", "[tweet https://twitter.com/merhl/status/621360533833265152]", "[tweet https://twitter.com/redman/status/621361221233700864]", "[tweet https://twitter.com/redman/status/621361275608670209]", "[tweet https://twitter.com/ScrapingHub/status/621361662021386240]", "[tweet https://twitter.com/ScrapingHub/status/621362867107794945]", "[tweet https://twitter.com/wadefoster/status/621363235422232578]", "[tweet https://twitter.com/redman/status/621362980714872832]", "[tweet https://twitter.com/ScrapingHub/status/621364458527461377]", "[tweet https://twitter.com/ScrapingHub/status/621364996862119936]", "[tweet https://twitter.com/ScrapingHub/status/621366315467444228]", "Thanks to everyone at QuickBooks and Startup Canada, as well as Joe Johnson, Wade Foster, Alex Brown and Tom Redman!"]},
{"tite": "EuroPython 2015", "date": "July 21, 2015 ", "author": "Richard Dowinton", "blog_data": ["EuroPython 2015 is happening this week and we\u2019re having the largest company meetup so far as a part of it, with more than 30 members from our fully remote-working team attending. The event which is held in Bilbao started on Monday and is providing great quality talks, sessions and plenty of tasty Spanish dishes.", "As sponsors we\u2019ve been privileged with a nice booth where we are connecting with Pythonistas, discussing scraping practises and giving away lots of swag such as hats, t-shirts, stickers, bottle-openers and clocks.", "If you\u2019re attending to EuroPython, make sure to drop by and get one of these. Our booth is in the main area (exhibition hall and lounge area) of the Euskalduna Conference Center. :)", "In the conference\u2019s first day Scrapinghubbers participated with two talks: one about Frontera, hosted by Alexander Sibiryakov, and one about testing, hosted by Eugene Amirov - both available below.", "By Tuesday we hosted two more talks: one about best practises on web scraping, by Shane Evans, and one about Scrapy, by Juan Riaza - also available below. In addition to that, we\u2019ve had a poster session about Frontera and a recruiting session sharing job opportunities and the perks from working for Scrapinghub.", "In the third day Lluis Esquerda hosted a talk about City Bikes, a personal project developed by him on bike sharing networks (check the video below). As the night came we went to Hotel Ercilla, where EuroPython organizers were hosting the conference's social event (a.k.a. \"Pyntxos Night\"). There, we were able to enjoy the nice food, other attendees' companionship and also to have some tricky moves on the dance for. :)", "After resting from the party, Thursday came and with it Juan Riaza's Scrapy Helpdesk. We invited attendees to join us and learn about Scrapy, also getting a couple of swags in our booth.", "This Friday Juan Riaza hosted a 3 hours training on Scrapy, sharing tips and building spiders in real time with the attendees. We also went to the Guggenheim Museum where we met the \"", "\" or, as some people say, one of Scrapinghub's ancestor.", "Here are the full recorded version of the talks given by Scrapinghubbers:", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "\u00a0", "It was an amazing experience, our special thanks to the EuroPython organization and for all team members that made it happen!"]},
{"tite": "Scrapinghub Crawls the Deep Web", "date": "February 24, 2015 ", "author": "Shane Evans", "blog_data": ["\"The easiest way to think about Memex is: How can I make the unseen seen?\"", "-- Dan Kaufman, director of the innovation office at DARPA", "Scrapinghub is participating in Memex, an ambitious DARPA project that tackles the huge challenge of crawling, indexing, and making sense of areas of the Deep Web, that is, web content not being indexed by traditional search engines such as Google, Bing and others. This content, according to current estimations, dwarfs Google\u2019s total indexed content by a ratio of almost 20 to 1. It includes all sorts of criminal activity that, until Memex became available, had proven to be really hard to track down in a systematic way.", "The inventor of Memex, Chris White, appeared on 60 Minutes to explain how it works and how it could revolutionize law enforcement investigations:", "\u00a0", "\u00a0", "Scrapinghub will be participating alongside Cloudera, Elephant Scale and Openindex as part of the Hyperion Gray team. We\u2019re delighted to be able to bring our web scraping expertise and open source projects, such as ", ", ", " and ", ", to a project that has such a positive impact in the real world.", "We hope to share more news regarding Memex and Scrapinghub in the coming months!"]},
{"tite": "Skinfer: A Tool for Inferring JSON Schemas", "date": "March 05, 2015 ", "author": "Elias Dorneles", "blog_data": ["Imagine that you have a lot of samples for a certain kind of data in JSON format. Maybe you want to have a better feel of it, know which fields appear in all records, which appear only in some and what are their types. In other words, you want to know the ", " for the data that you have.", "We'd like to present you ", ", a tool that we built for inferring the schema from samples in JSON format. Skinfer will take a list of JSON samples and give you one ", " that describes all of the samples. (For more information about JSON Schema, we recommend the online book ", ".)", "Install skinfer with ", ", then generate a schema running the command ", " passing a list of JSON samples (it can be a ", " file with all samples or a list of JSON files passed via the command line).", "Here is an example of usage with a simple input:", "Once you've generated a schema for your data, you can:", "Another interesting feature of Skinfer is that it can also ", ", giving you a new schema that describes samples from all previously given schemas. For this, use the ", " command passing it a list of schemas.", "This is cool because you can continuously keep updating a schema even after you've already generated it: you can just merge it with the one you already have.", "Feel free to ", ", ", " and please ", ". :)"]},
{"tite": "The History of Scrapinghub", "date": "March 16, 2015 ", "author": "Joanne O'Flynn", "blog_data": ["Scrapinghub may be a very young company but it already has a great story to tell. Shane Evans from Ireland and Pablo Hoffman from Uruguay came together in a meeting of great web crawling minds to form the business after working together on the same project but for different companies.", "In 2007, Shane was leading software development for MyDeco, a London-based startup. Shane\u2019s team needed data to develop the MyDeco systems and set about trying to find an appropriate company that they could trust to deliver the data. Frustrated with the lack of high quality software and services available, Shane took the matter into his own hands and he decided to create a framework with his team to build web crawlers to the highest standard.", "This turned out well and it didn\u2019t take long to write plugins for the most important websites that they wanted to obtain data from. Continued support for more websites and maintenance of the framework was required so Shane looked to find someone to help.", "Meanwhile, after studying computer science and electrical engineering, Pablo graduated in 2007. Soon after graduating, he set up Insophia, a Python development outsourcing company in Montevideo, Uruguay. One of Pablo\u2019s main clients recommended Insophia to MyDeco and it wasn\u2019t long before he was running the MyDeco web scraping team and helping to develop the data processing architecture.", "Pablo could see massive potential in the code and asked Shane six months later if they could open source this web crawler. Cleverly combining the words \u2018Scrape\u2019 and \u2018Python\u2019, the ", " project was born. After Pablo spent many months refining it and releasing updates, Scrapy became quite popular and word reached his ears that several high-profile companies, including a social media giant, were using their technology!", "In 2010, after developing a fantastic working relationship, Shane and Pablo could see there was an opportunity to start a company that could really make a difference by providing web crawling services, while continuing to advance open source projects. The two men decided to go into business together with one goal: To make it easier to get structured data from the internet.", "With a tight and knowledgeable core group of developers and a huge drive to provide the most functional and efficient web crawlers, Shane and Pablo formed Scrapinghub. They started off as just a handful of hard-working programmers; by 2011 there were 10 employees, by 2012 there were 20 and staff numbers continued to double each year, up to the present where there are now almost 100 employees - or Scrapinghubbers as they affectionately call themselves - globally dedicated to developing the best web crawling and data processing solutions.", "In 2014 alone, the company scraped and stored data from more than 10 billion pages (more than five times the amount the company did in 2013) and an extra 5 billion have passed through Crawlera. 2015 is already off to a great start with the release of two new open source projects: ", " and ", ", a tool for inferring JSON schemas. The icing on the cake is Scrapinghub\u2019s ", " of its participation in the DARPA project Memex. It\u2019s testament to the knowledge and experience of a very dedicated team working together all around the world. From small beginnings come great things and it\u2019s clear that for Scrapinghub, a very bright future awaits."]},
{"tite": "Handling JavaScript in Scrapy with Splash", "date": "March 02, 2015 ", "author": "Richard Dowinton", "blog_data": ["A common roadblock when developing spiders is dealing with sites that use a heavy amount of JavaScript. Many modern websites run entirely on JavaScript, and require scripts to be run in order for the page to render properly. In many cases, pages also present modals and other dialogues that need to be interacted with to show the full page. In this post we\u2019re going to show you how you can use Splash to handle JavaScript in your Scrapy projects.", " is our in-house solution for JavaScript rendering, implemented in Python using ", " and ", ". Splash is a lightweight web browser which is capable of processing multiple pages in parallel, executing custom JavaScript in the page context, and much more. Best of all, it\u2019s open source!", "The easiest way to set up Splash is through ", ":", "Splash will now be running on localhost:8050. If you\u2019re using a Docker Machine on OS X or Windows, it will be running on the IP address of Docker\u2019s virtual machine.", "If you would like to install Splash without using Docker, please refer to the ", ".", "Now that Splash is running, you can test it in your browser:", "On the right enter a URL (e.g. http://amazon.com) and click 'Render me!'. Splash will display a screenshot of the page as well as charts and a list of requests with their timings. At the bottom you should see a text box containing the rendered HTML.", "You can use ", " to send links to Splash:", "If you\u2019re using ", ", the easiest way is to override the ", " function in your spider to replace links with their Splash equivalents:", "The preferred way to integrate Splash with Scrapy is using ", ". See ", " for why it's recommended you use the middleware instead of using it manually. You can install scrapy-splash using pip:", "To use scrapy-splash in your project, you first need to enable the middleware:", "The middleware needs to take precedence over HttpProxyMiddleware, which by default is at position 750, so we set the middleware positions to numbers below 750.", "You then need to set the ", " setting in your project's settings.py:", "Don\u2019t forget, if you\u2019re using a Docker Machine on OS X or Windows, you will need to set this to the IP address of Docker\u2019s virtual machine, e.g.:", "Enable ", " to support ", " feature: it allows to save disk space by not storing duplicate Splash arguments multiple times in a disk request queue. If Splash 2.1+ is used the middleware also allows to save network traffic by not sending these duplicate arguments to Splash server multiple times.", "Scrapy currently doesn\u2019t provide a way to override request fingerprints calculation globally, so you will also have to set a custom ", " and a custom cache storage backend:", "If you already use another cache storage backend, you will need to subclass it and replace all calls to ", " with ", ".", "Now that the Splash middleware is enabled, you can use ", " in place of ", " to render pages with Splash.", "For example, if we wanted to retrieve the rendered HTML for a page, we could do something like this:", "The \u2018args\u2019 dict contains arguments to send to Splash. You can find a full list of available arguments in the ", ". By default the endpoint is set to \u2018render.json\u2019, but here we have overridden it and set it to \u2018render.html\u2019 to provide an HTML response.", "Sometimes you may need to press a button or close a modal to view the page properly. Splash lets you run your own JavaScript code within the context of the web page you\u2019re requesting. There are several ways you can accomplish this:", "You can use the ", " parameter to send the JavaScript you want to execute. The JavaScript code is executed after the page finished loading but before the page is rendered. This allow to use the JavaScript code to modify the page being rendered. For example, you can do it with Scrapy-Splash:", "Splash supports Lua scripts through its ", " endpoint. This is the preferred way to execute JavaScript as you can preload libraries, choose when to execute the JavaScript, and retrieve the output.", "Here\u2019s an example script:", "You need to send that script to the ", " endpoint, in the ", " argument.", " This will return a JSON object containing the title:", "Every script requires a main function to act as the entry point. You can return a Lua table which will be rendered as JSON, which is what we have done here. We use the ", " function to tell Splash to visit the URL. The ", " function lets you execute JavaScript within the page context, however, if you don't need the result you should use ", " instead.", "You can test your Splash scripts in your browser by visiting your Splash instance\u2019s index page (e.g. http://localhost:8050/). It\u2019s also possible to use Splash with IPython notebook as an interactive web-based development environment, see ", " for more details.", "It\u2019s often the case that you need to click a button before the page is displayed. We can do that using\u00a0", " function:", "Here we use ", " to define a function that will return the element coordinates, then make sure the element is visible with ", ", and click on the element. Splash then returns the rendered HTML.", "You can find more info on running JavaScript with Splash in the ", ", and for a more in-depth tutorial, check out the ", ".", "We hope this tutorial gave you a nice introduction to Splash, and please let us know if you have any questions or comments!"]},
{"tite": "Why we moved to Slack", "date": "March 16, 2015 ", "author": "Pablo Hoffman", "blog_data": ["We are veterans in the chat group arena. We have been using one form of another since we started Scrapinghub in 2010 and I've been personally using corporate group chats since 2004. We started Scrapinghub\u00a0using\u00a0our own hosted version of ", ", then moved to\u00a0", "\u00a0in 2013 and we just finished\u00a0moving\u00a0to ", ". Thanks to Slack's\u00a0migration tools, the process\u00a0went pretty\u00a0smoothly. In this post we\u00a0explain\u00a0why we moved and why\u00a0", "\u00a0is a better fit for us.", "Slack is much better for controlling noise, specially in high volume accounts. You can control notification on a per-channel basis, and even disable notifications entirely for a channel. This was a clear win over HipChat.", "Slack search is simply awesome. It is so better than HipChat's that\u00a0a comparison sounds absurd. With Slack search you actually find stuff, allowing you to search over all channels & private communications just as easily as searching into a single room.", "Slack has a much more visual pleasing UI; avatars are shown alongside messages, and the application as a whole is much more vibrant and colorful. A few things\u00a0we really like about Slack is you can see a nice summary of your recent mentions and the ability to star things (messages, files and people) for quick access (very useful for todo lists & reminders).", "Migrating from HipChat to Slack was a breeze. Slack allows easy importing of logs from HipChat (among other chat services) with a great ", " for\u00a0doing so.", "The single-channel guest accounts works better for us because we give those to our clients instead of having to add a new paid users which also have access to all our channels. This allows us to use channels,\u00a0open\u00a0to all the company staff by default, which aligns better with our culture of inclusion & open discussion.", "Another benefit of Slack is that you can sign into multiple teams simultaneously which got rid of the problem we had before with clients that use HipChat in their own company.", "Even though Slack (as opposed to HipChat) does not offer a native Linux app, the Linux experience is miles ahead on Slack using the Chrome desktop app, which works pretty much like a native app.", ": Slack released a Linux app last week (currently in beta). It works the same way as the Chrome desktop app. Like all the other desktop app (Mac, Windows) it embed the web app, providing a 100% consistent experience across all platforms.", "Slack integration\u00a0are very well implemented\u00a0and easy to setup,\u00a0with a ", "\u00a0connecting to it, way above the standard integration you find off the shelf in other services (Twitter, etc). The builtin support for RSS feeds subscription has proven very useful for us, to follow different public things, from Google Alerts in our marketing team to StackOverflow questions by our support team.", "We did find some downsides on Slack compared to HipChat, that we outline below:", "And this has been our experience with the HipChat\u2192Slack transition, let us know in the comments if you have any question."]},
{"tite": "Scrapinghub: A Remote Working Success Story", "date": "March 17, 2015 ", "author": "Joanne O'Flynn", "blog_data": ["When Scrapinghub came into the world in 2010, one thing we wanted was for it to be a company which could be powered by a global workforce, each individual working remotely from anywhere in the world.", " Reduced commuting time and as a consequence increased family time were the primary reasons for this. In Uruguay, Pablo was commuting long distances to do work which realistically could have be done just as easily from home, and \u00a0Shane wanted to divide his time between Ireland, London and Japan. Having a regular office space was never going to work out for these guys.", "Where we are based!", "From the employee\u2019s point of view, as well as eliminating the daily commute and wiping out the associated costs - fuel, parking, tax and insurance if \u00a0you own a car, or bus/train fares if relying on public transport - \u00a0remote working allows you to work in an office space of your choosing. You decorate and fit out your own space to your own tastes. No more putting up with the pitfalls of open plan, with distractions and interruptions possible every minute of the day. No more complaining about air con or lack thereof. How often have you picked up a cold or flu from a sick co-worker? The spread of colds and other illnesses is a huge disadvantage to a shared working space.", "Yes an open plan environment is good for collaboration but with the likes of Skype and Google Hangouts, we get all the benefits of face-to-face communication in an instant. All you need is a webcam, mic and a Google+ or Skype account. Simple! We can hold meetings, conduct interviews, brainstorm and share presentations.", " For real time messaging, HipChat and Slack are the primary team communication tools used by remote companies. In Scrapinghub, we use Slack as a platform to bring all of our communication together in one place. It\u2019s great for real-time messaging, as well as sharing and archiving documents. It encourages daily intercommunication and significantly reduces the amount of emails sent.", "From an employer\u2019s point of view, a major benefit of a fully remote company is huge cost savings on office rent. This in particular is important for small start-ups who might be tight on initial cashflow. Other benefits include having wider access to talent and the fact that remote working is a huge selling point to potential hires.", "The big downside of remote working from an employers\u2019 perspective is obviously productivity or a worry of reduced productivity if an employee works unsupervised from home. ", " shows that productivity will actually increase when people are trusted by their company to work remotely. This is mainly due to a quieter environment. \u00a0Some employers are very slow to change from a traditional workplace to remote working because of productivity worries.", " Whether productivity increases or decreases completely depends on the inner workings of the individual business, and if a culture of creativity, trust and motivation exists, then it\u2019s the perfect working model.", "Scrapinghub regularly holds a virtual office tour day. So often we have meetings via Hangouts and we get little glimpses into our colleagues' offices often on the other side of the world. A poster or book spine might catch the eye, and these virtual office tour days are a way of learning more about each other, stepping into our colleague's space for just a minute and seeing what it's like on their end.", " Social interaction is also encouraged through the use of an off topic channel on Slack and different communities on Google+ such as Scholars, Book Club and Technology. Scrapinghubbers can discuss non-work related issues this way and many team members meet up with their colleagues from around the world when they are travelling.", "\u00a0", "\u00a0"]},
{"tite": "Frontera: The Brain Behind the Crawls", "date": "April 22, 2015 ", "author": "Richard Dowinton", "blog_data": ["At Scrapinghub we're always building and running large crawls\u2013last year we had 11 billion requests made on Scrapy Cloud alone. Crawling millions of pages from the internet requires more sophistication than getting a few contacts of a list, as we need to make sure that we get reliable data, up to date lists of item pages and are able to optimise our crawl as much as possible.", "From these complex projects emerge technologies that can be used across all of our spiders, and we're very pleased to release Frontera, a flexible frontier for web crawlers.", ", formerly Crawl Frontier, is an open source framework we developed to facilitate building a crawl frontier, helping manage our crawling logic and sharing it between spiders in our Scrapy projects.", "A crawl frontier is the system in charge of the logic and policies to follow when crawling websites, and plays a key role in more sophisticated crawling systems. It allows us to set rules about what pages should be crawled next, visiting priorities and ordering, how often pages are revisited, and any behaviour we may want to build into the crawl.", "While Frontera was originally designed for use with Scrapy, it\u2019s completely agnostic and can be used with any other crawling framework or standalone project.", "In this post we\u2019re going to demonstrate how Frontera can improve the way you crawl using Scrapy. We\u2019ll show you how you can use Scrapy to scrape articles from Hacker News while using Frontera to ensure the same articles aren\u2019t visited again in subsequent crawls.", "The frontier needs to be initialised with a set of starting URLs (seeds), and then the crawler will ask the frontier which pages should visit. As the crawler visits pages it will inform back to the frontier of each page\u2019s response and extracted URLs.", "The frontier will decide how to use this information according to the defined logic. This process continues until an end condition is reached. Some crawlers may never stop, we refer to these as continuous crawls.", "Hopefully you're now familiar with what Frontera does. If not, have take a look at ", " for more theory on how a crawl frontier works.", "You can checkout the project we'll be developing in this example from ", ".", "Let\u2019s start by creating a new project and spider:", "You should have a directory structure similar to the following:", "Due to the way the spider template is set up, your ", " in ", " will look like this:", "So you will want to correct it like so:", "We also need to create an item definition for the article we're scraping:", "Here the ", " field will refer to the outbound URL, the ", " to the article's title, and the ", " to HN's item ID.", "We then need to define a link extractor so Scrapy will know which links to follow and extract data from.", "Hacker News doesn\u2019t make use of CSS classes for each item row, and another problem is that the article's item URL, author and comments count are on a separate row from the article title and outbound URL. We\u2019ll need to use XPath in this case.", "First let's gather all of the rows containing a title and outbound URL. If you inspect the DOM, you will notice these rows contain 3 cells, whereas the subtext rows contain 2 cells. So we can use something like the following:", "We then iterate over each row, retrieving the article URL and title, and we also need to retrieve the item URL and author from the subtext row, which we can find using the ", " axis. You should create a method similar to the following:", "The ", " method is a helper function to extract the first result:", "There\u2019s currently a bug with Frontera's SQLalchemy middleware where callbacks aren\u2019t called, so right now we need to inherit from Spider and override the ", " method and make it call our ", " function. Here's an example of what the spider should look like:", "Now all we need to do is configure the Scrapy project to use Frontera with the SQLalchemy middleware. First install Frontera:", "First enable Frontera's middlewares and scheduler by adding the following to ", ":", "Next create a file named ", ", as specified above in ", ", to store any settings related to the frontier:", "Here we specify ", " as the SQLite database file, which is where Frontera will store pages it has crawled.", "Let\u2019s run the spider:", "You can review the items being scraped in ", " while the spider is running.", "You will notice the ", " file we specified earlier will be created. You can browse it using the sqlite3 command line tool:", "As shown above, the database has one table, pages, which stores the URL, its fingerprint, timestamp and response code. This schema is specific to the SQLalchemy backend, and different backends will may use different schemas, and some don't persist crawled pages at all.", "Frontera backends aren't limited to storing crawled pages; they're the core component of Frontera, and hold all crawl frontier related logic you wish to make use of, so which backend you use is heavily tied to what you want to achieve with Frontera.", "In many cases you will want to create your own backend. This is a lot easier than it sounds, and you can find all the information you need in the ", ".", "Hopefully this tutorial has given you a good insight into Frontera and how you can use it to improve the way you manage your crawling logic. Feel free to checkout the ", " and ", ". If you run into a problem please report it at the ", "."]},
{"tite": "Scrape Data Visually with Portia and Scrapy Cloud", "date": "April 07, 2015 ", "author": "Richard Dowinton", "blog_data": ["It\u2019s been several months since we first integrated ", " into our Scrapy Cloud platform, and last week we officially began to phase out Autoscraping in favor of Portia.", "In case you aren\u2019t familiar with Portia, it\u2019s an open source tool we developed for visually scraping websites. Portia allows you to make templates of pages you want to scrape and uses those templates to create a spider to scrape similar pages.", "\u00a0", "\u00a0", "Autoscraping is our predecessor to Portia, and for the time being it\u2019s still available to users who already have Autoscraping-based projects. Any new projects as well as existing projects without Autoscraping spiders will be only be able to use Portia.", "In this post we\u2019re going to introduce Portia by creating a spider for ", ". Let's start by creating a new Portia project:", "Once the project has been created, you will be redirected to the main Portia screen:", "To create a new spider for the project, begin by entering the website URL in Portia\u2019s address bar and clicking the \u2018New Spider\u2019 button. Portia will create a new spider and display the page:", "You can navigate the site like you normally would until you find a page containing data you want to scrape. Sites which require JavaScript to render aren\u2019t currently supported.", "Once you\u2019ve found a page with data you\u2019re interested in, click the \u2018Annotate this page\u2019 button at the top to create a new template.", "You will notice that hovering over an element will highlight it, and clicking it will create an annotation. An annotation defines a mapping between an element\u2019s attribute or content to a field in an item you wish to scrape.", "In the screenshot above we have clicked the title of a recipe. On the left of the annotation window you will see an \u2018Attribute\u2019 dropdown. This allows you to select which part of the element you wish to map. In this case we\u2019re going to map the content, but when annotating elements like images you may want to select a different attribute such as the \u2018src\u2019 value.", "The value which will be extracted for this particular page is shown in the middle of the annotation window under \u2018Value\u2019. On the right you can select the field to map the attribute to. Because new projects are created with a default item, there are already fields we can map to.", "Let\u2019s say we don\u2019t want to use the default item. For the time being we will discard the annotation by clicking the red trash can icon at the top of the annotation window.", "Move your mouse to the right to display the right-hand sidebar and expand the \u2018Extracted item\u2019 tab. You will notice the current extracted item type will be \u2018default\u2019, click the \u2018Edit items\u2019 button.", "Here you can edit the default item and its fields, as well as create more items if you wish. In this example we\u2019ll simply edit the default item:", "Click \u2018Save changes\u2019 and you will now be able to map elements to your new set of fields. Once you have annotated everything you wish to extract, click \u2018Save template\u2019 and you will be redirected to the spider\u2019s start URL. You can now test your spider by visiting another page similar to the one you annotated:", "Once you\u2019ve tested several pages and are satisfied your spider is working, you can now deploy your project to Dash. Click the project link in the breadcrumbs (displayed top left) to leave the spider and go to the project page.", "Click the \u2018Publish changes\u2019 button on the right-hand sidebar to publish your project, and you should receive a message box asking if you want to be redirected to the schedule page. Click \u2018OK\u2019 and you will be redirected to the jobs page in Dash where you can now schedule your spider.", "Click the \u2018Schedule\u2019 button on the top right and select your spider from the dropdown. Click \u2018Schedule\u2019 and Dash will start your spider. You will notice that items are being scraped just like any standard Scrapy spider, and you can go into the jobs item page and download the scraped items as you normally would:", "That's all there is to it! Hopefully this demonstrates just how easy it is to create spiders using Portia without writing any code whatsoever. There are a lot of features we didn't cover, and we recommend taking a look at the ", " if you want to learn more. Portia is open source, so you can run your own instance if you don't wish to use Scrapy Cloud, and we are open to pull requests!"]},
{"tite": "A Career in Remote Working", "date": "April 28, 2015 ", "author": "Richard Dowinton", "blog_data": ["This year I have reached a major milestone in my life, which is getting my bachelor's degree in mathematics. When I made the decision to go back to college, it was solely because my experience working at Scrapinghub, I figured out that having a math background would be a great foundation for getting into ML-related stuff.", "Now my life journey has\u00a0a new beginning and I wouldn't be here if it weren't for the opportunity you gave me. I will be always grateful with you.", "-- Rolando, our first\u00a0hire", ", Scrapinghub has been a fully\u00a0remote\u00a0company, and now boasts over 100 employees working from all over the world, either from their homes or local coworking spaces. Our decision to maintain a fully remote workforce has proven to be a very good decision, allowing us to access a much wider range of talent compared to hiring locally.", "So what about the career of someone who is considering remote work? It can seem risky to leave your cushy on-site job behind in favour of working for a company located across the globe, but the risk is smaller than you would imagine.", "Rolando is a great example of someone who has had a lot of success with working remotely for the past 7 years. Born in Argentina, and raised in Bolivia, Rolando Espinoza at 30 years old has been a very important part of Scrapinghub since the very beginning. Rolando began his journey at Scrapinghub on\u00a0", "\u2019s previous\u00a0company, Insophia, where he started as a Python developer. By June 2010 he was working on the first version of Scrapinghub\u2019s dashboard.", "The only time Rolando has worked on-site since, is as a software developer in Uruguay alongside founders Pablo and Shane at the former headquarters of Insophia, where Scrapinghub\u2019s Uruguay office now resides.", "Working at Scrapinghub allowed Rolando to pursue a degree in mathematics, as he was able to fit his work schedule around his studies. At Scrapinghub, we allocate teams who share similar time zones and give each member control over their own schedule. You can work mornings, nights, or during whichever time you feel like. Day-to-day rearrangements can always be made and agreed upon within the team, and this flexibility has shown to be a win-win for all.", "Rolando was very happy with his decision to complete a degree in mathematics. In his own words: \"being a remote employee, rather than an independent freelancer, gives the opportunity to work in really interesting and challenging large-scale projects along with very smart people.\"", "Rolando worked on a number of machine learning projects here at Scrapinghub, allowing him to make use of the mathematical knowledge he was gaining at university from the outset. He is now looking forward to joining a CS/Math graduate program in the near future.", "From web development to large scale web data mining projects, from ", " projects to large professional services projects, such as the ", " project, he thinks \"this is something very attractive, especially for those who live in cities or countries with a small and narrow software industry.\"", "Rolando believes that working with smart, highly skilled colleagues from such a variety of countries, has been an excellent opportunity to learn from them and push himself further. He\u2019s proud of being able to keep up with the expectations of a company that strives to provide world-class services from top talent from all over the world.", "We\u2019re all incredibly grateful here at Scrapinghub for all of Rolando\u2019s excellent work and contributions to the company, which he has brought from the very beginning.", "If you\u2019re inspired by Rolando\u2019s story, excited about the prospect of working remotely and looking to join a team of smart, motivated people, check out our ", "."]},
{"tite": "Traveling Tips for Remote Workers", "date": "May 12, 2015 ", "author": "Giovanni Piazza", "blog_data": ["Being free to work from wherever you feel like, no boundaries holding you to a specific place or country. This is one of the greatest advantages of working remotely, and it's leading many people to travel around the globe while completing their work. Today Claudio Salazar, a Scrapinghubber from Chile, is here to share his experiences and tips for these who seek working on the road.", "Claudio\u2019s traveling adventures started in September 2013 and so far he has visited 8 countries and more than 20 cities. He was never the kind of guy that loved traveling, but motivated by the need to improve his English skills, he decided to buy his first flight ticket and get started.", "When asked about the benefits from starting a journey like his, he points the escape from the routine as one of the most positive aspects, impacting both your morale and your open-mindedness. \"I think that staying in one place for a long time makes you live in a routine, but staying in constant change refill your energies since every day you wake up with the discovery of new things in mind. This improves your motivation to work and keep you in a good mood.\"", "But like many trips, things are not always so easy and comfortable. Claudio has faced some drawbacks since he left Chile, such as dealing with different time zones and adapting to new places and cultures. As well as planning a nice trip, you'll also need to be flexible, adapting your working hours and sometimes having to attend to virtual meetings during late or unusual hours. \"An important thing when you try this lifestyle is the flexibility you'll need to have. Scrapinghub gives me the freedom to manage my working hours and actually work at any time without restriction.\"", "After traveling around the world for 16 months, Claudio is currently living the good life with his girlfriend in Paris, France. If you'd like to start a journey like Claudio's, here\u2019s some good advice:", "First, you need to research about the countries that you want to visit - check if they are safe, its legislation, visa requirements, where do you plan to live and so on. Also, always keep in mind the following country you plan to visit, especially if you move as a tourist, because when you enter a country they will probably ask for your outbound ticket. Figure out the details before arriving and avoid unnecessary stress.", "Make sure you have health insurance. You never know when you might get sick, and medical services are expensive in any country, so you better be prepared. You can find online many companies offering health insurances and many sites that offer comparisons between them.", "If you want to visit multiple countries, a good thing is to look for a continental insurance and check if it fits your needs. Most of the insurances must be contracted from your country, so figure it out beforehand. Usually you can contract an insurance for 3 months and renew it, but if you miss the deadline while traveling you can't re-contract it. There are also a few more expensive options that allow you to contract the insurance independently from your departure or current location.", "When you fell sick, you'll have to call the phone number your insurance company provided and they'll make an appointment in the nearest hospital from your residency. In case you need medicines you might have to buy it yourself in a pharmacy, depending on your health insurance, and then ask for refunds.", "Make sure you rent an apartment or room before arriving to the country, because they could ask you where are you going to live while you're staying. Try to get a flat with a nice desk, a comfortable chair and internet connection so you can properly do your work. Sites like Airbnb are useful for finding shorter term lets; more expensive than a 6-12 month lease but cheaper than a hotel.", "Print your bank statements before traveling because you'll probably be asked for them (in a typical \"show me the money\" case by arriving). Keep in mind to always travel with two credit cards, and have debit cards for emergency cases.", "As a foreigner you will learn new things daily as you meet people. A good tip is to check beforehand for popular forums and communities online, or even the well-known Facebook, Couchsurfing. Another good option is finding a meetup site to meet people and have fun. Being a foreigner, you\u2019re likely to receive some kind of special treatment from the locals.", "Since you'll be working while traveling, one of your challenges will be to keep fulfilling your responsibilities at work while on the road. Aside from the trip preparations, you'll have to manage any unexpected travel issue and still get things done remotely.", "Make sure to always bring your gadgets (smartphone, tablet) and your notebook with you - you never know when one of them may break or malfunction, so a backup gadget can save the communication with your team and buy you time until you address the issues. A good advice is to buy a prepaid cellphone chip with 3G or 4G internet when you arrive to a country so you have a backup internet connection if needed.", "Also, before renting a flat or choosing a new place to visit, it is important to check for close cafes with internet connection, coworking spaces and wifi zones - so you have more options to keep on working in case your internet lets you down. A good thing is to narrow your choices for places with these resources available nearby.", "In addition to\u00a0Claudio, many other Scrapinghubbers have been traveling while working remotely, and you can see a few of their adventures and\u00a0routes in the following map (feel free to navigate through\u00a0the left menu for more):", "[googlemaps https://www.google.com/maps/d/embed?mid=zcgxf08YWWwc.kOwkgDH2Ztiw&w=640&h=480]", "Do you have an interesting story or tip to share about traveling while working remotely? We'd be glad to hear it! Feel free to share in the comments below. Safe travels!"]},
{"tite": "The Road to Loading JavaScript in Portia", "date": "August 03, 2015 ", "author": "Ruairi Fahy", "blog_data": ["Support for JavaScript has been a ", " ", " feature ever since Portia\u2019s first release 2 years ago. The wait is nearly over and we are happy to inform you that we will be launching these changes in the very near future. If you\u2019re feeling adventurous you can try it out on the develop branch at Github. This post aims to highlight the path we took to achieving JavaScript support in Portia.", "\u00a0", "\u00a0", "As with everything in software, we started out by investigating what our requirements were and what others had done in this situation. We were looking for a solution that was reliable and would allow for reproducible interaction with the web pages.", " A solution that could render the pages in the same way during spider creation and crawling.", " A system that would allow us to record the user's actions so that they could be replayed while crawling.", "The results of the investigation produced some interesting and some crazy ideas, here are the ones we probed further:", "We rejected 7 and 8 because they would increase the barrier of entry for using Portia and make it more difficult to use. This method is used by Import.io for their spider creation tool.", "1 and 2 were rejected because it would be hard to fit the whole Portia UI into an add on in the way we'd prefer, although we may revisit these options in the future. ParseHub and Kimono use these method to great effect.", "3 and 4 were investigated further, inspired by the work done by ", " for their Android document editor.\u00a0In the end though it was clunky and we could achieve better performance by sending DOM updates rather than image tiles.", "The solution we have now built is a combination of 5 and 6. The most important aspect is the server-side browser. This browser provides a tab for each user allowing the page to be loaded and interacted with in a controlled manner.", "We looked at using existing solutions including ", ", ", " and ", ". All of these technologies are wrappers around WebKit providing domain specific functionality. We use Splash for our browser not because it is a Scrapinghub technology but because it is designed to be used for web crawling rather than automated testing making it a better fit for our requirements.", "The server side browser gets input from the user. Websockets are used to send events and DOM updates between the user and the server. Initially we looked at React's virtual DOM, and while it worked it wasn't perfect. Luckily, there is an inbuilt solution, available in most browsers released since 2012, called ", ". This in conjunction with the ", " library allows us to update the page in the UI for the user when they interact with it.", "We now proxy all of the resources that the page needs rather than loading them from the host. The advantage of this is that we can load resources from the cache in our server side browser or from the original host and provide SSL protection to the resources if the host doesn't already provide it.", "\u00a0", "\u00a0", "For now we\u2019re very happy with how it works and hope it will make it easier for users to extract the data they need.", "This initial release will provide the means to crawl and extract pages that require JavaScript, but we want to make it better! We are now building a system to allow actions to be recorded and replayed on pages during crawling. We are hoping that this feature will make filling out forms, pressing buttons and triggering infinite scrolling simple and easy to use.", "If you have any ideas for what features you would like to see in Portia leave a comment below!"]},
{"tite": "Looking Back at 2013", "date": "December 31, 2013 ", "author": "Shane Evans", "blog_data": ["This time last year Pablo and I were chatting about the previous year and what to expect in 2013. I noticed that our team had almost doubled in size in the previous year and we wondered could that possibly continue in 2013?", "It turns out it did! We went from 20 team members to almost 40 and the number of projects, customers, etc. all doubled. We've become even more distributed as we've grown, covering 19 countries:", "\u00a0", "\u00a0", "What the numbers don't show is that these new hires have become indispensable members of the team and it's hard to imagine working without them.", "Over 2 billion web pages were scraped using our platform in 2013! We're pleased this was done politely, without receiving a single complaint. More users participated in our beta and we saw a 4x growth in our platform usage", ":", "This would not have been possible without our new ", ", for reasons explained in our ", ".", "Our open source contributions increased - we had 2 large stable ", " releases (0.18 and 0.20) and started several new projects (e.g. ", ", ", ", ", ",\u00a0", "\u00a0and ", "). Scrapy usage (github stars, forks, etc.) doubled again this year and is currently number 12 in github's ", " this month. We have more exciting releases planned, such as a new open source annotation UI for Autoscraping and Scrapy 1.0!", "We are very proud of our team and what we have achieved this year. We would like to thank all our customers and supporters. Happy New Year everyone!"]},
{"tite": "Open Source at Scrapinghub", "date": "January 18, 2014 ", "author": "Pablo Hoffman", "blog_data": ["Here at Scrapinghub we love open source. We love using and contributing to it. Over these years we have open sourced a few projects, that we keep using over and over, in the hope that it will make others lives easier. Writing reusable code is harder than it sounds, but it enforces good practices such as documenting accurately, testing extensively and worrying about backwards support. In the end it produces better software, and keeps programmers happier. This is why we open source as much as we can and always deliver the complete source code to our clients, so they can run everything on their machines if they ever want or need to do so.", "Here is a list of open source projects we currently maintain, most of them born and raised at Scrapinghub:", " is the most popular web crawling framework for Python, used by thousands of companies around the world to power their web crawlers. At Scrapinghub we use it to crawl millions of pages daily. We use ", " for running our Scrapy crawlers without having to manage servers or plan capacity beforehand.", " combines the power of Scrapy and Scrapely into a standalone web crawler application. We are currently working in a new version that will include a fully-featured visual annotation tool (the one used so far by ", " never got open sourced). ", "\u00a0the new version has been released, see ", ".", " is used to extract repeated data (such as records in tables) automatically. It\u2019s based on the ", " paper.", " is a framework for creating machine-learning-based ", " \u00a0systems that work on HTML data. Trained webstruct models can work on many different websites, while Scrapely shines where you need to extract data from a single website.\u00a0Webstruct models require much more training than Scrapely ones, but we do it once per task \u00a0(e.g. \"contact extraction\"), not per website, so it scales better to a larger number of websites. A\u00a0", " is in the works and due to be merged soon.", " is used for filling website login forms given just the login page url, username & password. Which form and fields to submit are inferred automatically", " is used to paginate search results automatically without having to specify where the \u201cnext\u201d button is", " is a web service to render pages using javascript. It\u2019s quite lightweight compared to running a complete web browser (like selenium)", "If you are working on web data mining, take a moment to review them, there\u2019s a high chance you will need one of those for your next project, and it might not be the kind of wheel you would want to reinvent."]},
{"tite": "Optimizing Memory Usage of Scikit-Learn Models Using Succinct Tries", "date": "March 26, 2014 ", "author": "Mikhail Korobov", "blog_data": ["We use the ", " library for various machine-learning tasks at Scrapinghub. For example, for text classification we'd typically build a statistical model using sklearn's Pipeline, FeatureUnion, some classifier (e.g. LinearSVC) + feature extraction and preprocessing classes. The model is usually trained on a developers machine, then serialized (using pickle/joblib) and uploaded to a server where the classification takes place.", "Sometimes there can be too little available memory on the server for the classifier. One way to address this is to change the model: use simpler features, do feature selection, change the classifier to a less memory intensive one, use simpler preprocessing steps, etc. It usually means trading accuracy for better memory usage.", "For text it is often CountVectorizer or TfidfVectorizer that consume most memory. For the last few months we have been using a trick to make them much more memory efficient in production (50x+) without changing anything from statistical point of view - this is what this article is about.", "Let's start with the basics. Most machine learning algorithms expect fixed size numeric feature vectors, so text should be converted to this format. Scikit-learn provides CountVectorizer, TfidfVectorizer and HashingVectorizer for text feature extraction (see the scikit-learn ", " for more info).", "CountVectorizer.transform converts a collection of text documents into a matrix of token counts. The counts matrix has a column for each known token and a row for each document; the value is a number of occurrences of a token in a document.", "To create the counts matrix CountVectorizer must know which column corresponds to which token. The CountVectorizer.fit method basically remembers all tokens from some collection of documents and stores them in a \"vocabulary\". Vocabulary is a Python dictionary: keys are tokens (or n-grams) and values are integer ids (column indices) ranging from 0 to len(vocabulary)-1.", "Storing such a vocabulary in a standard Python dict is problematic; it can take a lot of memory even on relatively small data.", "Let's try it! Let's use the \"20 newsgroups\" dataset available in scikit-learn. The \"train\" subset of this dataset has about 11k short documents (average document size is about 2KB, or 300 tokens; there are 130k unique tokens; average token length is 6.5).", "Create and persist CountVectorizer:", "Load and use it:", "On my machine, the loaded vectorizer uses about ", " of memory in this case. If we add bigrams (by using CountVectorizer(ngram_range=(1,2))) then it would take about ", " - and this is for a corpus that is quite small.", "There are only 130k unique tokens; it'll require less than 1MB to store these tokens in a plain text file ((6.5+1) * 130k). Maybe add an another megabyte to store column indices if they are not implicit (130k * 8). So the data itself should take only a couple of MBs. We may also have to somehow enumerate tokens and enable fast O(1) access to data, so there would be an overhead, but it shouldn't take 80+MB - we'd expect 5-10MB at most. The serialized version of our CountVectorizer takes about 6MB on disk without any compression, but it expands to 80+MB when loaded to memory.", "Why does it happen? There are two main reasons:", "Storing static string->id mapping in a hash table is not the most efficient way to do it: there are perfect hashes, tries, etc.; add Python objects overhead and here we are.", "So I decided to try an alternative storage for vocabulary. ", " (via Python ", ") looked like a suitable data structure, as it:", "MARISA-Trie is not a general replacement for dict: you can't add a key after building, it requires more time and memory to build, lookups (via Python wrapper) are slower - about 10x slower than dict's, and it works best for \"meaningful\" string keys which have common parts (not for some random data).", "I must admit I don't fully understand how MARISA-Tries work :) The implementation is available in a folder named \"", "\", and the only information about the implementation I could find is Japanese slides which are outdated (as library author Susumu Yata ", "). It seems to be a succinct implementation of ", " which can store references to other MARISA-Tries in addition to text data; this allows it to compress more than just prefixes (as in \"standard\" tries). \"Succinct\" means the Trie is encoded as a bit array.", "You may never heard of this library, but if you have a recent Android phone it is likely MARISA-Trie is in your pocket - a copy of marisa-trie is in the Android 4.3+ source tree.", "Ok, great, but we have to tell scikit-learn to use this data structure instead of a dict for vocabulary storage.", "Scikit-learn allows passing a custom vocabulary (a dict-like object) to CountVectorizer. But this won't help us because MARISA-Trie is not exactly dict-like; it can't be built and modified like dict. CountVectorizer should build a vocabulary for us (using its tokenization and preprocessing features) and only then we may \"freeze\" it to a compact representation.", "At first, we were doing it using a hack. fit and fit_transform methods were overridden: first, they call the parent method to build a vocabulary, then they freeze that vocabulary (i.e. build a MARISA-Trie from it) and trick CountVectorizer to think a fixed vocabulary was passed to the constructor, and then parents method is called once more. Calling fit/fit_transform twice is necessary because the indices learned on the first call and indices in the frozen vocabulary are different. This quick & dirty implementation is ", ", and this is what we're using in production.", "I recently improved it and removed this \"call fit/fit_transform twice\" hack for CountVectorizer, but we haven't used this implementation yet. See https://gist.github.com/kmike/9750796.", "The results? For the same dataset, MarisaCountVectorizer uses about ", " for unigrams (instead of 82MB) and about ", " for unigrams+bigrams (instead of 650MB+). This is a ", " reduction of memory usage. Tada!", "The downside is that MarisaCountVectorizer.fit and MarisaCountVectorizer.fit_transform methods are 10-30% slower than CountVectorizer's (new version; old version was up to 2x+ slower).", "Numbers:", "'fit' method was executed on 'train' subset of '20 newsgroups' dataset; 'transform' method was executed on 'test' subset.", "marisa-trie stores all data in a contignuous memory block so saving it to disk and loading it from disk is much faster than saving/loading a Python dict serialized using pickle.", "Serialized file sizes (uncompressed):", "TfidfVectorizer is implemented on top of CountVectorizer; it could also benefit from more efficient storage for vocabulary. I tried it, and for MarisaTfidfVectorizer the results are similar. It is possible to optimize DictVectorizer as well.", "Note that MARISA-based vectorizers don't help with memory usage during training. They may help with memory usage when saving models to disk though - pickle allocates big chunks of memory when saving Python dicts.", "So when memory usage is an issue, ditch scikit-learn standard vectorizers and use marisa-based variants? Not so fast: don't forget about HashingVectorizer. It has a number of benefits. Check the ", ": HashingVectorizer doesn't need a vocabulary so it fits and serializes in no time and it is very memory efficient because it is stateless.", "As always, there are some tradeoffs:", "Of course, all vectorizers have their own advantages and disadvantages, and there are use cases for all of them. You can use e.g. CountVectorizer for development and switch to HashingVectorizer for production, avoiding some of HashingVectorizer downsides. Also, don't forget about feature selection and other similar techniques. Using succinct Trie-based vectorizers is not the only way to reduce memory usage, and often it is not the best way, but sometimes they are useful; being a drop-in replacement for CountVectorizer and TfidfVectorizer helps.", "In our recent project, min_df > 1 was crucial for removing noisy features. Vocabulary wasn't the only thing that used memory; MarisaTfidfVectorizer instead of TfidfVectorizer (+ MarisaCountVectorizer instead of CountVectorizer) decreased the total classifier memory consumption by about 30%. It is not a brilliant 50x-80x, but it made the difference between \"classifier fits into memory\" and \"classifier doesn't fit into memory\".", "Some links:", "There is a ", " to discuss efficient vocabulary storage with scikit-learn developers. Once the discussion settles our plan is to make a PR to scikit-learn to make using such vectorizers easier and/or release an open-source package with MarisaCountVectorizer & friends - stay tuned!"]},
{"tite": "Announcing Portia, the Open Source Visual Web Scraper!", "date": "April 01, 2014 ", "author": "Shane Evans", "blog_data": ["We\u2019re proud to announce the developer release of Portia, our new open source visual scraping tool based on ", ". Check out this video:", "\u00a0", "\u00a0", "As you can see, Portia allows you to visually configure what\u2019s crawled and extracted in a very natural way. It provides immediate feedback, making the process of creating web scrapers quicker and easier than ever before!", "Portia is available to developers ", ". We plan to offer a hosted version on Scrapinghub soon, which will be compatible with ", " and fully integrated with our platform.", "Please send us your feedback!"]},
{"tite": "Extracting schema.org Microdata Using Scrapy Selectors and XPath", "date": "June 18, 2014 ", "author": "Paul Tremberth", "blog_data": ["We have released an lxml-based version of this code\u00a0as an open source library called ", ". ", ", and the package is ", ". Enjoy!", "Web pages are full of data, that is what web scraping is mostly about. But often you want more than data, you want ", ". Microdata markup embedded in HTML source helps machines understand what the pages are about: contact information, product reviews, events etc.", "Web authors have several ways to add metadata to their web pages: HTML \"meta\" tags, ", ", social media meta tags (Facebook's ", ", ", ").", "And then there is ", ", an initiative from big players in the search space (", ", ", ", ", " and ", ") to \u201ccreate and support a common set of schemas for structured data markup on web pages.\u201d", " from Web Data Commons project on the use of microdata in the ", ":", "In summary, we found structured data within 585 million HTML pages out of the 2.24 billion pages contained in the crawl (26%).", "Let's focus on Schema.org syntax for the rest of this article. The markup looks like this (example from ", "):", "Let's assume you want to extract this microdata and get the ", " item from the snippet above.", "Using Scrapy ", " let's first loop on elements with an ", " attribute (this represents a container for an item's properties) using ", ", and for each item, get all properties, i.e. elements that have an ", " attribute:", "Let's print out more interesting stuff, like the item's ", " and the properties' values (the text representation of their HTML element):", "Hm. The value for trailer isnt that interesting. We should have selected the ", " of ", ".", "For the following extended example markup, ", ",", "we see it's not always ", " attributes that are interesting, but ", " perhaps, or ", " for images, or ", " for meta element like ``... any attribute really.", "Therefore we need to get ", " text content and elements attributes (we use the ", " XPath expression for that).", "Let's do that on the 2nd HTML snippet:", "Wait a minute! We're getting only attribute values, not attribute names (and ", " attribute twice for that matter, once for ", " and once for ", ").", "To get names of attributes, one cannot apparently use ", " on an attribute... BUT you can do ", ", ", " being the attribute position:", "There's still something wrong, right? Duplicate properties.", "It's because the 2nd HTML snippet is using ", ". Indeed, James Cameron is the ", " of \"Avatar\", but he's also a ", " (yes, he is!). That's why the markup says ", " with an ", " attribute.", "How can we fix that, only selecting properties at the current scope, and leave nested properties when we reach the nested item?", "Well, it happens that Scrapy Selectors support ", ", notably the ", " operations. Here, we'll use ", ": we'll select ", " under the current ", ", and then exclude those that are themselves children of another ", ", under the current one.", "But wouldn't it be nice to keep a reference to the ", " item as property of the ", " director ", "? We'd need to uniquely identify items in the markup, maybe the position of each ", " element, or its number.", "You can use XPath ", " function for that, counting other ", " that are siblings before (", ") or ancestors of the current one (", "):", "Here's a cleaned up example routine, referencing items by their ID when ", " are also ", ":", "This dict output is not quite what ", ", but it's close enough to leave the rest to you as an exercise ;-)", "You can find a Github Gist of the above code ", "."]},
{"tite": "XPath Tips from the Web Scraping Trenches", "date": "July 17, 2014 ", "author": "Elias Dorneles", "blog_data": ["In the context of web scraping, ", " is a nice tool to have in your belt, as it allows you to write specifications of document locations more flexibly than CSS selectors. In case you're looking for a tutorial, ", ".", "In this post, we'll show you some tips we found valuable when using XPath in the trenches, using ", " for our examples.", "Here is why: the expression ", " yields a collection of text elements -- a ", ". And when a node-set is converted to a string, which happens when it is passed as argument to a string function like ", " or ", ", results in the text for the ", " element only.", "A ", " converted to a string, however, puts together the text of itself plus of all its descendants:", "So, in general:", "You can read ", ".", " selects all the nodes occurring first under their respective parents.", " selects all the nodes in the document, and then gets only the first of them.", "Also,", " gets a collection of the local anchors that occur first under their respective parents.", " gets the first local anchor in the document.", "If you want to select elements by a CSS class, the XPath way to do that is the rather verbose:", "Let's cook up some examples:", " doesn't work because there are multiple classes in the attribute", " gets more than we want", "And many times, you can just use a CSS selector instead, and even combine the two of them if needed:", "Read ", ".", "It is handy to know how to use the axes, you can ", " to quickly review this.", "In particular, you should note that ", " and ", " are not the same thing, this is a common source of confusion. The same goes for ", " and ", ", and also ", " and ", ".", "Here is another XPath trick that you may use to get the interesting text contents:", "This excludes the content from ", " and ", " tags and also skip whitespace-only text nodes. Source: ", "Please, leave us a comment with your tips or questions. :)", "And for\u00a0everybody who contributed tips and reviewed this article, a big thank you!"]},
{"tite": "Introducing Data Reviews", "date": "June 27, 2014 ", "author": "Breno Colom", "blog_data": ["One of the things that takes more time when building a spider is reviewing the scraped data and making sure it conforms to the requirements and expectations of your client or team. This process is so time consuming that, in many cases, it ends up taking more time than writing the spider code itself, depending on how well the requirements are written. To make this process more efficient we have introduced the ability to comment data directly on Dash (Scrapinghub UI), right next to the data, instead of relying on other channels (like issue trackers, emails or chat).", "\u00a0", "\u00a0", "With this new feature you can discuss problems with data right where they appear without having to copy/paste data around, and have a conversation with your client or team until the issue is resolved. This reduces the time spent on data QA, making the whole process more productive and rewarding.", "So go ahead, start adding comments to your data (you can comment whole items or individual fields) and let the conversation flow around data! You can mark resolved issues by archiving comments, and you will see jobs with unresolved (unarchived) comments directly on the Jobs Dashboard.", "Last, but not least, you have the ", " to insert comments programmatically. This is useful, for example, to report problems in post-processing scripts that analyze the scraped data.", "Happy scraping!"]},
{"tite": "Introducing ScrapyRT: An API for Scrapy spiders", "date": "January 22, 2015 ", "author": "Richard Dowinton", "blog_data": ["We\u2019re proud to announce our new open source project, ", "! ScrapyRT, short for Scrapy Real Time, allows you to extract data from a single web page via an API using your existing Scrapy spiders.", "We needed to be able to retrieve the latest data for a previously scraped page, on demand. ScrapyRT made this easy by allowing us to reuse our spider logic to extract data from a single page, rather than running the whole crawl again.", "ScrapyRT runs as a web service and retrieving data is as simple as making a request with the URL you want to extract data from and the name of the spider you would like to use.", "Let\u2019s say you were running ScrapyRT on localhost, you could make a request like this:", "ScrapyRT will schedule a request in Scrapy for the URL specified and use the \u2018foo\u2019 spider\u2019s parse method as a callback. The data extracted from the page will be serialized into JSON and returned in the response body. If the spider specified doesn\u2019t exist, a 404 will be returned. The majority of Scrapy spiders will be compatible without any additional programming necessary.", "ScrapyRT will be running on port 9080, and you can schedule your spiders per the example shown earlier.", "We hope you find ScrapyRT useful and look forward to hearing your feedback!", "Comment here or ", "."]},
{"tite": "Looking Back at 2014", "date": "December 31, 2014 ", "author": "Breno Colom", "blog_data": ["\u00a0we were looking back at the great 2013 we had and realized we would have quite a big challenge in front of us in order to have as much growth as we had during last year. So here are some highlights of the things we\u2019ve been up to during this year, let's see how well we did!", "2014 was quite the travelling year for Scrapinghub! We sponsored both the US PyCon in Montreal and the spanish PyCon in Zaragoza. We\u2019ve also been to Codemotion in Madrid and PythonBrasil. We hope to hit the road during 2015 too, bringing some spider magic to even more cities!", "\u00a0", "\u00a0", "During this year we\u2019ve also continued to work on both new and ongoing ", " projects for clients all around the world and we\u2019re glad to see that our efforts are paying off, we have increased our customer base while maintaining the same quality standards we\u2019ve had since we were just a few guys, back in 2010.", "Our ", " has grown too! There\u2019s been steady effort in getting it to the point where we\u2019ve been able to accommodate the ever increasing volume of scraping we and our customers have been doing. In 2014 alone ", " has scraped and stored data from over 10 billion pages (more than 5 times the amount we did in 2013!) and an extra 5 billion have passed through ", ".", "We are excited to see our revenue tripling from last year, and it makes us very proud to have grown organically so far. We can only imagine what we could do with some funding, but we won't do anything that could jeopardize the way we run the company, which has proven very successful.", "In the open source front, we\u2019ve been spending a lot of time improving our annotation based scraping tool Portia. Our main focus has been on integrating it into our Scrapy cloud platform, and soon Scrapinghub users will be able to open their Autoscraping projects in Portia. You can see an example of the current Dash integration ", ". This will eventually be our successor to our Autoscraping tool. If you just cannot wait to try Portia you\u2019re in luck, we ", " sometime ago (it was trending Python project on Github for a month!) so you can try it locally if you wish!", "We also have a number of new and interesting open source projects: Dateparser, Crawl Frontier and Splash.", "Of course, all our other existing open source projects such as ", ", ", " and ", " have seen major improvements, something that will keep on going during 2015 and beyond.", "We\u2019re glad to be able to share all these experiences, numbers and new projects with you, but we know very well that behind every single one of those stands the hard work done by all members of our team, saying that this wouldn\u2019t have been possible without them is an understatement. And the team has grown, 2014 marks the second year in a row that our team has doubled, we\u2019re now ", "!\u00a0(and we'll be over 90 by the end of January)", "\u00a0", "\u00a0", "So here\u2019s a sincere thank you from the Scrapinghub team to all of our customers and supporters. Thanks for an amazing 2014 and watch out 2015, here we come! Happy New Year!"]},
{"tite": "New Changes to Our Scrapy Cloud Platform", "date": "January 23, 2015 ", "author": "Richard Dowinton", "blog_data": ["We are proud to announce some exciting changes we've introduced this week. These changes bring a much more pleasant user experience, and several new features including the addition of ", " to our platform!", "Here are the highlights:", "We have introduced a number of improvements in the way our dashboard looks and feels. This includes a new layout based on Bootstrap 3, a more user-friendly color scheme, the ability to schedule jobs once per month, and a greatly improved spiders page with pagination and search.", "Filtering and pagination of spiders:", "A new user interface for adding periodic jobs:", "A new user interface for scheduling spiders:", "And much more!", "You are now able to create and manage organizations in Scrapy Cloud, add members if necessary and from there create new projects under your organization. This will make it much easier for you to manage your projects and keep them all in one place. Also, to make things simpler, our billing system will soon be managed at the organization level rather than per individual user.", "You are now able to create projects within the context of an organization, however other organization members will need to be invited in order to access it. A user can be invited to a project even if that user is not a member of the project's organization.", "Due to popular demand, we have added the ability to download your items as XML:", "We have made several improvements to the way periodic jobs are handled:", ", we open-sourced our annotation based scraping tool, Portia. We have since been working to integrate it into Dash, and it's finally here!", "We have added an 'Open in Portia' button to your projects' ", " page, so you can now open your Scrapy Cloud projects in Portia. We intend Portia to be a successor to our existing Autoscraping interface, and hope you find it to be a much more pleasant experience. No longer do you have to do a preliminary crawl to begin annotating, you can just jump straight in!", "Check out ", " of how you can create a spider using Portia and Dash!", "Enjoy the new features, and of course if you have any feedback please don't hesitate to post on our ", "!"]},
{"tite": "Autoscraping casts a wider net", "date": "February 27, 2012 ", "author": "Shane Evans", "blog_data": ["We have recently started letting more users into the private beta for our ", ". We're receiving a lot of applications following the\u00a0", " and we're increasing our capacity to accommodate these users.", "Natalia made a screencast to help our new users get started:", "\u00a0\n", "It's also a great introduction to what this service can do.", "We released ", " as an open source integration of the ", " extraction library and the ", "framework. This is the core technology behind the autoscraping service and we will make it easy to export autoscraping spiders from Scrapinghub\u00a0 and run them completely with slybot - allowing our users to have the flexibility and freedom provided by open source."]},
{"tite": "Finding Similar Items", "date": "July 23, 2012 ", "author": "Shane Evans", "blog_data": ["Near duplicate content is everywhere on the web and needs to be considered in any web crawling project.", "Web pages might differ only in a small portion of their content, such as advertising, timestamps, counters, etc. This fact can be used when crawling to improve quality and performance and there are efficient ways to detect these near duplicate web pages", ".", "However, there are times when you need to identify similar items in the extracted data. This could be unwanted duplication, or we may want to find the same product, artist, business, holiday package, book, etc. from different sources.", "As an example, let\u2019s say we\u2019re interested in compiling information on tourist attractions in Ireland and would like to output each attraction, with links to various websites where more information can be found. \u00a0Consider the following examples of records that could be crawled:", "Although they are obviously the same place, there\u2019s not much textual similarity between these two records. \u201cCathedral\u201d is the only word in common in the name, and we have a lot of those in Ireland!", "It turns out there are other common spellings of St. Fin Barre's, many websites list it multiple times (not realizing they have duplicates) and not all websites have the location listed.", "A common way to implement this is to first produce a set of tokens (could be hashes, words, shingles, sketches, etc.) from each item, measure the similarity between each pair of sets and if it\u2019s above a threshold then the items are said to be near duplicates.", "Consider the name fields in the example above. If we split each into words, then we can say that they have one word in common out of a total of 7 unique words - a similarity of 14% ", ".", "Unfortunately, comparing all pairs of items is only feasible when we have very few items", ". It may work (eventually) for tourist attractions in Ireland, but we need to use this on hundreds of millions of items. Instead of comparing all pairs, we restrict it to only pairs with at least one token in common (e.g. by using an ", "). The performance of this approach depends mainly on how many \u201ccandidate pairs\u201d are generated.", "The quality of the similarity function can be improved by generating better tokens. Firstly, we could make a database of common synonyms, and recognise that \u201cSt.\u201d is an abbreviation for \u201cSaint\u201d (and street - be careful!). Now we have 2 in common out of 6 unique words - up to 33%! In addition to synonyms, it\u2019s a good idea to remove markup, ignore case, include stop words (reduces false-positives and the number of candidate pairs) and stem words. There are other possible ways to generate tokens instead of using words, such as by taking into account position, adjacent words, using the characters that make up the words, extracting \u201centities\u201d, etc.", "For a more detailed description, please see the excellent book ", ". Tokenization and linguistic processing are covered in Section ", " and section ", " covers near duplicate detection.", "Including more fields in the similarity calculation will make it more accurate. Returning to our example, there are many cathedrals named after saints, but the location can narrow it down. Additionally, the description can help disambiguate from other crawled items (e.g. hotels near St. Fin Barre\u2019s Cathedral).", "The same techniques as described above can be used to make tokens for the description. We just have to be aware that longer text generates more candidate matches and is more likely to be similar to other random text, so taking multiple words together as the tokens is a good idea. For example, instead of:", "We take each 3 adjacent words:", "This technique is called ", " and is commonly used when calculating similarity.", "Location needs to be treated differently. The two examples given above happen to share many digits, but that won\u2019t always be the case. We use ", " to convert the co-ordinates into a bucket and use both the bucket and its neighbours as the tokens. For our first example record, we generate the following buckets:", " ", "Locations that are close will share some buckets.", "Counting tokens in common across multiple fields gives poor results; the field with the most tokens (description in our example) will completely dominate the calculation. Instead we calculate the similarity between each field in common (name, description, location) individually and combine these similarities into a total similarity for the pair of items. When combining scores, we can give more weight to more important fields.", "With a high enough similarity threshold, we can prove that some similarity in multiple fields is necessary for the final score to be above the threshold. Therefore, we can generate tokens that combine the token data of more than one field into \u201csuper tokens\u201d. Although there are more tokens generated, they will be much rarer and this reduces the size of the candidate pairs and improves performance significantly.", "We observed that requiring a match on multiple fields can improve the quality and we can use super tokens to encode these rules. In our example, we could add a rule saying that we must match on either (name, description) or (name, location) and generate super tokens for these field combinations. For (name, description) with shingling of description, the first example record would have the following tokens:", "Once we have found pairs of similar items, the next step is to merge these into a single item. In our case, we want to find all St. Fin Barre's Cathedrals in our data and output a single record with links.", "The first thing to notice is that similarity is not transitive - if A is similar to B and B is similar to C, our function may not find A similar to C. So, we connect these into clusters and generate our output from the connected-up items.", "In practise, these clusters can sometimes get too large as a weak similarity between clusters can cause them to be merged, resulting in large clusters of unrelated items. We found that algorithms designed for detecting communities in social networks are able to efficiently generate good clusters. The following example is from a large crawl, where a single item has some similarity with many different items, causing their respective clusters to get connected:", "It's like someone who friends everyone on Facebook!", "The final step is to output a record for each cluster. In our example, we would output the attraction name, and a link to each page we found it on.", "We have described a system for finding similar items in scraped data. We have implemented it as a library based on ", ", which has been in use for over a year and has proven successful on many scraping projects.", "If you are interested in using this on your crawling project, please contact our ", " team."]},
{"tite": "Scrapy 0.15 dropping support for Python 2.5", "date": "February 27, 2012 ", "author": "Pablo Hoffman", "blog_data": ["After a year considering it, we have decided to go ahead and drop support for Python 2.5 in Scrapy.", "Starting from 0.15, Scrapy will require Python 2.6 or above.", "The main reasons for dropping support for Python 2.5 are:", "The code cleanup and the changes haven't been done yet, nor they will happen all at once in a single commit, so Scrapy 0.15 still works on Python 2.5. Not for long however, since we will be making these changes during the remaining development phase of 0.15."]},
{"tite": "Spiders activity graphs", "date": "August 25, 2012 ", "author": "Pablo Hoffman", "blog_data": ["Today we are introducing a new feature called Spider activity graphs. These allow you to visualize quickly how your spiders are working, and it's a very useful tool for busy projects to find out which spiders are not working as expected.", "This new graph shows you a snapshot of your project each day, displaying one dot per spider. The best way to illustrate how it works is to look at a particular example. This a snapshot of a single day in a busy project (each dot is a spider running that day):", "That is just one day, from the last 30 days covered by the graph (which you can quickly traverse). \u00a0Here is how the graph is read:", "Once we learn how to read it, we can get a sense of the following things by quickly glancing over the graph:", "As you can see, this graph concentrates a lot of information into a single place, which allows to quickly grasp an idea of how the spiders are working and how they compare against each other.", "There are other useful features not mentioned here, like being able to track a specific spider (or group of them), changing how metrics are displayed (for example, mapping colors to errors instead of job counts), and playing an automated animation over time. Our customers can already enjoy this new report in the panel, by going to Reports -> Spiders activity.", "These are some of the features you can expect from our platform, and it covers both the ", " and ", " services. We believe visualization plays an important role when monitoring and keeping track of many spiders, even if you are lucky enough to have time to dive into detailed reports and lots of numbers (most of us aren't)."]},
{"tite": "How to Fill Login Forms Automatically", "date": "October 26, 2012 ", "author": "Pablo Hoffman", "blog_data": ["We often have to write spiders that need to login to sites, in order to scrape data from them. Our customers provide us with the site, username and password, and we do the rest.", "The classic way to approach this problem is:", "Being fans of automation, we figured we could write some code to automate point 2 (which is actually the most time-consuming) and the result is ", ", a library to automatically fill login forms given the login page, username and password.", "Here is the code of a simple spider that would use loginform to login to sites\u00a0automatically:", "In addition to being open source, ", " code is very simple and easy to hack (check the ", "\u00a0on Github for more details). It also contains a collection of HTML samples to keep the library well-tested, and a convenient tool to manage them. Even with the simple code so far, we have seen accuracy rates of 95% in our tests.\u00a0We encourage everyone with similar needs to give it a try, provide feedback and contribute patches.", "We contributed this project to the\u00a0", "\u00a0to better encourage community adoption and contributions. Like\u00a0", "\u00a0and\u00a0", ",\u00a0loginform is completely decoupled and not\u00a0dependent\u00a0on the\u00a0", "."]},
{"tite": "Why MongoDB Is a Bad Choice for Storing Our Scraped Data", "date": "May 13, 2013 ", "author": "Shane Evans", "blog_data": ["MongoDB was used early on at Scrapinghub to store scraped data because it's convenient. Scraped data is represented as (possibly nested) records which can be serialized to JSON. The schema is not known ahead of time and may change from one job to the next. We need to support browsing, querying and downloading the stored data. This was very easy to implement using MongoDB (easier than the alternatives available a few years ago) and it worked well for some time.", "Usage has grown from a simple store for scraped data used on a few projects to the back end of our Scrapy Cloud platform. Now we are experiencing limitations with our current architecture and rather than continue to work with MongoDB, we have decided to move to a different technology (more in a later blog post). Many customers are surprised to hear that we are moving away from MongoDB, I hope this blog post helps explain why it didn't work for us.", "We have a large volume of short queries which are mostly writes from web crawls. These rarely cause problems as they are fast to execute and the volumes are quite predictable. However, we have a lower volume of longer running queries (e.g. exporting, filtering, bulk deleting, sorting, etc.) and when a few of these run at the same time we get lock contention.", "Each MongoDB database (server prior to 2.2) has a ", ". Due to lock contention all the short queries need to wait longer and the longer running queries get ", " longer! Short queries take so long they time out and are retried. Requests from our website (e.g. users browsing data) take so long that all worker threads in our web server get blocked querying MongoDB. Eventually the website and all web crawls stop working!", "To address this we:", "MongoDB does not automatically reclaim disk space used by deleted objects and it is not feasible (due to locking) to manually reclaim space without substantial downtime. It will attempt to reuse space for newly inserted objects, but we often end up with very fragmented data. Due to locking, it's not possible for us to defragment without downtime.", "Scraped data often compresses well, but unfortunately there is no built in compression in MongoDB. It doesn't make sense for us to compress data before inserting because the individual records are often small and we need to search the data.", "Always storing object field names can be wasteful, particularly when they never change in some collections.", "We run too many databases for MongoDB to comfortably handle. Each database has a minimum size allocation so we have wasted space if the size of the data in that DB is small. If no data is in the disk cache (e.g. after a server restart), then it can take a long time to start MongoDB as it needs to check each database.", "Some data (e.g. crawl logs) needs to be returned in the order it was written. Retrieving data in order requires sorting which is impractical when the number of records gets large.", "It is only possible to maintain order in MongoDB if you use capped collections, which are not suitable for crawl output.", "There is no limit on the number of items written per crawl job and it's not unusual to see jobs that have a few million items. When reading data from the middle of a crawl job, MongoDB needs to walk the index from the beginning to the offset specified. It gets slow browsing deep into a job with a lot of data.", "Users may download job data via our API by ", ". For large jobs (say, over a million items), it's very slow and some users work around this by issuing multiple queries in parallel, which of course causes high server load and lock contention.", "There are some odd restrictions, like ", ".\u00a0This is unfortunate, since we lack control over the field names we need to store.", "We have many TB of data per node. The frequently accessed parts are small enough that it should be possible to keep them in memory. The infrequently accessed data is often sequentially scanned crawl data.", "MongoDB does not give us much control over where data is placed, so the frequently accessed data (or data that is scanned together) may be spread over a large area. When scanning data only once, there is no way to prevent that data evicting the more frequently accessed data from memory. Once the frequently accessed data is no longer in memory, MongoDB becomes IO bound and lock contention becomes an issue.", "After embracing MongoDB, its use spread to many areas, including as a back-end for our django UI. The data stored here should be clean and structured, but MongoDB makes this difficult. Some limitations that affected us are:", "There is a niche where MongoDB can work well. Many customers tell us that they have positive experiences using MongoDB to store crawl data. So did Scrapinghub for a while, but it\u2019s no longer a good fit for our requirements and we cannot easily work around the problems presented in this post.", "I'll describe the new storage system in future posts, so please follow ", " if you are interested!", "Comment here or in ", "."]},
{"tite": "Introducing Dash", "date": "July 27, 2013 ", "author": "Shane Evans", "blog_data": ["We're excited to introduce ", ", a major update to our scraping platform.", "This release is the final step in migrating to our new storage back end and contains improvements to almost every part of our infrastructure. In this post I'd like to introduce some of the highlights.", "Our new storage back end is based on HBase", " and attempts to address some of the ", ".", "The areas with noticeable improvements are:", "During our testing and parallel run, we had 2 disk failures that had no impact on users. Machine failures caused errors until the machine was automatically marked as unavailable. Running crawls were never affected.", "When you run an ", " spider in Dash, all pages are saved and available under the new ", " tab:", " ", "Here you can see the requests made by the spider, our cache of the page and what is extracted with the current templates. It's much easier to analyze a scraping run and jump directly into fixing issues - no more ", "!", ".", "There is a new API for our storage back end. It's not yet stable and our documentation isn't published, but the ", " is available. This API makes it possible to access functionality not currently available elsewhere, such as:", "Because our bots now use this API, they can run from any location. We expect to open up new locations in the near future and support non scrapy-based jobs.", "The UI is using this API to load data asynchronously, making it more responsive and giving us a cleaner separation between our UI and back end. We\u2019re building on this refactor now and hope to see more UI improvements coming soon!", "If you wish to use this new API before we make a stable release, please let us know!", "Some projects in Dash regularly schedule tens of thousands of jobs at once, which previously degraded performance and caused other projects' jobs to be stuck in a pending state for a very long time.", "Each project now has two settings that control running jobs:", "When there is capacity to run another job in a given bot group, a new job is selected by choosing the next job from the project with the fewest running jobs in that bot group.", "We're using infinite scrolling for crawl data in Dash. Initially the feedback was mixed, but now everybody prefers the new version. It makes navigating crawl data more fluid as you don\u2019t need to alternate between scrolling and clicking \u201cnext\u201d - just keep scrolling and data appears. Give it a try and let us know what you think!", "We have some changes coming soon to allow jumping ahead (and avoid scrolling) and various usability and performance tweaks.", "Items, pages and logs are returned in insertion order. This means (at last!) logs are always ordered correctly, even when filtering.", "Pending jobs are ordered by priority, then by age. So the next job to be run is at the bottom of the list. Running and finished jobs are ordered by the time they entered that state.", "We are doing our best to minimize disruption, however, some changes may be noticeable:", "Please visit our ", " if you have suggestions or bug reports, we're looking forward to your feedback!"]},
{"tite": "Introducing Crawlera, a Smart Page Downloader", "date": "May 11, 2013 ", "author": "Pablo Hoffman", "blog_data": ["We are proud to introduce ", ", a smart web downloader designed specifically for web crawling.", "Crawlera routes requests through a large, distributed pool of IPs, throttling access by introducing (carefully crafted) delays and discarding IPs from the pool when they get banned from certain domains, or have other problems. As a scraping user, you no longer have to worry about tinkering with download delays, concurrent requests, user agents, cookies or referrers to avoid getting banned or throttled, you just configure your crawler to download pages through Crawlera and relax.", "Crawlera supports\u00a0", "ustom region routing (only US supported for now, more regions to come).", "To learn more or sign up (public beta is open!) please visit the ", "."]},
{"tite": "Git Workflow for Scrapy Projects", "date": "March 06, 2013 ", "author": "Pablo Hoffman", "blog_data": ["Our customers often ask us what's the best workflow for working with Scrapy projects.\u00a0A popular approach we have seen and used in the past is to split the spiders folder (typically project/spiders) into two folders: project/spiders_prod and project/spiders_dev, and use the ", " setting to control which spiders are loaded on each environment. This works reasonably well, until you have to make changes to common code used by many spiders (ie. code outside the spiders folder), for example common base spiders.", "Nowadays, ", "\u00a0(in particular, git) have become more popular and people are quite used to branching, so we recommend using a simple git workflow (similar to ", ") where you branch for ", " change you make. You keep all changes in a branch while they're being tested and finally merge to master when they're finished. This\u00a0means\u00a0that master branch is always stable and contains only \"production-ready\" spiders.", "If you are using our ", " platform, you can have 2 projects (", ", ", ") and use ", " to test the changes in your branch. \u00a0", "\u00a0in Scrapy 0.17\u00a0now adds the branch name to the version name (when using version=GIT or version=HG), so you can see which branch you are going to run directly on the panel. This is particularly useful with large teams working on a single Scrapy project, to avoid stepping into each other when making changes to common code.", "Here is a concrete example to illustrate how this workflow works:y"]},
{"tite": "Hello, world", "date": "June 26, 2010 ", "author": "Shane Evans", "blog_data": ["It's finally time to start a Scrapinghub blog! In the upcoming months we expect to open our private beta to new customers, launch new services, add many new features and continue to contribute to open source projects. It's about time we had a way to to tell everyone about all the great things that are happening!"]},
{"tite": "Spoofing your Scrapy bot IP using tsocks", "date": "November 12, 2010 ", "author": "Pablo Hoffman", "blog_data": ["It is well known that many websites show different content depending on the region where they\u2019re accessed. For example, some retailer sites show products available only for the region (US, Europe) of the user accessing the site.", "Although this can be quite convenient for the website customers, it can be a pain for developers writing a spider for the site and running it from their local machines.", "There is a simple way to proxy all requests as if they came from another server. You only need SSH access to this other server, no need to install any HTTP proxy. For this, you can use a program called\u00a0", ".", "Here\u2019s how to do it in Ubuntu, though this recipe should be easy to extended to other Linux distros.", "First, install tsocks with:", "Then add this content to\u00a0", " (", " recent versions settings are stored at\u00a0", ", but it may vary across distributions):", "Next, SSH to the remote server you want to use:", "And finally, in another terminal (without closing the SSH console), just run Scrapy by prefixing it with the\u00a0", "\u00a0command, like this:", "That\u2019s all. Your spider will run in your local machine but proxying all communication through the remote server. No need to change any settings or configuration."]},
{"tite": "Introducing w3lib and scrapely", "date": "April 20, 2011 ", "author": "Pablo Hoffman", "blog_data": ["In an effort to make Scrapy code smaller and more reusable, we\u2019ve been working on splitting the Scrapy codebase into two different modules:", "A library with simple, reusable functions for working with URLs, HTML, forms, and HTTP. Things that aren\u2019t found in the Python standard library. This library doesn\u2019t have any external dependency.", "For more info see:", "Scrapely is library for extracting structured data from HTML pages. What makes it different from other Python web scraping libraries is that it doesn\u2019t depend on lxml or libxml2. Instead, it uses an internal pure-python parser, which can accept poorly formed HTML. The HTML is converted into an array of token ids, which is used for matching the items to be extracted.", "Scrapely depends on numpy (it uses it to speed up calculations) and w3lib.", "You can find more info, or try it out, in the Github page.", "After these changes, Scrapy codebase has been reduced by 4574 lines, including blank and comments (according to\u00a0", ").", "Before:", "After:", "Scrapy 0.14 will depend on w3lib. Scrapy 0.13 (current dev version) already depends on w3lib, but w3lib is already packaged and provided in the official APT repos (package python-w3lib). So, if you\u2019re using Scrapy 0.13 on Ubuntu, you can upgrade safely. Otherwise, you can always install/upgrade with\u00a0", "\u00a0or\u00a0", ". Stable version (Scrapy 0.12) is not affected at all by this change.", "If you have any comments or questions feel free to post them in the\u00a0", "\u00a0group."]},
{"tite": "Scrapy 0.12 released", "date": "January 03, 2011 ", "author": "Pablo Hoffman", "blog_data": ["Hello everyone, we\u2019re pleased to announce the release of Scrapy 0.12!", "This release is based on the last development branch, aka. Scrapy 0.11.", "Starting from this release, we\u2019ll be following the\u00a0", ". That means trunk is now Scrapy 0.13 and will became Scrapy 0.14 on next release.", "Notable changes of this release:", "For a more detailed list of changes, check the\u00a0", "."]},
{"tite": "Dirbot - a new example Scrapy project", "date": "April 28, 2011 ", "author": "Pablo Hoffman", "blog_data": ["Scrapy users have complained in the past about the lack of a pre-built example project that contains, for example, the dmoz spider described in the tutorial.", "Complain no more!. We're happy to let you know that there is a now functional Scrapy project available on Github which contains the old Google Directory spider and the Dmoz spider described in the tutorial.", "The project is called \"dirbot\", and it's available at\u00a0", "The documentation of Scrapy 0.13 (which will become the next stable release, Scrapy 0.14) has been updated to point to this new example project."]},
{"tite": "Scrapy 0.14 released", "date": "November 18, 2011 ", "author": "Pablo Hoffman", "blog_data": ["After 10 months of work, and many changes, we are pleased to announce the release of Scrapy 0.14.", "For a detailed list of changes see the ", ".", "We expect this one to be the last release in the Scrapy 0.x series, and we are aiming for Scrapy 0.15 to become 1.0 sometime around the first quarter of 2012, where we may consider adopting semantic versioning (semver.org).", "You may have also noticed that the source code, wiki and issues have been moved to Github and the documentation to readthedocs. Here are the new official links:", "Homepage:", "Source code, issues & wiki:", "Documentation (all versions):", "Happy scraping!"]},
{"tite": "Marcos Campal Is a ScrapingHubber!", "date": "October 01, 2013 ", "author": "Pablo Hoffman", "blog_data": ["We\u2019re excited to welcome Marcos Campal to the Scrapinghub engineering team.", "Marcos has a strong academic background in NLP, machine learning and general data processing. During his college years, he taught C Programming, Language Theory, Machine Learning and Information Retrieval courses at the State University in Montevideo.", "He then moved to Europe where he worked in Germany and finally settled in Ireland where he joined Google and worked there for almost 3 years, making heavy use of Python and processing lots of data using their well-known MapReduce framework.", "He returned to Uruguay 3 years ago and built an iOS games development company, most known for their ", " (which is rated 4.5 stars and reached #1 Top Paid game in many countries).", "He likes cooking and is willing to organize the weekly soccer match for the Uruguayan team, although we think he doesn't realize the full extent of that challenge."]}
]